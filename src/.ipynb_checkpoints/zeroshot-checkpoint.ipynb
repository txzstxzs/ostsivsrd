{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4b405c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from pypots.data import load_specific_dataset, mcar, masked_fill\n",
    "from pypots.imputation import MRNN,BRITS,SAITS\n",
    "from pypots.utils.metrics import cal_mse,cal_rmse,cal_mae\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font',family='Times New Roman') # 设置全局字体\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f86ab1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 30, 6])\n",
      "tensor([0.4575,    nan, 0.0009,    nan, 0.4520, 0.4520], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "'第一个样本'\n",
    "oneshot_data = np.loadtxt('AAL.csv',delimiter = \",\")\n",
    "norm_data = MinMaxScaler().fit_transform(oneshot_data)\n",
    "\n",
    "zero_shot = np.array(norm_data[60:90])    # 40-70     seed1 100-130\n",
    "zero_shot = torch.from_numpy(zero_shot).float().cuda()\n",
    "zero_shot = zero_shot.unsqueeze(dim=0)\n",
    "print(zero_shot.shape)\n",
    "\n",
    "\n",
    "np.random.seed(4)\n",
    "# torch.manual_seed(4)\n",
    "# torch.cuda.manual_seed_all(4)\n",
    "\n",
    "X_ori, X_missed_1, missing_mask, indicating_mask = mcar(zero_shot, 0.3, np.nan)\n",
    "print(X_missed[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1233d1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]2025-05-29 21:01:23 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:01:23 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:01:23 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:01:23 [INFO]: epoch 0: training loss 1.2912\n",
      "2025-05-29 21:01:23 [INFO]: epoch 1: training loss 0.7307\n",
      "2025-05-29 21:01:23 [INFO]: epoch 2: training loss 0.5695\n",
      "2025-05-29 21:01:23 [INFO]: epoch 3: training loss 0.6672\n",
      "2025-05-29 21:01:23 [INFO]: epoch 4: training loss 0.6124\n",
      "2025-05-29 21:01:24 [INFO]: epoch 5: training loss 0.5873\n",
      "2025-05-29 21:01:24 [INFO]: epoch 6: training loss 0.5222\n",
      "2025-05-29 21:01:24 [INFO]: epoch 7: training loss 0.4289\n",
      "2025-05-29 21:01:24 [INFO]: epoch 8: training loss 0.3800\n",
      "2025-05-29 21:01:24 [INFO]: epoch 9: training loss 0.3727\n",
      "2025-05-29 21:01:24 [INFO]: epoch 10: training loss 0.3867\n",
      "2025-05-29 21:01:24 [INFO]: epoch 11: training loss 0.4071\n",
      "2025-05-29 21:01:24 [INFO]: epoch 12: training loss 0.4112\n",
      "2025-05-29 21:01:24 [INFO]: epoch 13: training loss 0.3267\n",
      "2025-05-29 21:01:24 [INFO]: epoch 14: training loss 0.2186\n",
      "2025-05-29 21:01:24 [INFO]: epoch 15: training loss 0.2571\n",
      "2025-05-29 21:01:24 [INFO]: epoch 16: training loss 0.2563\n",
      "2025-05-29 21:01:24 [INFO]: epoch 17: training loss 0.2624\n",
      "2025-05-29 21:01:24 [INFO]: epoch 18: training loss 0.2340\n",
      "2025-05-29 21:01:24 [INFO]: epoch 19: training loss 0.2223\n",
      "2025-05-29 21:01:24 [INFO]: epoch 20: training loss 0.2355\n",
      "2025-05-29 21:01:24 [INFO]: epoch 21: training loss 0.2084\n",
      "2025-05-29 21:01:24 [INFO]: epoch 22: training loss 0.1823\n",
      "2025-05-29 21:01:24 [INFO]: epoch 23: training loss 0.2267\n",
      "2025-05-29 21:01:24 [INFO]: epoch 24: training loss 0.1931\n",
      "2025-05-29 21:01:24 [INFO]: epoch 25: training loss 0.2117\n",
      "2025-05-29 21:01:24 [INFO]: epoch 26: training loss 0.1737\n",
      "2025-05-29 21:01:24 [INFO]: epoch 27: training loss 0.1531\n",
      "2025-05-29 21:01:24 [INFO]: epoch 28: training loss 0.1505\n",
      "2025-05-29 21:01:24 [INFO]: epoch 29: training loss 0.1586\n",
      "2025-05-29 21:01:24 [INFO]: epoch 30: training loss 0.1503\n",
      "2025-05-29 21:01:24 [INFO]: epoch 31: training loss 0.1457\n",
      "2025-05-29 21:01:24 [INFO]: epoch 32: training loss 0.1770\n",
      "2025-05-29 21:01:24 [INFO]: epoch 33: training loss 0.1694\n",
      "2025-05-29 21:01:24 [INFO]: epoch 34: training loss 0.1797\n",
      "2025-05-29 21:01:24 [INFO]: epoch 35: training loss 0.1467\n",
      "2025-05-29 21:01:24 [INFO]: epoch 36: training loss 0.1564\n",
      "2025-05-29 21:01:24 [INFO]: epoch 37: training loss 0.1384\n",
      "2025-05-29 21:01:24 [INFO]: epoch 38: training loss 0.1725\n",
      "2025-05-29 21:01:24 [INFO]: epoch 39: training loss 0.1402\n",
      "2025-05-29 21:01:24 [INFO]: epoch 40: training loss 0.1474\n",
      "2025-05-29 21:01:24 [INFO]: epoch 41: training loss 0.1478\n",
      "2025-05-29 21:01:24 [INFO]: epoch 42: training loss 0.1678\n",
      "2025-05-29 21:01:24 [INFO]: epoch 43: training loss 0.1608\n",
      "2025-05-29 21:01:24 [INFO]: epoch 44: training loss 0.1498\n",
      "2025-05-29 21:01:24 [INFO]: epoch 45: training loss 0.1401\n",
      "2025-05-29 21:01:24 [INFO]: epoch 46: training loss 0.1429\n",
      "2025-05-29 21:01:24 [INFO]: epoch 47: training loss 0.1517\n",
      "2025-05-29 21:01:24 [INFO]: epoch 48: training loss 0.1491\n",
      "2025-05-29 21:01:24 [INFO]: epoch 49: training loss 0.1337\n",
      "2025-05-29 21:01:24 [INFO]: epoch 50: training loss 0.1236\n",
      "2025-05-29 21:01:24 [INFO]: epoch 51: training loss 0.1437\n",
      "2025-05-29 21:01:24 [INFO]: epoch 52: training loss 0.1749\n",
      "2025-05-29 21:01:24 [INFO]: epoch 53: training loss 0.1456\n",
      "2025-05-29 21:01:24 [INFO]: epoch 54: training loss 0.1458\n",
      "2025-05-29 21:01:24 [INFO]: epoch 55: training loss 0.1711\n",
      "2025-05-29 21:01:24 [INFO]: epoch 56: training loss 0.1476\n",
      "2025-05-29 21:01:24 [INFO]: epoch 57: training loss 0.1604\n",
      "2025-05-29 21:01:24 [INFO]: epoch 58: training loss 0.1559\n",
      "2025-05-29 21:01:24 [INFO]: epoch 59: training loss 0.1314\n",
      "2025-05-29 21:01:24 [INFO]: epoch 60: training loss 0.1276\n",
      "2025-05-29 21:01:24 [INFO]: epoch 61: training loss 0.1521\n",
      "2025-05-29 21:01:24 [INFO]: epoch 62: training loss 0.1457\n",
      "2025-05-29 21:01:24 [INFO]: epoch 63: training loss 0.1065\n",
      "2025-05-29 21:01:24 [INFO]: epoch 64: training loss 0.1218\n",
      "2025-05-29 21:01:24 [INFO]: epoch 65: training loss 0.1286\n",
      "2025-05-29 21:01:24 [INFO]: epoch 66: training loss 0.1224\n",
      "2025-05-29 21:01:24 [INFO]: epoch 67: training loss 0.1256\n",
      "2025-05-29 21:01:24 [INFO]: epoch 68: training loss 0.1063\n",
      "2025-05-29 21:01:24 [INFO]: epoch 69: training loss 0.1132\n",
      "2025-05-29 21:01:24 [INFO]: epoch 70: training loss 0.1123\n",
      "2025-05-29 21:01:24 [INFO]: epoch 71: training loss 0.1088\n",
      "2025-05-29 21:01:24 [INFO]: epoch 72: training loss 0.0884\n",
      "2025-05-29 21:01:24 [INFO]: epoch 73: training loss 0.1225\n",
      "2025-05-29 21:01:24 [INFO]: epoch 74: training loss 0.1019\n",
      "2025-05-29 21:01:24 [INFO]: epoch 75: training loss 0.1354\n",
      "2025-05-29 21:01:24 [INFO]: epoch 76: training loss 0.1110\n",
      "2025-05-29 21:01:24 [INFO]: epoch 77: training loss 0.0873\n",
      "2025-05-29 21:01:24 [INFO]: epoch 78: training loss 0.0974\n",
      "2025-05-29 21:01:24 [INFO]: epoch 79: training loss 0.1049\n",
      "2025-05-29 21:01:24 [INFO]: epoch 80: training loss 0.0977\n",
      "2025-05-29 21:01:24 [INFO]: epoch 81: training loss 0.0963\n",
      "2025-05-29 21:01:24 [INFO]: epoch 82: training loss 0.1034\n",
      "2025-05-29 21:01:25 [INFO]: epoch 83: training loss 0.1027\n",
      "2025-05-29 21:01:25 [INFO]: epoch 84: training loss 0.1005\n",
      "2025-05-29 21:01:25 [INFO]: epoch 85: training loss 0.1003\n",
      "2025-05-29 21:01:25 [INFO]: epoch 86: training loss 0.0959\n",
      "2025-05-29 21:01:25 [INFO]: epoch 87: training loss 0.1010\n",
      "2025-05-29 21:01:25 [INFO]: epoch 88: training loss 0.1010\n",
      "2025-05-29 21:01:25 [INFO]: epoch 89: training loss 0.1141\n",
      "2025-05-29 21:01:25 [INFO]: epoch 90: training loss 0.1089\n",
      "2025-05-29 21:01:25 [INFO]: epoch 91: training loss 0.0843\n",
      "2025-05-29 21:01:25 [INFO]: epoch 92: training loss 0.1122\n",
      "2025-05-29 21:01:25 [INFO]: epoch 93: training loss 0.1059\n",
      "2025-05-29 21:01:25 [INFO]: epoch 94: training loss 0.1183\n",
      "2025-05-29 21:01:25 [INFO]: epoch 95: training loss 0.1064\n",
      "2025-05-29 21:01:25 [INFO]: epoch 96: training loss 0.1327\n",
      "2025-05-29 21:01:25 [INFO]: epoch 97: training loss 0.1185\n",
      "2025-05-29 21:01:25 [INFO]: epoch 98: training loss 0.1126\n",
      "2025-05-29 21:01:25 [INFO]: epoch 99: training loss 0.1036\n",
      "2025-05-29 21:01:25 [INFO]: epoch 100: training loss 0.0899\n",
      "2025-05-29 21:01:25 [INFO]: epoch 101: training loss 0.0966\n",
      "2025-05-29 21:01:25 [INFO]: epoch 102: training loss 0.1294\n",
      "2025-05-29 21:01:25 [INFO]: epoch 103: training loss 0.1054\n",
      "2025-05-29 21:01:25 [INFO]: epoch 104: training loss 0.0802\n",
      "2025-05-29 21:01:25 [INFO]: epoch 105: training loss 0.1136\n",
      "2025-05-29 21:01:25 [INFO]: epoch 106: training loss 0.0986\n",
      "2025-05-29 21:01:25 [INFO]: epoch 107: training loss 0.0989\n",
      "2025-05-29 21:01:25 [INFO]: epoch 108: training loss 0.0990\n",
      "2025-05-29 21:01:25 [INFO]: epoch 109: training loss 0.0998\n",
      "2025-05-29 21:01:25 [INFO]: epoch 110: training loss 0.1001\n",
      "2025-05-29 21:01:25 [INFO]: epoch 111: training loss 0.0859\n",
      "2025-05-29 21:01:25 [INFO]: epoch 112: training loss 0.0984\n",
      "2025-05-29 21:01:25 [INFO]: epoch 113: training loss 0.1021\n",
      "2025-05-29 21:01:25 [INFO]: epoch 114: training loss 0.0913\n",
      "2025-05-29 21:01:25 [INFO]: epoch 115: training loss 0.1008\n",
      "2025-05-29 21:01:25 [INFO]: epoch 116: training loss 0.1139\n",
      "2025-05-29 21:01:25 [INFO]: epoch 117: training loss 0.1157\n",
      "2025-05-29 21:01:25 [INFO]: epoch 118: training loss 0.1059\n",
      "2025-05-29 21:01:25 [INFO]: epoch 119: training loss 0.1143\n",
      "2025-05-29 21:01:25 [INFO]: epoch 120: training loss 0.1020\n",
      "2025-05-29 21:01:25 [INFO]: epoch 121: training loss 0.1035\n",
      "2025-05-29 21:01:25 [INFO]: epoch 122: training loss 0.0940\n",
      "2025-05-29 21:01:25 [INFO]: epoch 123: training loss 0.0998\n",
      "2025-05-29 21:01:25 [INFO]: epoch 124: training loss 0.1003\n",
      "2025-05-29 21:01:25 [INFO]: epoch 125: training loss 0.1018\n",
      "2025-05-29 21:01:25 [INFO]: epoch 126: training loss 0.1061\n",
      "2025-05-29 21:01:25 [INFO]: epoch 127: training loss 0.0927\n",
      "2025-05-29 21:01:25 [INFO]: epoch 128: training loss 0.1086\n",
      "2025-05-29 21:01:25 [INFO]: epoch 129: training loss 0.0853\n",
      "2025-05-29 21:01:25 [INFO]: epoch 130: training loss 0.0969\n",
      "2025-05-29 21:01:25 [INFO]: epoch 131: training loss 0.0979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:25 [INFO]: epoch 132: training loss 0.0993\n",
      "2025-05-29 21:01:25 [INFO]: epoch 133: training loss 0.0841\n",
      "2025-05-29 21:01:25 [INFO]: epoch 134: training loss 0.0978\n",
      "2025-05-29 21:01:25 [INFO]: epoch 135: training loss 0.1134\n",
      "2025-05-29 21:01:25 [INFO]: epoch 136: training loss 0.0916\n",
      "2025-05-29 21:01:25 [INFO]: epoch 137: training loss 0.0915\n",
      "2025-05-29 21:01:25 [INFO]: epoch 138: training loss 0.1077\n",
      "2025-05-29 21:01:25 [INFO]: epoch 139: training loss 0.1199\n",
      "2025-05-29 21:01:25 [INFO]: epoch 140: training loss 0.0988\n",
      "2025-05-29 21:01:25 [INFO]: epoch 141: training loss 0.1125\n",
      "2025-05-29 21:01:25 [INFO]: epoch 142: training loss 0.1027\n",
      "2025-05-29 21:01:25 [INFO]: epoch 143: training loss 0.1111\n",
      "2025-05-29 21:01:25 [INFO]: epoch 144: training loss 0.0979\n",
      "2025-05-29 21:01:25 [INFO]: epoch 145: training loss 0.1015\n",
      "2025-05-29 21:01:25 [INFO]: epoch 146: training loss 0.0994\n",
      "2025-05-29 21:01:25 [INFO]: epoch 147: training loss 0.0847\n",
      "2025-05-29 21:01:25 [INFO]: epoch 148: training loss 0.0867\n",
      "2025-05-29 21:01:25 [INFO]: epoch 149: training loss 0.1075\n",
      "2025-05-29 21:01:25 [INFO]: epoch 150: training loss 0.0852\n",
      "2025-05-29 21:01:25 [INFO]: epoch 151: training loss 0.0811\n",
      "2025-05-29 21:01:25 [INFO]: epoch 152: training loss 0.0868\n",
      "2025-05-29 21:01:25 [INFO]: epoch 153: training loss 0.0977\n",
      "2025-05-29 21:01:25 [INFO]: epoch 154: training loss 0.0919\n",
      "2025-05-29 21:01:25 [INFO]: epoch 155: training loss 0.0851\n",
      "2025-05-29 21:01:25 [INFO]: epoch 156: training loss 0.0942\n",
      "2025-05-29 21:01:25 [INFO]: epoch 157: training loss 0.0974\n",
      "2025-05-29 21:01:25 [INFO]: epoch 158: training loss 0.0903\n",
      "2025-05-29 21:01:25 [INFO]: epoch 159: training loss 0.0950\n",
      "2025-05-29 21:01:25 [INFO]: epoch 160: training loss 0.0846\n",
      "2025-05-29 21:01:25 [INFO]: epoch 161: training loss 0.0671\n",
      "2025-05-29 21:01:25 [INFO]: epoch 162: training loss 0.0859\n",
      "2025-05-29 21:01:25 [INFO]: epoch 163: training loss 0.0907\n",
      "2025-05-29 21:01:25 [INFO]: epoch 164: training loss 0.0876\n",
      "2025-05-29 21:01:26 [INFO]: epoch 165: training loss 0.0741\n",
      "2025-05-29 21:01:26 [INFO]: epoch 166: training loss 0.0822\n",
      "2025-05-29 21:01:26 [INFO]: epoch 167: training loss 0.0743\n",
      "2025-05-29 21:01:26 [INFO]: epoch 168: training loss 0.0724\n",
      "2025-05-29 21:01:26 [INFO]: epoch 169: training loss 0.0814\n",
      "2025-05-29 21:01:26 [INFO]: epoch 170: training loss 0.0844\n",
      "2025-05-29 21:01:26 [INFO]: epoch 171: training loss 0.0874\n",
      "2025-05-29 21:01:26 [INFO]: epoch 172: training loss 0.0683\n",
      "2025-05-29 21:01:26 [INFO]: epoch 173: training loss 0.0770\n",
      "2025-05-29 21:01:26 [INFO]: epoch 174: training loss 0.0915\n",
      "2025-05-29 21:01:26 [INFO]: epoch 175: training loss 0.0964\n",
      "2025-05-29 21:01:26 [INFO]: epoch 176: training loss 0.0815\n",
      "2025-05-29 21:01:26 [INFO]: epoch 177: training loss 0.0811\n",
      "2025-05-29 21:01:26 [INFO]: epoch 178: training loss 0.0789\n",
      "2025-05-29 21:01:26 [INFO]: epoch 179: training loss 0.0887\n",
      "2025-05-29 21:01:26 [INFO]: epoch 180: training loss 0.0963\n",
      "2025-05-29 21:01:26 [INFO]: epoch 181: training loss 0.0797\n",
      "2025-05-29 21:01:26 [INFO]: epoch 182: training loss 0.0724\n",
      "2025-05-29 21:01:26 [INFO]: epoch 183: training loss 0.0723\n",
      "2025-05-29 21:01:26 [INFO]: epoch 184: training loss 0.0860\n",
      "2025-05-29 21:01:26 [INFO]: epoch 185: training loss 0.0722\n",
      "2025-05-29 21:01:26 [INFO]: epoch 186: training loss 0.0698\n",
      "2025-05-29 21:01:26 [INFO]: epoch 187: training loss 0.0675\n",
      "2025-05-29 21:01:26 [INFO]: epoch 188: training loss 0.0801\n",
      "2025-05-29 21:01:26 [INFO]: epoch 189: training loss 0.0711\n",
      "2025-05-29 21:01:26 [INFO]: epoch 190: training loss 0.0751\n",
      "2025-05-29 21:01:26 [INFO]: epoch 191: training loss 0.0813\n",
      "2025-05-29 21:01:26 [INFO]: epoch 192: training loss 0.0847\n",
      "2025-05-29 21:01:26 [INFO]: epoch 193: training loss 0.0675\n",
      "2025-05-29 21:01:26 [INFO]: epoch 194: training loss 0.0751\n",
      "2025-05-29 21:01:26 [INFO]: epoch 195: training loss 0.0665\n",
      "2025-05-29 21:01:26 [INFO]: epoch 196: training loss 0.0746\n",
      "2025-05-29 21:01:26 [INFO]: epoch 197: training loss 0.0796\n",
      "2025-05-29 21:01:26 [INFO]: epoch 198: training loss 0.0689\n",
      "2025-05-29 21:01:26 [INFO]: epoch 199: training loss 0.0689\n",
      "2025-05-29 21:01:26 [INFO]: epoch 200: training loss 0.0804\n",
      "2025-05-29 21:01:26 [INFO]: epoch 201: training loss 0.0684\n",
      "2025-05-29 21:01:26 [INFO]: epoch 202: training loss 0.0715\n",
      "2025-05-29 21:01:26 [INFO]: epoch 203: training loss 0.0766\n",
      "2025-05-29 21:01:26 [INFO]: epoch 204: training loss 0.0719\n",
      "2025-05-29 21:01:26 [INFO]: epoch 205: training loss 0.0668\n",
      "2025-05-29 21:01:26 [INFO]: epoch 206: training loss 0.0715\n",
      "2025-05-29 21:01:26 [INFO]: epoch 207: training loss 0.0701\n",
      "2025-05-29 21:01:26 [INFO]: epoch 208: training loss 0.0711\n",
      "2025-05-29 21:01:26 [INFO]: epoch 209: training loss 0.0582\n",
      "2025-05-29 21:01:26 [INFO]: epoch 210: training loss 0.0654\n",
      "2025-05-29 21:01:26 [INFO]: epoch 211: training loss 0.0715\n",
      "2025-05-29 21:01:26 [INFO]: epoch 212: training loss 0.0678\n",
      "2025-05-29 21:01:26 [INFO]: epoch 213: training loss 0.0645\n",
      "2025-05-29 21:01:26 [INFO]: epoch 214: training loss 0.0733\n",
      "2025-05-29 21:01:26 [INFO]: epoch 215: training loss 0.0688\n",
      "2025-05-29 21:01:26 [INFO]: epoch 216: training loss 0.0720\n",
      "2025-05-29 21:01:26 [INFO]: epoch 217: training loss 0.0597\n",
      "2025-05-29 21:01:26 [INFO]: epoch 218: training loss 0.0656\n",
      "2025-05-29 21:01:26 [INFO]: epoch 219: training loss 0.0625\n",
      "2025-05-29 21:01:26 [INFO]: epoch 220: training loss 0.0691\n",
      "2025-05-29 21:01:26 [INFO]: epoch 221: training loss 0.0609\n",
      "2025-05-29 21:01:26 [INFO]: epoch 222: training loss 0.0627\n",
      "2025-05-29 21:01:26 [INFO]: epoch 223: training loss 0.0730\n",
      "2025-05-29 21:01:26 [INFO]: epoch 224: training loss 0.0680\n",
      "2025-05-29 21:01:26 [INFO]: epoch 225: training loss 0.0563\n",
      "2025-05-29 21:01:26 [INFO]: epoch 226: training loss 0.0619\n",
      "2025-05-29 21:01:26 [INFO]: epoch 227: training loss 0.0720\n",
      "2025-05-29 21:01:26 [INFO]: epoch 228: training loss 0.0708\n",
      "2025-05-29 21:01:26 [INFO]: epoch 229: training loss 0.0707\n",
      "2025-05-29 21:01:26 [INFO]: epoch 230: training loss 0.0551\n",
      "2025-05-29 21:01:26 [INFO]: epoch 231: training loss 0.0669\n",
      "2025-05-29 21:01:26 [INFO]: epoch 232: training loss 0.0658\n",
      "2025-05-29 21:01:26 [INFO]: epoch 233: training loss 0.0590\n",
      "2025-05-29 21:01:26 [INFO]: epoch 234: training loss 0.0596\n",
      "2025-05-29 21:01:26 [INFO]: epoch 235: training loss 0.0569\n",
      "2025-05-29 21:01:26 [INFO]: epoch 236: training loss 0.0617\n",
      "2025-05-29 21:01:26 [INFO]: epoch 237: training loss 0.0687\n",
      "2025-05-29 21:01:26 [INFO]: epoch 238: training loss 0.0662\n",
      "2025-05-29 21:01:26 [INFO]: epoch 239: training loss 0.0645\n",
      "2025-05-29 21:01:26 [INFO]: epoch 240: training loss 0.0608\n",
      "2025-05-29 21:01:26 [INFO]: epoch 241: training loss 0.0653\n",
      "2025-05-29 21:01:26 [INFO]: epoch 242: training loss 0.0675\n",
      "2025-05-29 21:01:26 [INFO]: epoch 243: training loss 0.0593\n",
      "2025-05-29 21:01:26 [INFO]: epoch 244: training loss 0.0587\n",
      "2025-05-29 21:01:27 [INFO]: epoch 245: training loss 0.0622\n",
      "2025-05-29 21:01:27 [INFO]: epoch 246: training loss 0.0598\n",
      "2025-05-29 21:01:27 [INFO]: epoch 247: training loss 0.0640\n",
      "2025-05-29 21:01:27 [INFO]: epoch 248: training loss 0.0502\n",
      "2025-05-29 21:01:27 [INFO]: epoch 249: training loss 0.0666\n",
      "2025-05-29 21:01:27 [INFO]: epoch 250: training loss 0.0549\n",
      "2025-05-29 21:01:27 [INFO]: epoch 251: training loss 0.0605\n",
      "2025-05-29 21:01:27 [INFO]: epoch 252: training loss 0.0688\n",
      "2025-05-29 21:01:27 [INFO]: epoch 253: training loss 0.0560\n",
      "2025-05-29 21:01:27 [INFO]: epoch 254: training loss 0.0665\n",
      "2025-05-29 21:01:27 [INFO]: epoch 255: training loss 0.0670\n",
      "2025-05-29 21:01:27 [INFO]: epoch 256: training loss 0.0570\n",
      "2025-05-29 21:01:27 [INFO]: epoch 257: training loss 0.0635\n",
      "2025-05-29 21:01:27 [INFO]: epoch 258: training loss 0.0640\n",
      "2025-05-29 21:01:27 [INFO]: epoch 259: training loss 0.0553\n",
      "2025-05-29 21:01:27 [INFO]: epoch 260: training loss 0.0695\n",
      "2025-05-29 21:01:27 [INFO]: epoch 261: training loss 0.0671\n",
      "2025-05-29 21:01:27 [INFO]: epoch 262: training loss 0.0552\n",
      "2025-05-29 21:01:27 [INFO]: epoch 263: training loss 0.0736\n",
      "2025-05-29 21:01:27 [INFO]: epoch 264: training loss 0.0657\n",
      "2025-05-29 21:01:27 [INFO]: epoch 265: training loss 0.0621\n",
      "2025-05-29 21:01:27 [INFO]: epoch 266: training loss 0.0745\n",
      "2025-05-29 21:01:27 [INFO]: epoch 267: training loss 0.0774\n",
      "2025-05-29 21:01:27 [INFO]: epoch 268: training loss 0.0571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:27 [INFO]: epoch 269: training loss 0.0639\n",
      "2025-05-29 21:01:27 [INFO]: epoch 270: training loss 0.0736\n",
      "2025-05-29 21:01:27 [INFO]: epoch 271: training loss 0.0558\n",
      "2025-05-29 21:01:27 [INFO]: epoch 272: training loss 0.0534\n",
      "2025-05-29 21:01:27 [INFO]: epoch 273: training loss 0.0645\n",
      "2025-05-29 21:01:27 [INFO]: epoch 274: training loss 0.0530\n",
      "2025-05-29 21:01:27 [INFO]: epoch 275: training loss 0.0598\n",
      "2025-05-29 21:01:27 [INFO]: epoch 276: training loss 0.0722\n",
      "2025-05-29 21:01:27 [INFO]: epoch 277: training loss 0.0634\n",
      "2025-05-29 21:01:27 [INFO]: epoch 278: training loss 0.0540\n",
      "2025-05-29 21:01:27 [INFO]: epoch 279: training loss 0.0674\n",
      "2025-05-29 21:01:27 [INFO]: epoch 280: training loss 0.0673\n",
      "2025-05-29 21:01:27 [INFO]: epoch 281: training loss 0.0714\n",
      "2025-05-29 21:01:27 [INFO]: epoch 282: training loss 0.0657\n",
      "2025-05-29 21:01:27 [INFO]: epoch 283: training loss 0.0489\n",
      "2025-05-29 21:01:27 [INFO]: epoch 284: training loss 0.0516\n",
      "2025-05-29 21:01:27 [INFO]: epoch 285: training loss 0.0547\n",
      "2025-05-29 21:01:27 [INFO]: epoch 286: training loss 0.0635\n",
      "2025-05-29 21:01:27 [INFO]: epoch 287: training loss 0.0533\n",
      "2025-05-29 21:01:27 [INFO]: epoch 288: training loss 0.0581\n",
      "2025-05-29 21:01:27 [INFO]: epoch 289: training loss 0.0576\n",
      "2025-05-29 21:01:27 [INFO]: epoch 290: training loss 0.0553\n",
      "2025-05-29 21:01:27 [INFO]: epoch 291: training loss 0.0611\n",
      "2025-05-29 21:01:27 [INFO]: epoch 292: training loss 0.0625\n",
      "2025-05-29 21:01:27 [INFO]: epoch 293: training loss 0.0573\n",
      "2025-05-29 21:01:27 [INFO]: epoch 294: training loss 0.0510\n",
      "2025-05-29 21:01:27 [INFO]: epoch 295: training loss 0.0437\n",
      "2025-05-29 21:01:27 [INFO]: epoch 296: training loss 0.0681\n",
      "2025-05-29 21:01:27 [INFO]: epoch 297: training loss 0.0674\n",
      "2025-05-29 21:01:27 [INFO]: epoch 298: training loss 0.0545\n",
      "2025-05-29 21:01:27 [INFO]: epoch 299: training loss 0.0610\n",
      "2025-05-29 21:01:27 [INFO]: Finished training.\n",
      "2025-05-29 21:01:27 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 20%|████████████████▊                                                                   | 1/5 [00:03<00:15,  3.95s/it]2025-05-29 21:01:27 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:01:27 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:01:27 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:01:27 [INFO]: epoch 0: training loss 0.8358\n",
      "2025-05-29 21:01:27 [INFO]: epoch 1: training loss 0.9176\n",
      "2025-05-29 21:01:27 [INFO]: epoch 2: training loss 0.6208\n",
      "2025-05-29 21:01:27 [INFO]: epoch 3: training loss 0.4952\n",
      "2025-05-29 21:01:27 [INFO]: epoch 4: training loss 0.5328\n",
      "2025-05-29 21:01:27 [INFO]: epoch 5: training loss 0.4727\n",
      "2025-05-29 21:01:27 [INFO]: epoch 6: training loss 0.4091\n",
      "2025-05-29 21:01:27 [INFO]: epoch 7: training loss 0.4216\n",
      "2025-05-29 21:01:27 [INFO]: epoch 8: training loss 0.5124\n",
      "2025-05-29 21:01:27 [INFO]: epoch 9: training loss 0.4401\n",
      "2025-05-29 21:01:27 [INFO]: epoch 10: training loss 0.3696\n",
      "2025-05-29 21:01:27 [INFO]: epoch 11: training loss 0.3367\n",
      "2025-05-29 21:01:27 [INFO]: epoch 12: training loss 0.3316\n",
      "2025-05-29 21:01:27 [INFO]: epoch 13: training loss 0.3893\n",
      "2025-05-29 21:01:27 [INFO]: epoch 14: training loss 0.3899\n",
      "2025-05-29 21:01:27 [INFO]: epoch 15: training loss 0.4116\n",
      "2025-05-29 21:01:27 [INFO]: epoch 16: training loss 0.3427\n",
      "2025-05-29 21:01:27 [INFO]: epoch 17: training loss 0.2878\n",
      "2025-05-29 21:01:28 [INFO]: epoch 18: training loss 0.3175\n",
      "2025-05-29 21:01:28 [INFO]: epoch 19: training loss 0.3270\n",
      "2025-05-29 21:01:28 [INFO]: epoch 20: training loss 0.2892\n",
      "2025-05-29 21:01:28 [INFO]: epoch 21: training loss 0.2922\n",
      "2025-05-29 21:01:28 [INFO]: epoch 22: training loss 0.2944\n",
      "2025-05-29 21:01:28 [INFO]: epoch 23: training loss 0.2819\n",
      "2025-05-29 21:01:28 [INFO]: epoch 24: training loss 0.2842\n",
      "2025-05-29 21:01:28 [INFO]: epoch 25: training loss 0.2732\n",
      "2025-05-29 21:01:28 [INFO]: epoch 26: training loss 0.2521\n",
      "2025-05-29 21:01:28 [INFO]: epoch 27: training loss 0.2655\n",
      "2025-05-29 21:01:28 [INFO]: epoch 28: training loss 0.2657\n",
      "2025-05-29 21:01:28 [INFO]: epoch 29: training loss 0.2907\n",
      "2025-05-29 21:01:28 [INFO]: epoch 30: training loss 0.2557\n",
      "2025-05-29 21:01:28 [INFO]: epoch 31: training loss 0.2675\n",
      "2025-05-29 21:01:28 [INFO]: epoch 32: training loss 0.2290\n",
      "2025-05-29 21:01:28 [INFO]: epoch 33: training loss 0.2280\n",
      "2025-05-29 21:01:28 [INFO]: epoch 34: training loss 0.2247\n",
      "2025-05-29 21:01:28 [INFO]: epoch 35: training loss 0.2202\n",
      "2025-05-29 21:01:28 [INFO]: epoch 36: training loss 0.2362\n",
      "2025-05-29 21:01:28 [INFO]: epoch 37: training loss 0.2397\n",
      "2025-05-29 21:01:28 [INFO]: epoch 38: training loss 0.2328\n",
      "2025-05-29 21:01:28 [INFO]: epoch 39: training loss 0.2132\n",
      "2025-05-29 21:01:28 [INFO]: epoch 40: training loss 0.2197\n",
      "2025-05-29 21:01:28 [INFO]: epoch 41: training loss 0.2324\n",
      "2025-05-29 21:01:28 [INFO]: epoch 42: training loss 0.2291\n",
      "2025-05-29 21:01:28 [INFO]: epoch 43: training loss 0.2471\n",
      "2025-05-29 21:01:28 [INFO]: epoch 44: training loss 0.2070\n",
      "2025-05-29 21:01:28 [INFO]: epoch 45: training loss 0.2134\n",
      "2025-05-29 21:01:28 [INFO]: epoch 46: training loss 0.2051\n",
      "2025-05-29 21:01:28 [INFO]: epoch 47: training loss 0.2077\n",
      "2025-05-29 21:01:28 [INFO]: epoch 48: training loss 0.2168\n",
      "2025-05-29 21:01:28 [INFO]: epoch 49: training loss 0.2053\n",
      "2025-05-29 21:01:28 [INFO]: epoch 50: training loss 0.1978\n",
      "2025-05-29 21:01:28 [INFO]: epoch 51: training loss 0.1890\n",
      "2025-05-29 21:01:28 [INFO]: epoch 52: training loss 0.2308\n",
      "2025-05-29 21:01:28 [INFO]: epoch 53: training loss 0.2189\n",
      "2025-05-29 21:01:28 [INFO]: epoch 54: training loss 0.2033\n",
      "2025-05-29 21:01:28 [INFO]: epoch 55: training loss 0.1896\n",
      "2025-05-29 21:01:28 [INFO]: epoch 56: training loss 0.2060\n",
      "2025-05-29 21:01:28 [INFO]: epoch 57: training loss 0.1821\n",
      "2025-05-29 21:01:28 [INFO]: epoch 58: training loss 0.2062\n",
      "2025-05-29 21:01:28 [INFO]: epoch 59: training loss 0.2026\n",
      "2025-05-29 21:01:28 [INFO]: epoch 60: training loss 0.1764\n",
      "2025-05-29 21:01:28 [INFO]: epoch 61: training loss 0.1603\n",
      "2025-05-29 21:01:28 [INFO]: epoch 62: training loss 0.1873\n",
      "2025-05-29 21:01:28 [INFO]: epoch 63: training loss 0.1882\n",
      "2025-05-29 21:01:28 [INFO]: epoch 64: training loss 0.1867\n",
      "2025-05-29 21:01:28 [INFO]: epoch 65: training loss 0.1860\n",
      "2025-05-29 21:01:28 [INFO]: epoch 66: training loss 0.1738\n",
      "2025-05-29 21:01:28 [INFO]: epoch 67: training loss 0.1800\n",
      "2025-05-29 21:01:28 [INFO]: epoch 68: training loss 0.1799\n",
      "2025-05-29 21:01:28 [INFO]: epoch 69: training loss 0.1857\n",
      "2025-05-29 21:01:28 [INFO]: epoch 70: training loss 0.1838\n",
      "2025-05-29 21:01:28 [INFO]: epoch 71: training loss 0.1862\n",
      "2025-05-29 21:01:28 [INFO]: epoch 72: training loss 0.1626\n",
      "2025-05-29 21:01:28 [INFO]: epoch 73: training loss 0.1700\n",
      "2025-05-29 21:01:28 [INFO]: epoch 74: training loss 0.1979\n",
      "2025-05-29 21:01:28 [INFO]: epoch 75: training loss 0.1695\n",
      "2025-05-29 21:01:28 [INFO]: epoch 76: training loss 0.1748\n",
      "2025-05-29 21:01:28 [INFO]: epoch 77: training loss 0.1621\n",
      "2025-05-29 21:01:28 [INFO]: epoch 78: training loss 0.1737\n",
      "2025-05-29 21:01:28 [INFO]: epoch 79: training loss 0.1958\n",
      "2025-05-29 21:01:28 [INFO]: epoch 80: training loss 0.1822\n",
      "2025-05-29 21:01:28 [INFO]: epoch 81: training loss 0.1692\n",
      "2025-05-29 21:01:28 [INFO]: epoch 82: training loss 0.1787\n",
      "2025-05-29 21:01:28 [INFO]: epoch 83: training loss 0.1637\n",
      "2025-05-29 21:01:28 [INFO]: epoch 84: training loss 0.1921\n",
      "2025-05-29 21:01:28 [INFO]: epoch 85: training loss 0.1909\n",
      "2025-05-29 21:01:28 [INFO]: epoch 86: training loss 0.1609\n",
      "2025-05-29 21:01:28 [INFO]: epoch 87: training loss 0.1803\n",
      "2025-05-29 21:01:28 [INFO]: epoch 88: training loss 0.1784\n",
      "2025-05-29 21:01:28 [INFO]: epoch 89: training loss 0.1816\n",
      "2025-05-29 21:01:28 [INFO]: epoch 90: training loss 0.1694\n",
      "2025-05-29 21:01:28 [INFO]: epoch 91: training loss 0.1756\n",
      "2025-05-29 21:01:28 [INFO]: epoch 92: training loss 0.1377\n",
      "2025-05-29 21:01:28 [INFO]: epoch 93: training loss 0.1869\n",
      "2025-05-29 21:01:28 [INFO]: epoch 94: training loss 0.1939\n",
      "2025-05-29 21:01:28 [INFO]: epoch 95: training loss 0.1551\n",
      "2025-05-29 21:01:28 [INFO]: epoch 96: training loss 0.1588\n",
      "2025-05-29 21:01:29 [INFO]: epoch 97: training loss 0.1942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:29 [INFO]: epoch 98: training loss 0.1595\n",
      "2025-05-29 21:01:29 [INFO]: epoch 99: training loss 0.1500\n",
      "2025-05-29 21:01:29 [INFO]: epoch 100: training loss 0.1487\n",
      "2025-05-29 21:01:29 [INFO]: epoch 101: training loss 0.1556\n",
      "2025-05-29 21:01:29 [INFO]: epoch 102: training loss 0.1502\n",
      "2025-05-29 21:01:29 [INFO]: epoch 103: training loss 0.1357\n",
      "2025-05-29 21:01:29 [INFO]: epoch 104: training loss 0.1354\n",
      "2025-05-29 21:01:29 [INFO]: epoch 105: training loss 0.1460\n",
      "2025-05-29 21:01:29 [INFO]: epoch 106: training loss 0.1544\n",
      "2025-05-29 21:01:29 [INFO]: epoch 107: training loss 0.1313\n",
      "2025-05-29 21:01:29 [INFO]: epoch 108: training loss 0.1538\n",
      "2025-05-29 21:01:29 [INFO]: epoch 109: training loss 0.1610\n",
      "2025-05-29 21:01:29 [INFO]: epoch 110: training loss 0.1390\n",
      "2025-05-29 21:01:29 [INFO]: epoch 111: training loss 0.1448\n",
      "2025-05-29 21:01:29 [INFO]: epoch 112: training loss 0.1491\n",
      "2025-05-29 21:01:29 [INFO]: epoch 113: training loss 0.1586\n",
      "2025-05-29 21:01:29 [INFO]: epoch 114: training loss 0.1292\n",
      "2025-05-29 21:01:29 [INFO]: epoch 115: training loss 0.1659\n",
      "2025-05-29 21:01:29 [INFO]: epoch 116: training loss 0.1693\n",
      "2025-05-29 21:01:29 [INFO]: epoch 117: training loss 0.1566\n",
      "2025-05-29 21:01:29 [INFO]: epoch 118: training loss 0.1454\n",
      "2025-05-29 21:01:29 [INFO]: epoch 119: training loss 0.1439\n",
      "2025-05-29 21:01:29 [INFO]: epoch 120: training loss 0.1664\n",
      "2025-05-29 21:01:29 [INFO]: epoch 121: training loss 0.1661\n",
      "2025-05-29 21:01:29 [INFO]: epoch 122: training loss 0.1304\n",
      "2025-05-29 21:01:29 [INFO]: epoch 123: training loss 0.1271\n",
      "2025-05-29 21:01:29 [INFO]: epoch 124: training loss 0.1439\n",
      "2025-05-29 21:01:29 [INFO]: epoch 125: training loss 0.1555\n",
      "2025-05-29 21:01:29 [INFO]: epoch 126: training loss 0.1300\n",
      "2025-05-29 21:01:29 [INFO]: epoch 127: training loss 0.1262\n",
      "2025-05-29 21:01:29 [INFO]: epoch 128: training loss 0.1331\n",
      "2025-05-29 21:01:29 [INFO]: epoch 129: training loss 0.1345\n",
      "2025-05-29 21:01:29 [INFO]: epoch 130: training loss 0.1368\n",
      "2025-05-29 21:01:29 [INFO]: epoch 131: training loss 0.1334\n",
      "2025-05-29 21:01:29 [INFO]: epoch 132: training loss 0.1181\n",
      "2025-05-29 21:01:29 [INFO]: epoch 133: training loss 0.1418\n",
      "2025-05-29 21:01:29 [INFO]: epoch 134: training loss 0.1291\n",
      "2025-05-29 21:01:29 [INFO]: epoch 135: training loss 0.1346\n",
      "2025-05-29 21:01:29 [INFO]: epoch 136: training loss 0.1174\n",
      "2025-05-29 21:01:29 [INFO]: epoch 137: training loss 0.1309\n",
      "2025-05-29 21:01:29 [INFO]: epoch 138: training loss 0.1273\n",
      "2025-05-29 21:01:29 [INFO]: epoch 139: training loss 0.1364\n",
      "2025-05-29 21:01:29 [INFO]: epoch 140: training loss 0.1181\n",
      "2025-05-29 21:01:29 [INFO]: epoch 141: training loss 0.1119\n",
      "2025-05-29 21:01:29 [INFO]: epoch 142: training loss 0.1225\n",
      "2025-05-29 21:01:29 [INFO]: epoch 143: training loss 0.1152\n",
      "2025-05-29 21:01:29 [INFO]: epoch 144: training loss 0.1195\n",
      "2025-05-29 21:01:29 [INFO]: epoch 145: training loss 0.1221\n",
      "2025-05-29 21:01:29 [INFO]: epoch 146: training loss 0.1007\n",
      "2025-05-29 21:01:29 [INFO]: epoch 147: training loss 0.1199\n",
      "2025-05-29 21:01:29 [INFO]: epoch 148: training loss 0.1304\n",
      "2025-05-29 21:01:29 [INFO]: epoch 149: training loss 0.1197\n",
      "2025-05-29 21:01:29 [INFO]: epoch 150: training loss 0.1169\n",
      "2025-05-29 21:01:29 [INFO]: epoch 151: training loss 0.1035\n",
      "2025-05-29 21:01:29 [INFO]: epoch 152: training loss 0.1225\n",
      "2025-05-29 21:01:29 [INFO]: epoch 153: training loss 0.1208\n",
      "2025-05-29 21:01:29 [INFO]: epoch 154: training loss 0.1055\n",
      "2025-05-29 21:01:29 [INFO]: epoch 155: training loss 0.1094\n",
      "2025-05-29 21:01:29 [INFO]: epoch 156: training loss 0.1082\n",
      "2025-05-29 21:01:29 [INFO]: epoch 157: training loss 0.0976\n",
      "2025-05-29 21:01:29 [INFO]: epoch 158: training loss 0.1138\n",
      "2025-05-29 21:01:29 [INFO]: epoch 159: training loss 0.1169\n",
      "2025-05-29 21:01:29 [INFO]: epoch 160: training loss 0.1086\n",
      "2025-05-29 21:01:29 [INFO]: epoch 161: training loss 0.1033\n",
      "2025-05-29 21:01:29 [INFO]: epoch 162: training loss 0.1034\n",
      "2025-05-29 21:01:29 [INFO]: epoch 163: training loss 0.1093\n",
      "2025-05-29 21:01:29 [INFO]: epoch 164: training loss 0.0946\n",
      "2025-05-29 21:01:29 [INFO]: epoch 165: training loss 0.1022\n",
      "2025-05-29 21:01:29 [INFO]: epoch 166: training loss 0.1000\n",
      "2025-05-29 21:01:29 [INFO]: epoch 167: training loss 0.1093\n",
      "2025-05-29 21:01:29 [INFO]: epoch 168: training loss 0.1007\n",
      "2025-05-29 21:01:29 [INFO]: epoch 169: training loss 0.1151\n",
      "2025-05-29 21:01:29 [INFO]: epoch 170: training loss 0.1101\n",
      "2025-05-29 21:01:29 [INFO]: epoch 171: training loss 0.0916\n",
      "2025-05-29 21:01:29 [INFO]: epoch 172: training loss 0.1010\n",
      "2025-05-29 21:01:29 [INFO]: epoch 173: training loss 0.0963\n",
      "2025-05-29 21:01:29 [INFO]: epoch 174: training loss 0.0970\n",
      "2025-05-29 21:01:29 [INFO]: epoch 175: training loss 0.0910\n",
      "2025-05-29 21:01:30 [INFO]: epoch 176: training loss 0.0932\n",
      "2025-05-29 21:01:30 [INFO]: epoch 177: training loss 0.0934\n",
      "2025-05-29 21:01:30 [INFO]: epoch 178: training loss 0.0912\n",
      "2025-05-29 21:01:30 [INFO]: epoch 179: training loss 0.0872\n",
      "2025-05-29 21:01:30 [INFO]: epoch 180: training loss 0.0880\n",
      "2025-05-29 21:01:30 [INFO]: epoch 181: training loss 0.0867\n",
      "2025-05-29 21:01:30 [INFO]: epoch 182: training loss 0.0838\n",
      "2025-05-29 21:01:30 [INFO]: epoch 183: training loss 0.0948\n",
      "2025-05-29 21:01:30 [INFO]: epoch 184: training loss 0.0826\n",
      "2025-05-29 21:01:30 [INFO]: epoch 185: training loss 0.0914\n",
      "2025-05-29 21:01:30 [INFO]: epoch 186: training loss 0.0926\n",
      "2025-05-29 21:01:30 [INFO]: epoch 187: training loss 0.1070\n",
      "2025-05-29 21:01:30 [INFO]: epoch 188: training loss 0.0955\n",
      "2025-05-29 21:01:30 [INFO]: epoch 189: training loss 0.0893\n",
      "2025-05-29 21:01:30 [INFO]: epoch 190: training loss 0.0955\n",
      "2025-05-29 21:01:30 [INFO]: epoch 191: training loss 0.0891\n",
      "2025-05-29 21:01:30 [INFO]: epoch 192: training loss 0.0865\n",
      "2025-05-29 21:01:30 [INFO]: epoch 193: training loss 0.0816\n",
      "2025-05-29 21:01:30 [INFO]: epoch 194: training loss 0.0884\n",
      "2025-05-29 21:01:30 [INFO]: epoch 195: training loss 0.0813\n",
      "2025-05-29 21:01:30 [INFO]: epoch 196: training loss 0.0866\n",
      "2025-05-29 21:01:30 [INFO]: epoch 197: training loss 0.0837\n",
      "2025-05-29 21:01:30 [INFO]: epoch 198: training loss 0.0918\n",
      "2025-05-29 21:01:30 [INFO]: epoch 199: training loss 0.0912\n",
      "2025-05-29 21:01:30 [INFO]: epoch 200: training loss 0.0780\n",
      "2025-05-29 21:01:30 [INFO]: epoch 201: training loss 0.0877\n",
      "2025-05-29 21:01:30 [INFO]: epoch 202: training loss 0.0722\n",
      "2025-05-29 21:01:30 [INFO]: epoch 203: training loss 0.0884\n",
      "2025-05-29 21:01:30 [INFO]: epoch 204: training loss 0.0711\n",
      "2025-05-29 21:01:30 [INFO]: epoch 205: training loss 0.0708\n",
      "2025-05-29 21:01:30 [INFO]: epoch 206: training loss 0.0709\n",
      "2025-05-29 21:01:30 [INFO]: epoch 207: training loss 0.0914\n",
      "2025-05-29 21:01:30 [INFO]: epoch 208: training loss 0.0765\n",
      "2025-05-29 21:01:30 [INFO]: epoch 209: training loss 0.0748\n",
      "2025-05-29 21:01:30 [INFO]: epoch 210: training loss 0.0869\n",
      "2025-05-29 21:01:30 [INFO]: epoch 211: training loss 0.0924\n",
      "2025-05-29 21:01:30 [INFO]: epoch 212: training loss 0.0894\n",
      "2025-05-29 21:01:30 [INFO]: epoch 213: training loss 0.1095\n",
      "2025-05-29 21:01:30 [INFO]: epoch 214: training loss 0.0795\n",
      "2025-05-29 21:01:30 [INFO]: epoch 215: training loss 0.0996\n",
      "2025-05-29 21:01:30 [INFO]: epoch 216: training loss 0.1106\n",
      "2025-05-29 21:01:30 [INFO]: epoch 217: training loss 0.0895\n",
      "2025-05-29 21:01:30 [INFO]: epoch 218: training loss 0.0880\n",
      "2025-05-29 21:01:30 [INFO]: epoch 219: training loss 0.0797\n",
      "2025-05-29 21:01:30 [INFO]: epoch 220: training loss 0.0780\n",
      "2025-05-29 21:01:30 [INFO]: epoch 221: training loss 0.0750\n",
      "2025-05-29 21:01:30 [INFO]: epoch 222: training loss 0.0962\n",
      "2025-05-29 21:01:30 [INFO]: epoch 223: training loss 0.0761\n",
      "2025-05-29 21:01:30 [INFO]: epoch 224: training loss 0.0867\n",
      "2025-05-29 21:01:30 [INFO]: epoch 225: training loss 0.0793\n",
      "2025-05-29 21:01:30 [INFO]: epoch 226: training loss 0.0700\n",
      "2025-05-29 21:01:30 [INFO]: epoch 227: training loss 0.0828\n",
      "2025-05-29 21:01:30 [INFO]: epoch 228: training loss 0.0748\n",
      "2025-05-29 21:01:30 [INFO]: epoch 229: training loss 0.0677\n",
      "2025-05-29 21:01:30 [INFO]: epoch 230: training loss 0.0714\n",
      "2025-05-29 21:01:30 [INFO]: epoch 231: training loss 0.0746\n",
      "2025-05-29 21:01:30 [INFO]: epoch 232: training loss 0.0656\n",
      "2025-05-29 21:01:30 [INFO]: epoch 233: training loss 0.0721\n",
      "2025-05-29 21:01:30 [INFO]: epoch 234: training loss 0.0877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:30 [INFO]: epoch 235: training loss 0.0655\n",
      "2025-05-29 21:01:30 [INFO]: epoch 236: training loss 0.0672\n",
      "2025-05-29 21:01:30 [INFO]: epoch 237: training loss 0.0704\n",
      "2025-05-29 21:01:30 [INFO]: epoch 238: training loss 0.0624\n",
      "2025-05-29 21:01:30 [INFO]: epoch 239: training loss 0.0743\n",
      "2025-05-29 21:01:30 [INFO]: epoch 240: training loss 0.0640\n",
      "2025-05-29 21:01:30 [INFO]: epoch 241: training loss 0.0622\n",
      "2025-05-29 21:01:30 [INFO]: epoch 242: training loss 0.0767\n",
      "2025-05-29 21:01:30 [INFO]: epoch 243: training loss 0.0682\n",
      "2025-05-29 21:01:30 [INFO]: epoch 244: training loss 0.0669\n",
      "2025-05-29 21:01:30 [INFO]: epoch 245: training loss 0.0718\n",
      "2025-05-29 21:01:30 [INFO]: epoch 246: training loss 0.0666\n",
      "2025-05-29 21:01:30 [INFO]: epoch 247: training loss 0.0612\n",
      "2025-05-29 21:01:30 [INFO]: epoch 248: training loss 0.0662\n",
      "2025-05-29 21:01:30 [INFO]: epoch 249: training loss 0.0589\n",
      "2025-05-29 21:01:30 [INFO]: epoch 250: training loss 0.0592\n",
      "2025-05-29 21:01:30 [INFO]: epoch 251: training loss 0.0654\n",
      "2025-05-29 21:01:30 [INFO]: epoch 252: training loss 0.0597\n",
      "2025-05-29 21:01:30 [INFO]: epoch 253: training loss 0.0619\n",
      "2025-05-29 21:01:30 [INFO]: epoch 254: training loss 0.0565\n",
      "2025-05-29 21:01:30 [INFO]: epoch 255: training loss 0.0591\n",
      "2025-05-29 21:01:31 [INFO]: epoch 256: training loss 0.0704\n",
      "2025-05-29 21:01:31 [INFO]: epoch 257: training loss 0.0609\n",
      "2025-05-29 21:01:31 [INFO]: epoch 258: training loss 0.0611\n",
      "2025-05-29 21:01:31 [INFO]: epoch 259: training loss 0.0530\n",
      "2025-05-29 21:01:31 [INFO]: epoch 260: training loss 0.0653\n",
      "2025-05-29 21:01:31 [INFO]: epoch 261: training loss 0.0571\n",
      "2025-05-29 21:01:31 [INFO]: epoch 262: training loss 0.0573\n",
      "2025-05-29 21:01:31 [INFO]: epoch 263: training loss 0.0552\n",
      "2025-05-29 21:01:31 [INFO]: epoch 264: training loss 0.0490\n",
      "2025-05-29 21:01:31 [INFO]: epoch 265: training loss 0.0577\n",
      "2025-05-29 21:01:31 [INFO]: epoch 266: training loss 0.0790\n",
      "2025-05-29 21:01:31 [INFO]: epoch 267: training loss 0.0590\n",
      "2025-05-29 21:01:31 [INFO]: epoch 268: training loss 0.0572\n",
      "2025-05-29 21:01:31 [INFO]: epoch 269: training loss 0.0603\n",
      "2025-05-29 21:01:31 [INFO]: epoch 270: training loss 0.0674\n",
      "2025-05-29 21:01:31 [INFO]: epoch 271: training loss 0.0529\n",
      "2025-05-29 21:01:31 [INFO]: epoch 272: training loss 0.0619\n",
      "2025-05-29 21:01:31 [INFO]: epoch 273: training loss 0.0614\n",
      "2025-05-29 21:01:31 [INFO]: epoch 274: training loss 0.0674\n",
      "2025-05-29 21:01:31 [INFO]: epoch 275: training loss 0.0723\n",
      "2025-05-29 21:01:31 [INFO]: epoch 276: training loss 0.0541\n",
      "2025-05-29 21:01:31 [INFO]: epoch 277: training loss 0.0664\n",
      "2025-05-29 21:01:31 [INFO]: epoch 278: training loss 0.0561\n",
      "2025-05-29 21:01:31 [INFO]: epoch 279: training loss 0.0578\n",
      "2025-05-29 21:01:31 [INFO]: epoch 280: training loss 0.0632\n",
      "2025-05-29 21:01:31 [INFO]: epoch 281: training loss 0.0609\n",
      "2025-05-29 21:01:31 [INFO]: epoch 282: training loss 0.0564\n",
      "2025-05-29 21:01:31 [INFO]: epoch 283: training loss 0.0624\n",
      "2025-05-29 21:01:31 [INFO]: epoch 284: training loss 0.0558\n",
      "2025-05-29 21:01:31 [INFO]: epoch 285: training loss 0.0570\n",
      "2025-05-29 21:01:31 [INFO]: epoch 286: training loss 0.0602\n",
      "2025-05-29 21:01:31 [INFO]: epoch 287: training loss 0.0596\n",
      "2025-05-29 21:01:31 [INFO]: epoch 288: training loss 0.0557\n",
      "2025-05-29 21:01:31 [INFO]: epoch 289: training loss 0.0612\n",
      "2025-05-29 21:01:31 [INFO]: epoch 290: training loss 0.0632\n",
      "2025-05-29 21:01:31 [INFO]: epoch 291: training loss 0.0537\n",
      "2025-05-29 21:01:31 [INFO]: epoch 292: training loss 0.0501\n",
      "2025-05-29 21:01:31 [INFO]: epoch 293: training loss 0.0549\n",
      "2025-05-29 21:01:31 [INFO]: epoch 294: training loss 0.0542\n",
      "2025-05-29 21:01:31 [INFO]: epoch 295: training loss 0.0561\n",
      "2025-05-29 21:01:31 [INFO]: epoch 296: training loss 0.0525\n",
      "2025-05-29 21:01:31 [INFO]: epoch 297: training loss 0.0522\n",
      "2025-05-29 21:01:31 [INFO]: epoch 298: training loss 0.0607\n",
      "2025-05-29 21:01:31 [INFO]: epoch 299: training loss 0.0482\n",
      "2025-05-29 21:01:31 [INFO]: Finished training.\n",
      "2025-05-29 21:01:31 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:07<00:11,  3.90s/it]2025-05-29 21:01:31 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:01:31 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:01:31 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:01:31 [INFO]: epoch 0: training loss 1.3747\n",
      "2025-05-29 21:01:31 [INFO]: epoch 1: training loss 0.5792\n",
      "2025-05-29 21:01:31 [INFO]: epoch 2: training loss 0.7564\n",
      "2025-05-29 21:01:31 [INFO]: epoch 3: training loss 0.7507\n",
      "2025-05-29 21:01:31 [INFO]: epoch 4: training loss 0.6263\n",
      "2025-05-29 21:01:31 [INFO]: epoch 5: training loss 0.4786\n",
      "2025-05-29 21:01:31 [INFO]: epoch 6: training loss 0.5012\n",
      "2025-05-29 21:01:31 [INFO]: epoch 7: training loss 0.5300\n",
      "2025-05-29 21:01:31 [INFO]: epoch 8: training loss 0.4835\n",
      "2025-05-29 21:01:31 [INFO]: epoch 9: training loss 0.4510\n",
      "2025-05-29 21:01:31 [INFO]: epoch 10: training loss 0.4799\n",
      "2025-05-29 21:01:31 [INFO]: epoch 11: training loss 0.4848\n",
      "2025-05-29 21:01:31 [INFO]: epoch 12: training loss 0.4066\n",
      "2025-05-29 21:01:31 [INFO]: epoch 13: training loss 0.4191\n",
      "2025-05-29 21:01:31 [INFO]: epoch 14: training loss 0.4442\n",
      "2025-05-29 21:01:31 [INFO]: epoch 15: training loss 0.4388\n",
      "2025-05-29 21:01:31 [INFO]: epoch 16: training loss 0.4316\n",
      "2025-05-29 21:01:31 [INFO]: epoch 17: training loss 0.3947\n",
      "2025-05-29 21:01:31 [INFO]: epoch 18: training loss 0.3887\n",
      "2025-05-29 21:01:31 [INFO]: epoch 19: training loss 0.3686\n",
      "2025-05-29 21:01:31 [INFO]: epoch 20: training loss 0.3445\n",
      "2025-05-29 21:01:31 [INFO]: epoch 21: training loss 0.3559\n",
      "2025-05-29 21:01:31 [INFO]: epoch 22: training loss 0.3540\n",
      "2025-05-29 21:01:31 [INFO]: epoch 23: training loss 0.3382\n",
      "2025-05-29 21:01:31 [INFO]: epoch 24: training loss 0.3369\n",
      "2025-05-29 21:01:31 [INFO]: epoch 25: training loss 0.3439\n",
      "2025-05-29 21:01:31 [INFO]: epoch 26: training loss 0.3447\n",
      "2025-05-29 21:01:31 [INFO]: epoch 27: training loss 0.3443\n",
      "2025-05-29 21:01:31 [INFO]: epoch 28: training loss 0.3165\n",
      "2025-05-29 21:01:31 [INFO]: epoch 29: training loss 0.3346\n",
      "2025-05-29 21:01:31 [INFO]: epoch 30: training loss 0.3239\n",
      "2025-05-29 21:01:32 [INFO]: epoch 31: training loss 0.3094\n",
      "2025-05-29 21:01:32 [INFO]: epoch 32: training loss 0.3048\n",
      "2025-05-29 21:01:32 [INFO]: epoch 33: training loss 0.2996\n",
      "2025-05-29 21:01:32 [INFO]: epoch 34: training loss 0.2922\n",
      "2025-05-29 21:01:32 [INFO]: epoch 35: training loss 0.3165\n",
      "2025-05-29 21:01:32 [INFO]: epoch 36: training loss 0.3108\n",
      "2025-05-29 21:01:32 [INFO]: epoch 37: training loss 0.2838\n",
      "2025-05-29 21:01:32 [INFO]: epoch 38: training loss 0.3075\n",
      "2025-05-29 21:01:32 [INFO]: epoch 39: training loss 0.3060\n",
      "2025-05-29 21:01:32 [INFO]: epoch 40: training loss 0.3057\n",
      "2025-05-29 21:01:32 [INFO]: epoch 41: training loss 0.2727\n",
      "2025-05-29 21:01:32 [INFO]: epoch 42: training loss 0.2703\n",
      "2025-05-29 21:01:32 [INFO]: epoch 43: training loss 0.2822\n",
      "2025-05-29 21:01:32 [INFO]: epoch 44: training loss 0.3149\n",
      "2025-05-29 21:01:32 [INFO]: epoch 45: training loss 0.2963\n",
      "2025-05-29 21:01:32 [INFO]: epoch 46: training loss 0.2877\n",
      "2025-05-29 21:01:32 [INFO]: epoch 47: training loss 0.2829\n",
      "2025-05-29 21:01:32 [INFO]: epoch 48: training loss 0.2972\n",
      "2025-05-29 21:01:32 [INFO]: epoch 49: training loss 0.2981\n",
      "2025-05-29 21:01:32 [INFO]: epoch 50: training loss 0.2880\n",
      "2025-05-29 21:01:32 [INFO]: epoch 51: training loss 0.2724\n",
      "2025-05-29 21:01:32 [INFO]: epoch 52: training loss 0.3040\n",
      "2025-05-29 21:01:32 [INFO]: epoch 53: training loss 0.2902\n",
      "2025-05-29 21:01:32 [INFO]: epoch 54: training loss 0.2798\n",
      "2025-05-29 21:01:32 [INFO]: epoch 55: training loss 0.2554\n",
      "2025-05-29 21:01:32 [INFO]: epoch 56: training loss 0.2507\n",
      "2025-05-29 21:01:32 [INFO]: epoch 57: training loss 0.2702\n",
      "2025-05-29 21:01:32 [INFO]: epoch 58: training loss 0.2844\n",
      "2025-05-29 21:01:32 [INFO]: epoch 59: training loss 0.2632\n",
      "2025-05-29 21:01:32 [INFO]: epoch 60: training loss 0.2555\n",
      "2025-05-29 21:01:32 [INFO]: epoch 61: training loss 0.2258\n",
      "2025-05-29 21:01:32 [INFO]: epoch 62: training loss 0.2341\n",
      "2025-05-29 21:01:32 [INFO]: epoch 63: training loss 0.2460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:32 [INFO]: epoch 64: training loss 0.2336\n",
      "2025-05-29 21:01:32 [INFO]: epoch 65: training loss 0.2230\n",
      "2025-05-29 21:01:32 [INFO]: epoch 66: training loss 0.2380\n",
      "2025-05-29 21:01:32 [INFO]: epoch 67: training loss 0.2390\n",
      "2025-05-29 21:01:32 [INFO]: epoch 68: training loss 0.2197\n",
      "2025-05-29 21:01:32 [INFO]: epoch 69: training loss 0.2183\n",
      "2025-05-29 21:01:32 [INFO]: epoch 70: training loss 0.2245\n",
      "2025-05-29 21:01:32 [INFO]: epoch 71: training loss 0.2019\n",
      "2025-05-29 21:01:32 [INFO]: epoch 72: training loss 0.1995\n",
      "2025-05-29 21:01:32 [INFO]: epoch 73: training loss 0.2035\n",
      "2025-05-29 21:01:32 [INFO]: epoch 74: training loss 0.2043\n",
      "2025-05-29 21:01:32 [INFO]: epoch 75: training loss 0.2277\n",
      "2025-05-29 21:01:32 [INFO]: epoch 76: training loss 0.2106\n",
      "2025-05-29 21:01:32 [INFO]: epoch 77: training loss 0.2405\n",
      "2025-05-29 21:01:32 [INFO]: epoch 78: training loss 0.2375\n",
      "2025-05-29 21:01:32 [INFO]: epoch 79: training loss 0.2104\n",
      "2025-05-29 21:01:32 [INFO]: epoch 80: training loss 0.2053\n",
      "2025-05-29 21:01:32 [INFO]: epoch 81: training loss 0.2082\n",
      "2025-05-29 21:01:32 [INFO]: epoch 82: training loss 0.2371\n",
      "2025-05-29 21:01:32 [INFO]: epoch 83: training loss 0.2047\n",
      "2025-05-29 21:01:32 [INFO]: epoch 84: training loss 0.2074\n",
      "2025-05-29 21:01:32 [INFO]: epoch 85: training loss 0.1957\n",
      "2025-05-29 21:01:32 [INFO]: epoch 86: training loss 0.2140\n",
      "2025-05-29 21:01:32 [INFO]: epoch 87: training loss 0.1984\n",
      "2025-05-29 21:01:32 [INFO]: epoch 88: training loss 0.2062\n",
      "2025-05-29 21:01:32 [INFO]: epoch 89: training loss 0.1787\n",
      "2025-05-29 21:01:32 [INFO]: epoch 90: training loss 0.2252\n",
      "2025-05-29 21:01:32 [INFO]: epoch 91: training loss 0.1910\n",
      "2025-05-29 21:01:32 [INFO]: epoch 92: training loss 0.1794\n",
      "2025-05-29 21:01:32 [INFO]: epoch 93: training loss 0.1906\n",
      "2025-05-29 21:01:32 [INFO]: epoch 94: training loss 0.1687\n",
      "2025-05-29 21:01:32 [INFO]: epoch 95: training loss 0.1841\n",
      "2025-05-29 21:01:32 [INFO]: epoch 96: training loss 0.1721\n",
      "2025-05-29 21:01:32 [INFO]: epoch 97: training loss 0.1869\n",
      "2025-05-29 21:01:32 [INFO]: epoch 98: training loss 0.1816\n",
      "2025-05-29 21:01:32 [INFO]: epoch 99: training loss 0.1710\n",
      "2025-05-29 21:01:32 [INFO]: epoch 100: training loss 0.1827\n",
      "2025-05-29 21:01:32 [INFO]: epoch 101: training loss 0.1742\n",
      "2025-05-29 21:01:32 [INFO]: epoch 102: training loss 0.1800\n",
      "2025-05-29 21:01:32 [INFO]: epoch 103: training loss 0.1857\n",
      "2025-05-29 21:01:32 [INFO]: epoch 104: training loss 0.1763\n",
      "2025-05-29 21:01:32 [INFO]: epoch 105: training loss 0.1839\n",
      "2025-05-29 21:01:32 [INFO]: epoch 106: training loss 0.1548\n",
      "2025-05-29 21:01:32 [INFO]: epoch 107: training loss 0.1769\n",
      "2025-05-29 21:01:32 [INFO]: epoch 108: training loss 0.1715\n",
      "2025-05-29 21:01:32 [INFO]: epoch 109: training loss 0.1673\n",
      "2025-05-29 21:01:32 [INFO]: epoch 110: training loss 0.1618\n",
      "2025-05-29 21:01:33 [INFO]: epoch 111: training loss 0.1602\n",
      "2025-05-29 21:01:33 [INFO]: epoch 112: training loss 0.1553\n",
      "2025-05-29 21:01:33 [INFO]: epoch 113: training loss 0.1662\n",
      "2025-05-29 21:01:33 [INFO]: epoch 114: training loss 0.1578\n",
      "2025-05-29 21:01:33 [INFO]: epoch 115: training loss 0.1654\n",
      "2025-05-29 21:01:33 [INFO]: epoch 116: training loss 0.1630\n",
      "2025-05-29 21:01:33 [INFO]: epoch 117: training loss 0.1569\n",
      "2025-05-29 21:01:33 [INFO]: epoch 118: training loss 0.1499\n",
      "2025-05-29 21:01:33 [INFO]: epoch 119: training loss 0.1401\n",
      "2025-05-29 21:01:33 [INFO]: epoch 120: training loss 0.1567\n",
      "2025-05-29 21:01:33 [INFO]: epoch 121: training loss 0.1544\n",
      "2025-05-29 21:01:33 [INFO]: epoch 122: training loss 0.1598\n",
      "2025-05-29 21:01:33 [INFO]: epoch 123: training loss 0.1484\n",
      "2025-05-29 21:01:33 [INFO]: epoch 124: training loss 0.1449\n",
      "2025-05-29 21:01:33 [INFO]: epoch 125: training loss 0.1491\n",
      "2025-05-29 21:01:33 [INFO]: epoch 126: training loss 0.1430\n",
      "2025-05-29 21:01:33 [INFO]: epoch 127: training loss 0.1251\n",
      "2025-05-29 21:01:33 [INFO]: epoch 128: training loss 0.1305\n",
      "2025-05-29 21:01:33 [INFO]: epoch 129: training loss 0.1396\n",
      "2025-05-29 21:01:33 [INFO]: epoch 130: training loss 0.1517\n",
      "2025-05-29 21:01:33 [INFO]: epoch 131: training loss 0.1376\n",
      "2025-05-29 21:01:33 [INFO]: epoch 132: training loss 0.1306\n",
      "2025-05-29 21:01:33 [INFO]: epoch 133: training loss 0.1306\n",
      "2025-05-29 21:01:33 [INFO]: epoch 134: training loss 0.1320\n",
      "2025-05-29 21:01:33 [INFO]: epoch 135: training loss 0.1419\n",
      "2025-05-29 21:01:33 [INFO]: epoch 136: training loss 0.1456\n",
      "2025-05-29 21:01:33 [INFO]: epoch 137: training loss 0.1228\n",
      "2025-05-29 21:01:33 [INFO]: epoch 138: training loss 0.1395\n",
      "2025-05-29 21:01:33 [INFO]: epoch 139: training loss 0.1195\n",
      "2025-05-29 21:01:33 [INFO]: epoch 140: training loss 0.1438\n",
      "2025-05-29 21:01:33 [INFO]: epoch 141: training loss 0.1356\n",
      "2025-05-29 21:01:33 [INFO]: epoch 142: training loss 0.1266\n",
      "2025-05-29 21:01:33 [INFO]: epoch 143: training loss 0.1236\n",
      "2025-05-29 21:01:33 [INFO]: epoch 144: training loss 0.1213\n",
      "2025-05-29 21:01:33 [INFO]: epoch 145: training loss 0.1322\n",
      "2025-05-29 21:01:33 [INFO]: epoch 146: training loss 0.1328\n",
      "2025-05-29 21:01:33 [INFO]: epoch 147: training loss 0.1096\n",
      "2025-05-29 21:01:33 [INFO]: epoch 148: training loss 0.1247\n",
      "2025-05-29 21:01:33 [INFO]: epoch 149: training loss 0.1260\n",
      "2025-05-29 21:01:33 [INFO]: epoch 150: training loss 0.1364\n",
      "2025-05-29 21:01:33 [INFO]: epoch 151: training loss 0.1305\n",
      "2025-05-29 21:01:33 [INFO]: epoch 152: training loss 0.1306\n",
      "2025-05-29 21:01:33 [INFO]: epoch 153: training loss 0.1288\n",
      "2025-05-29 21:01:33 [INFO]: epoch 154: training loss 0.1184\n",
      "2025-05-29 21:01:33 [INFO]: epoch 155: training loss 0.1318\n",
      "2025-05-29 21:01:33 [INFO]: epoch 156: training loss 0.1161\n",
      "2025-05-29 21:01:33 [INFO]: epoch 157: training loss 0.1108\n",
      "2025-05-29 21:01:33 [INFO]: epoch 158: training loss 0.1301\n",
      "2025-05-29 21:01:33 [INFO]: epoch 159: training loss 0.1312\n",
      "2025-05-29 21:01:33 [INFO]: epoch 160: training loss 0.1142\n",
      "2025-05-29 21:01:33 [INFO]: epoch 161: training loss 0.1234\n",
      "2025-05-29 21:01:33 [INFO]: epoch 162: training loss 0.1339\n",
      "2025-05-29 21:01:33 [INFO]: epoch 163: training loss 0.1154\n",
      "2025-05-29 21:01:33 [INFO]: epoch 164: training loss 0.1215\n",
      "2025-05-29 21:01:33 [INFO]: epoch 165: training loss 0.1162\n",
      "2025-05-29 21:01:33 [INFO]: epoch 166: training loss 0.1246\n",
      "2025-05-29 21:01:33 [INFO]: epoch 167: training loss 0.1227\n",
      "2025-05-29 21:01:33 [INFO]: epoch 168: training loss 0.1059\n",
      "2025-05-29 21:01:33 [INFO]: epoch 169: training loss 0.1250\n",
      "2025-05-29 21:01:33 [INFO]: epoch 170: training loss 0.1288\n",
      "2025-05-29 21:01:33 [INFO]: epoch 171: training loss 0.0958\n",
      "2025-05-29 21:01:33 [INFO]: epoch 172: training loss 0.1064\n",
      "2025-05-29 21:01:33 [INFO]: epoch 173: training loss 0.1124\n",
      "2025-05-29 21:01:33 [INFO]: epoch 174: training loss 0.0980\n",
      "2025-05-29 21:01:33 [INFO]: epoch 175: training loss 0.1029\n",
      "2025-05-29 21:01:33 [INFO]: epoch 176: training loss 0.1076\n",
      "2025-05-29 21:01:33 [INFO]: epoch 177: training loss 0.1008\n",
      "2025-05-29 21:01:33 [INFO]: epoch 178: training loss 0.1033\n",
      "2025-05-29 21:01:33 [INFO]: epoch 179: training loss 0.1173\n",
      "2025-05-29 21:01:33 [INFO]: epoch 180: training loss 0.1007\n",
      "2025-05-29 21:01:33 [INFO]: epoch 181: training loss 0.0995\n",
      "2025-05-29 21:01:33 [INFO]: epoch 182: training loss 0.0975\n",
      "2025-05-29 21:01:33 [INFO]: epoch 183: training loss 0.1090\n",
      "2025-05-29 21:01:33 [INFO]: epoch 184: training loss 0.0942\n",
      "2025-05-29 21:01:33 [INFO]: epoch 185: training loss 0.0977\n",
      "2025-05-29 21:01:33 [INFO]: epoch 186: training loss 0.0971\n",
      "2025-05-29 21:01:33 [INFO]: epoch 187: training loss 0.0994\n",
      "2025-05-29 21:01:33 [INFO]: epoch 188: training loss 0.0976\n",
      "2025-05-29 21:01:33 [INFO]: epoch 189: training loss 0.0948\n",
      "2025-05-29 21:01:33 [INFO]: epoch 190: training loss 0.0936\n",
      "2025-05-29 21:01:33 [INFO]: epoch 191: training loss 0.1084\n",
      "2025-05-29 21:01:33 [INFO]: epoch 192: training loss 0.1016\n",
      "2025-05-29 21:01:34 [INFO]: epoch 193: training loss 0.0879\n",
      "2025-05-29 21:01:34 [INFO]: epoch 194: training loss 0.0987\n",
      "2025-05-29 21:01:34 [INFO]: epoch 195: training loss 0.0866\n",
      "2025-05-29 21:01:34 [INFO]: epoch 196: training loss 0.0825\n",
      "2025-05-29 21:01:34 [INFO]: epoch 197: training loss 0.0922\n",
      "2025-05-29 21:01:34 [INFO]: epoch 198: training loss 0.0997\n",
      "2025-05-29 21:01:34 [INFO]: epoch 199: training loss 0.0874\n",
      "2025-05-29 21:01:34 [INFO]: epoch 200: training loss 0.0910\n",
      "2025-05-29 21:01:34 [INFO]: epoch 201: training loss 0.1325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:34 [INFO]: epoch 202: training loss 0.1056\n",
      "2025-05-29 21:01:34 [INFO]: epoch 203: training loss 0.0881\n",
      "2025-05-29 21:01:34 [INFO]: epoch 204: training loss 0.0847\n",
      "2025-05-29 21:01:34 [INFO]: epoch 205: training loss 0.1028\n",
      "2025-05-29 21:01:34 [INFO]: epoch 206: training loss 0.1029\n",
      "2025-05-29 21:01:34 [INFO]: epoch 207: training loss 0.0814\n",
      "2025-05-29 21:01:34 [INFO]: epoch 208: training loss 0.0764\n",
      "2025-05-29 21:01:34 [INFO]: epoch 209: training loss 0.0876\n",
      "2025-05-29 21:01:34 [INFO]: epoch 210: training loss 0.0879\n",
      "2025-05-29 21:01:34 [INFO]: epoch 211: training loss 0.0860\n",
      "2025-05-29 21:01:34 [INFO]: epoch 212: training loss 0.0796\n",
      "2025-05-29 21:01:34 [INFO]: epoch 213: training loss 0.0841\n",
      "2025-05-29 21:01:34 [INFO]: epoch 214: training loss 0.0843\n",
      "2025-05-29 21:01:34 [INFO]: epoch 215: training loss 0.0920\n",
      "2025-05-29 21:01:34 [INFO]: epoch 216: training loss 0.0789\n",
      "2025-05-29 21:01:34 [INFO]: epoch 217: training loss 0.0745\n",
      "2025-05-29 21:01:34 [INFO]: epoch 218: training loss 0.0975\n",
      "2025-05-29 21:01:34 [INFO]: epoch 219: training loss 0.0828\n",
      "2025-05-29 21:01:34 [INFO]: epoch 220: training loss 0.0713\n",
      "2025-05-29 21:01:34 [INFO]: epoch 221: training loss 0.0811\n",
      "2025-05-29 21:01:34 [INFO]: epoch 222: training loss 0.0840\n",
      "2025-05-29 21:01:34 [INFO]: epoch 223: training loss 0.0705\n",
      "2025-05-29 21:01:34 [INFO]: epoch 224: training loss 0.0823\n",
      "2025-05-29 21:01:34 [INFO]: epoch 225: training loss 0.0716\n",
      "2025-05-29 21:01:34 [INFO]: epoch 226: training loss 0.0696\n",
      "2025-05-29 21:01:34 [INFO]: epoch 227: training loss 0.0788\n",
      "2025-05-29 21:01:34 [INFO]: epoch 228: training loss 0.0696\n",
      "2025-05-29 21:01:34 [INFO]: epoch 229: training loss 0.0734\n",
      "2025-05-29 21:01:34 [INFO]: epoch 230: training loss 0.0766\n",
      "2025-05-29 21:01:34 [INFO]: epoch 231: training loss 0.0745\n",
      "2025-05-29 21:01:34 [INFO]: epoch 232: training loss 0.0652\n",
      "2025-05-29 21:01:34 [INFO]: epoch 233: training loss 0.0726\n",
      "2025-05-29 21:01:34 [INFO]: epoch 234: training loss 0.0809\n",
      "2025-05-29 21:01:34 [INFO]: epoch 235: training loss 0.0643\n",
      "2025-05-29 21:01:34 [INFO]: epoch 236: training loss 0.0618\n",
      "2025-05-29 21:01:34 [INFO]: epoch 237: training loss 0.0873\n",
      "2025-05-29 21:01:34 [INFO]: epoch 238: training loss 0.0654\n",
      "2025-05-29 21:01:34 [INFO]: epoch 239: training loss 0.0649\n",
      "2025-05-29 21:01:34 [INFO]: epoch 240: training loss 0.0621\n",
      "2025-05-29 21:01:34 [INFO]: epoch 241: training loss 0.0710\n",
      "2025-05-29 21:01:34 [INFO]: epoch 242: training loss 0.0638\n",
      "2025-05-29 21:01:34 [INFO]: epoch 243: training loss 0.0622\n",
      "2025-05-29 21:01:34 [INFO]: epoch 244: training loss 0.0644\n",
      "2025-05-29 21:01:34 [INFO]: epoch 245: training loss 0.0714\n",
      "2025-05-29 21:01:34 [INFO]: epoch 246: training loss 0.0749\n",
      "2025-05-29 21:01:34 [INFO]: epoch 247: training loss 0.0588\n",
      "2025-05-29 21:01:34 [INFO]: epoch 248: training loss 0.0653\n",
      "2025-05-29 21:01:34 [INFO]: epoch 249: training loss 0.0632\n",
      "2025-05-29 21:01:34 [INFO]: epoch 250: training loss 0.0564\n",
      "2025-05-29 21:01:34 [INFO]: epoch 251: training loss 0.0691\n",
      "2025-05-29 21:01:34 [INFO]: epoch 252: training loss 0.0571\n",
      "2025-05-29 21:01:34 [INFO]: epoch 253: training loss 0.0559\n",
      "2025-05-29 21:01:34 [INFO]: epoch 254: training loss 0.0617\n",
      "2025-05-29 21:01:34 [INFO]: epoch 255: training loss 0.0632\n",
      "2025-05-29 21:01:34 [INFO]: epoch 256: training loss 0.0606\n",
      "2025-05-29 21:01:34 [INFO]: epoch 257: training loss 0.0644\n",
      "2025-05-29 21:01:34 [INFO]: epoch 258: training loss 0.0525\n",
      "2025-05-29 21:01:34 [INFO]: epoch 259: training loss 0.0682\n",
      "2025-05-29 21:01:34 [INFO]: epoch 260: training loss 0.0560\n",
      "2025-05-29 21:01:34 [INFO]: epoch 261: training loss 0.0562\n",
      "2025-05-29 21:01:34 [INFO]: epoch 262: training loss 0.0530\n",
      "2025-05-29 21:01:34 [INFO]: epoch 263: training loss 0.0593\n",
      "2025-05-29 21:01:34 [INFO]: epoch 264: training loss 0.0591\n",
      "2025-05-29 21:01:34 [INFO]: epoch 265: training loss 0.0539\n",
      "2025-05-29 21:01:34 [INFO]: epoch 266: training loss 0.0520\n",
      "2025-05-29 21:01:34 [INFO]: epoch 267: training loss 0.0499\n",
      "2025-05-29 21:01:34 [INFO]: epoch 268: training loss 0.0516\n",
      "2025-05-29 21:01:34 [INFO]: epoch 269: training loss 0.0514\n",
      "2025-05-29 21:01:34 [INFO]: epoch 270: training loss 0.0492\n",
      "2025-05-29 21:01:34 [INFO]: epoch 271: training loss 0.0612\n",
      "2025-05-29 21:01:35 [INFO]: epoch 272: training loss 0.0612\n",
      "2025-05-29 21:01:35 [INFO]: epoch 273: training loss 0.0545\n",
      "2025-05-29 21:01:35 [INFO]: epoch 274: training loss 0.0544\n",
      "2025-05-29 21:01:35 [INFO]: epoch 275: training loss 0.0513\n",
      "2025-05-29 21:01:35 [INFO]: epoch 276: training loss 0.0544\n",
      "2025-05-29 21:01:35 [INFO]: epoch 277: training loss 0.0598\n",
      "2025-05-29 21:01:35 [INFO]: epoch 278: training loss 0.0544\n",
      "2025-05-29 21:01:35 [INFO]: epoch 279: training loss 0.0444\n",
      "2025-05-29 21:01:35 [INFO]: epoch 280: training loss 0.0560\n",
      "2025-05-29 21:01:35 [INFO]: epoch 281: training loss 0.0492\n",
      "2025-05-29 21:01:35 [INFO]: epoch 282: training loss 0.0528\n",
      "2025-05-29 21:01:35 [INFO]: epoch 283: training loss 0.0529\n",
      "2025-05-29 21:01:35 [INFO]: epoch 284: training loss 0.0491\n",
      "2025-05-29 21:01:35 [INFO]: epoch 285: training loss 0.0470\n",
      "2025-05-29 21:01:35 [INFO]: epoch 286: training loss 0.0500\n",
      "2025-05-29 21:01:35 [INFO]: epoch 287: training loss 0.0620\n",
      "2025-05-29 21:01:35 [INFO]: epoch 288: training loss 0.0572\n",
      "2025-05-29 21:01:35 [INFO]: epoch 289: training loss 0.0609\n",
      "2025-05-29 21:01:35 [INFO]: epoch 290: training loss 0.0442\n",
      "2025-05-29 21:01:35 [INFO]: epoch 291: training loss 0.0493\n",
      "2025-05-29 21:01:35 [INFO]: epoch 292: training loss 0.0509\n",
      "2025-05-29 21:01:35 [INFO]: epoch 293: training loss 0.0544\n",
      "2025-05-29 21:01:35 [INFO]: epoch 294: training loss 0.0507\n",
      "2025-05-29 21:01:35 [INFO]: epoch 295: training loss 0.0535\n",
      "2025-05-29 21:01:35 [INFO]: epoch 296: training loss 0.0423\n",
      "2025-05-29 21:01:35 [INFO]: epoch 297: training loss 0.0493\n",
      "2025-05-29 21:01:35 [INFO]: epoch 298: training loss 0.0480\n",
      "2025-05-29 21:01:35 [INFO]: epoch 299: training loss 0.0497\n",
      "2025-05-29 21:01:35 [INFO]: Finished training.\n",
      "2025-05-29 21:01:35 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:11<00:07,  3.86s/it]2025-05-29 21:01:35 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:01:35 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:01:35 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:01:35 [INFO]: epoch 0: training loss 0.8674\n",
      "2025-05-29 21:01:35 [INFO]: epoch 1: training loss 0.6293\n",
      "2025-05-29 21:01:35 [INFO]: epoch 2: training loss 0.6714\n",
      "2025-05-29 21:01:35 [INFO]: epoch 3: training loss 0.6864\n",
      "2025-05-29 21:01:35 [INFO]: epoch 4: training loss 0.6127\n",
      "2025-05-29 21:01:35 [INFO]: epoch 5: training loss 0.4927\n",
      "2025-05-29 21:01:35 [INFO]: epoch 6: training loss 0.3827\n",
      "2025-05-29 21:01:35 [INFO]: epoch 7: training loss 0.4066\n",
      "2025-05-29 21:01:35 [INFO]: epoch 8: training loss 0.4919\n",
      "2025-05-29 21:01:35 [INFO]: epoch 9: training loss 0.4643\n",
      "2025-05-29 21:01:35 [INFO]: epoch 10: training loss 0.4687\n",
      "2025-05-29 21:01:35 [INFO]: epoch 11: training loss 0.4468\n",
      "2025-05-29 21:01:35 [INFO]: epoch 12: training loss 0.3849\n",
      "2025-05-29 21:01:35 [INFO]: epoch 13: training loss 0.3691\n",
      "2025-05-29 21:01:35 [INFO]: epoch 14: training loss 0.3656\n",
      "2025-05-29 21:01:35 [INFO]: epoch 15: training loss 0.3744\n",
      "2025-05-29 21:01:35 [INFO]: epoch 16: training loss 0.3831\n",
      "2025-05-29 21:01:35 [INFO]: epoch 17: training loss 0.3677\n",
      "2025-05-29 21:01:35 [INFO]: epoch 18: training loss 0.3350\n",
      "2025-05-29 21:01:35 [INFO]: epoch 19: training loss 0.3505\n",
      "2025-05-29 21:01:35 [INFO]: epoch 20: training loss 0.3274\n",
      "2025-05-29 21:01:35 [INFO]: epoch 21: training loss 0.3147\n",
      "2025-05-29 21:01:35 [INFO]: epoch 22: training loss 0.3346\n",
      "2025-05-29 21:01:35 [INFO]: epoch 23: training loss 0.3114\n",
      "2025-05-29 21:01:35 [INFO]: epoch 24: training loss 0.3216\n",
      "2025-05-29 21:01:35 [INFO]: epoch 25: training loss 0.3132\n",
      "2025-05-29 21:01:35 [INFO]: epoch 26: training loss 0.3002\n",
      "2025-05-29 21:01:35 [INFO]: epoch 27: training loss 0.2868\n",
      "2025-05-29 21:01:35 [INFO]: epoch 28: training loss 0.2903\n",
      "2025-05-29 21:01:35 [INFO]: epoch 29: training loss 0.3088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:35 [INFO]: epoch 30: training loss 0.3017\n",
      "2025-05-29 21:01:35 [INFO]: epoch 31: training loss 0.2847\n",
      "2025-05-29 21:01:35 [INFO]: epoch 32: training loss 0.2751\n",
      "2025-05-29 21:01:35 [INFO]: epoch 33: training loss 0.2647\n",
      "2025-05-29 21:01:35 [INFO]: epoch 34: training loss 0.2762\n",
      "2025-05-29 21:01:35 [INFO]: epoch 35: training loss 0.2939\n",
      "2025-05-29 21:01:35 [INFO]: epoch 36: training loss 0.2593\n",
      "2025-05-29 21:01:35 [INFO]: epoch 37: training loss 0.2789\n",
      "2025-05-29 21:01:35 [INFO]: epoch 38: training loss 0.2667\n",
      "2025-05-29 21:01:35 [INFO]: epoch 39: training loss 0.2507\n",
      "2025-05-29 21:01:35 [INFO]: epoch 40: training loss 0.2407\n",
      "2025-05-29 21:01:35 [INFO]: epoch 41: training loss 0.2665\n",
      "2025-05-29 21:01:35 [INFO]: epoch 42: training loss 0.2608\n",
      "2025-05-29 21:01:35 [INFO]: epoch 43: training loss 0.2521\n",
      "2025-05-29 21:01:36 [INFO]: epoch 44: training loss 0.2576\n",
      "2025-05-29 21:01:36 [INFO]: epoch 45: training loss 0.2558\n",
      "2025-05-29 21:01:36 [INFO]: epoch 46: training loss 0.2587\n",
      "2025-05-29 21:01:36 [INFO]: epoch 47: training loss 0.2575\n",
      "2025-05-29 21:01:36 [INFO]: epoch 48: training loss 0.2531\n",
      "2025-05-29 21:01:36 [INFO]: epoch 49: training loss 0.2544\n",
      "2025-05-29 21:01:36 [INFO]: epoch 50: training loss 0.2480\n",
      "2025-05-29 21:01:36 [INFO]: epoch 51: training loss 0.2470\n",
      "2025-05-29 21:01:36 [INFO]: epoch 52: training loss 0.2333\n",
      "2025-05-29 21:01:36 [INFO]: epoch 53: training loss 0.2286\n",
      "2025-05-29 21:01:36 [INFO]: epoch 54: training loss 0.2209\n",
      "2025-05-29 21:01:36 [INFO]: epoch 55: training loss 0.2408\n",
      "2025-05-29 21:01:36 [INFO]: epoch 56: training loss 0.2283\n",
      "2025-05-29 21:01:36 [INFO]: epoch 57: training loss 0.2180\n",
      "2025-05-29 21:01:36 [INFO]: epoch 58: training loss 0.2229\n",
      "2025-05-29 21:01:36 [INFO]: epoch 59: training loss 0.2271\n",
      "2025-05-29 21:01:36 [INFO]: epoch 60: training loss 0.2047\n",
      "2025-05-29 21:01:36 [INFO]: epoch 61: training loss 0.2007\n",
      "2025-05-29 21:01:36 [INFO]: epoch 62: training loss 0.2151\n",
      "2025-05-29 21:01:36 [INFO]: epoch 63: training loss 0.2286\n",
      "2025-05-29 21:01:36 [INFO]: epoch 64: training loss 0.2005\n",
      "2025-05-29 21:01:36 [INFO]: epoch 65: training loss 0.1946\n",
      "2025-05-29 21:01:36 [INFO]: epoch 66: training loss 0.2064\n",
      "2025-05-29 21:01:36 [INFO]: epoch 67: training loss 0.2325\n",
      "2025-05-29 21:01:36 [INFO]: epoch 68: training loss 0.2023\n",
      "2025-05-29 21:01:36 [INFO]: epoch 69: training loss 0.2073\n",
      "2025-05-29 21:01:36 [INFO]: epoch 70: training loss 0.2222\n",
      "2025-05-29 21:01:36 [INFO]: epoch 71: training loss 0.2015\n",
      "2025-05-29 21:01:36 [INFO]: epoch 72: training loss 0.2150\n",
      "2025-05-29 21:01:36 [INFO]: epoch 73: training loss 0.1916\n",
      "2025-05-29 21:01:36 [INFO]: epoch 74: training loss 0.1885\n",
      "2025-05-29 21:01:36 [INFO]: epoch 75: training loss 0.1962\n",
      "2025-05-29 21:01:36 [INFO]: epoch 76: training loss 0.1815\n",
      "2025-05-29 21:01:36 [INFO]: epoch 77: training loss 0.1847\n",
      "2025-05-29 21:01:36 [INFO]: epoch 78: training loss 0.1823\n",
      "2025-05-29 21:01:36 [INFO]: epoch 79: training loss 0.1967\n",
      "2025-05-29 21:01:36 [INFO]: epoch 80: training loss 0.1901\n",
      "2025-05-29 21:01:36 [INFO]: epoch 81: training loss 0.2011\n",
      "2025-05-29 21:01:36 [INFO]: epoch 82: training loss 0.1706\n",
      "2025-05-29 21:01:36 [INFO]: epoch 83: training loss 0.1783\n",
      "2025-05-29 21:01:36 [INFO]: epoch 84: training loss 0.1810\n",
      "2025-05-29 21:01:36 [INFO]: epoch 85: training loss 0.2032\n",
      "2025-05-29 21:01:36 [INFO]: epoch 86: training loss 0.1803\n",
      "2025-05-29 21:01:36 [INFO]: epoch 87: training loss 0.1753\n",
      "2025-05-29 21:01:36 [INFO]: epoch 88: training loss 0.1796\n",
      "2025-05-29 21:01:36 [INFO]: epoch 89: training loss 0.1928\n",
      "2025-05-29 21:01:36 [INFO]: epoch 90: training loss 0.1862\n",
      "2025-05-29 21:01:36 [INFO]: epoch 91: training loss 0.1721\n",
      "2025-05-29 21:01:36 [INFO]: epoch 92: training loss 0.1976\n",
      "2025-05-29 21:01:36 [INFO]: epoch 93: training loss 0.1869\n",
      "2025-05-29 21:01:36 [INFO]: epoch 94: training loss 0.1656\n",
      "2025-05-29 21:01:36 [INFO]: epoch 95: training loss 0.1903\n",
      "2025-05-29 21:01:36 [INFO]: epoch 96: training loss 0.1982\n",
      "2025-05-29 21:01:36 [INFO]: epoch 97: training loss 0.1659\n",
      "2025-05-29 21:01:36 [INFO]: epoch 98: training loss 0.1542\n",
      "2025-05-29 21:01:36 [INFO]: epoch 99: training loss 0.1760\n",
      "2025-05-29 21:01:36 [INFO]: epoch 100: training loss 0.1708\n",
      "2025-05-29 21:01:36 [INFO]: epoch 101: training loss 0.1784\n",
      "2025-05-29 21:01:36 [INFO]: epoch 102: training loss 0.1547\n",
      "2025-05-29 21:01:36 [INFO]: epoch 103: training loss 0.1492\n",
      "2025-05-29 21:01:36 [INFO]: epoch 104: training loss 0.1583\n",
      "2025-05-29 21:01:36 [INFO]: epoch 105: training loss 0.1557\n",
      "2025-05-29 21:01:36 [INFO]: epoch 106: training loss 0.1717\n",
      "2025-05-29 21:01:36 [INFO]: epoch 107: training loss 0.1549\n",
      "2025-05-29 21:01:36 [INFO]: epoch 108: training loss 0.1745\n",
      "2025-05-29 21:01:36 [INFO]: epoch 109: training loss 0.1497\n",
      "2025-05-29 21:01:36 [INFO]: epoch 110: training loss 0.1531\n",
      "2025-05-29 21:01:36 [INFO]: epoch 111: training loss 0.1732\n",
      "2025-05-29 21:01:36 [INFO]: epoch 112: training loss 0.1641\n",
      "2025-05-29 21:01:36 [INFO]: epoch 113: training loss 0.1563\n",
      "2025-05-29 21:01:36 [INFO]: epoch 114: training loss 0.1582\n",
      "2025-05-29 21:01:36 [INFO]: epoch 115: training loss 0.1460\n",
      "2025-05-29 21:01:36 [INFO]: epoch 116: training loss 0.1342\n",
      "2025-05-29 21:01:36 [INFO]: epoch 117: training loss 0.1515\n",
      "2025-05-29 21:01:36 [INFO]: epoch 118: training loss 0.1595\n",
      "2025-05-29 21:01:36 [INFO]: epoch 119: training loss 0.1399\n",
      "2025-05-29 21:01:36 [INFO]: epoch 120: training loss 0.1421\n",
      "2025-05-29 21:01:36 [INFO]: epoch 121: training loss 0.1644\n",
      "2025-05-29 21:01:36 [INFO]: epoch 122: training loss 0.1523\n",
      "2025-05-29 21:01:36 [INFO]: epoch 123: training loss 0.1421\n",
      "2025-05-29 21:01:37 [INFO]: epoch 124: training loss 0.1475\n",
      "2025-05-29 21:01:37 [INFO]: epoch 125: training loss 0.1312\n",
      "2025-05-29 21:01:37 [INFO]: epoch 126: training loss 0.1505\n",
      "2025-05-29 21:01:37 [INFO]: epoch 127: training loss 0.1429\n",
      "2025-05-29 21:01:37 [INFO]: epoch 128: training loss 0.1539\n",
      "2025-05-29 21:01:37 [INFO]: epoch 129: training loss 0.1348\n",
      "2025-05-29 21:01:37 [INFO]: epoch 130: training loss 0.1343\n",
      "2025-05-29 21:01:37 [INFO]: epoch 131: training loss 0.1388\n",
      "2025-05-29 21:01:37 [INFO]: epoch 132: training loss 0.1476\n",
      "2025-05-29 21:01:37 [INFO]: epoch 133: training loss 0.1474\n",
      "2025-05-29 21:01:37 [INFO]: epoch 134: training loss 0.1364\n",
      "2025-05-29 21:01:37 [INFO]: epoch 135: training loss 0.1372\n",
      "2025-05-29 21:01:37 [INFO]: epoch 136: training loss 0.1514\n",
      "2025-05-29 21:01:37 [INFO]: epoch 137: training loss 0.1720\n",
      "2025-05-29 21:01:37 [INFO]: epoch 138: training loss 0.1459\n",
      "2025-05-29 21:01:37 [INFO]: epoch 139: training loss 0.1267\n",
      "2025-05-29 21:01:37 [INFO]: epoch 140: training loss 0.1566\n",
      "2025-05-29 21:01:37 [INFO]: epoch 141: training loss 0.1487\n",
      "2025-05-29 21:01:37 [INFO]: epoch 142: training loss 0.1461\n",
      "2025-05-29 21:01:37 [INFO]: epoch 143: training loss 0.1374\n",
      "2025-05-29 21:01:37 [INFO]: epoch 144: training loss 0.1288\n",
      "2025-05-29 21:01:37 [INFO]: epoch 145: training loss 0.1249\n",
      "2025-05-29 21:01:37 [INFO]: epoch 146: training loss 0.1231\n",
      "2025-05-29 21:01:37 [INFO]: epoch 147: training loss 0.1274\n",
      "2025-05-29 21:01:37 [INFO]: epoch 148: training loss 0.1431\n",
      "2025-05-29 21:01:37 [INFO]: epoch 149: training loss 0.1247\n",
      "2025-05-29 21:01:37 [INFO]: epoch 150: training loss 0.1296\n",
      "2025-05-29 21:01:37 [INFO]: epoch 151: training loss 0.1185\n",
      "2025-05-29 21:01:37 [INFO]: epoch 152: training loss 0.1306\n",
      "2025-05-29 21:01:37 [INFO]: epoch 153: training loss 0.1327\n",
      "2025-05-29 21:01:37 [INFO]: epoch 154: training loss 0.1248\n",
      "2025-05-29 21:01:37 [INFO]: epoch 155: training loss 0.1357\n",
      "2025-05-29 21:01:37 [INFO]: epoch 156: training loss 0.1284\n",
      "2025-05-29 21:01:37 [INFO]: epoch 157: training loss 0.1289\n",
      "2025-05-29 21:01:37 [INFO]: epoch 158: training loss 0.1101\n",
      "2025-05-29 21:01:37 [INFO]: epoch 159: training loss 0.1165\n",
      "2025-05-29 21:01:37 [INFO]: epoch 160: training loss 0.1020\n",
      "2025-05-29 21:01:37 [INFO]: epoch 161: training loss 0.1192\n",
      "2025-05-29 21:01:37 [INFO]: epoch 162: training loss 0.1318\n",
      "2025-05-29 21:01:37 [INFO]: epoch 163: training loss 0.1238\n",
      "2025-05-29 21:01:37 [INFO]: epoch 164: training loss 0.1377\n",
      "2025-05-29 21:01:37 [INFO]: epoch 165: training loss 0.1479\n",
      "2025-05-29 21:01:37 [INFO]: epoch 166: training loss 0.1507\n",
      "2025-05-29 21:01:37 [INFO]: epoch 167: training loss 0.1095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:37 [INFO]: epoch 168: training loss 0.1230\n",
      "2025-05-29 21:01:37 [INFO]: epoch 169: training loss 0.1344\n",
      "2025-05-29 21:01:37 [INFO]: epoch 170: training loss 0.1173\n",
      "2025-05-29 21:01:37 [INFO]: epoch 171: training loss 0.1036\n",
      "2025-05-29 21:01:37 [INFO]: epoch 172: training loss 0.1276\n",
      "2025-05-29 21:01:37 [INFO]: epoch 173: training loss 0.1301\n",
      "2025-05-29 21:01:37 [INFO]: epoch 174: training loss 0.1007\n",
      "2025-05-29 21:01:37 [INFO]: epoch 175: training loss 0.1077\n",
      "2025-05-29 21:01:37 [INFO]: epoch 176: training loss 0.1056\n",
      "2025-05-29 21:01:37 [INFO]: epoch 177: training loss 0.1117\n",
      "2025-05-29 21:01:37 [INFO]: epoch 178: training loss 0.0914\n",
      "2025-05-29 21:01:37 [INFO]: epoch 179: training loss 0.1055\n",
      "2025-05-29 21:01:37 [INFO]: epoch 180: training loss 0.1029\n",
      "2025-05-29 21:01:37 [INFO]: epoch 181: training loss 0.1100\n",
      "2025-05-29 21:01:37 [INFO]: epoch 182: training loss 0.1045\n",
      "2025-05-29 21:01:37 [INFO]: epoch 183: training loss 0.1013\n",
      "2025-05-29 21:01:37 [INFO]: epoch 184: training loss 0.0995\n",
      "2025-05-29 21:01:37 [INFO]: epoch 185: training loss 0.0911\n",
      "2025-05-29 21:01:37 [INFO]: epoch 186: training loss 0.0981\n",
      "2025-05-29 21:01:37 [INFO]: epoch 187: training loss 0.0992\n",
      "2025-05-29 21:01:37 [INFO]: epoch 188: training loss 0.1050\n",
      "2025-05-29 21:01:37 [INFO]: epoch 189: training loss 0.0854\n",
      "2025-05-29 21:01:37 [INFO]: epoch 190: training loss 0.0894\n",
      "2025-05-29 21:01:37 [INFO]: epoch 191: training loss 0.0896\n",
      "2025-05-29 21:01:37 [INFO]: epoch 192: training loss 0.1021\n",
      "2025-05-29 21:01:37 [INFO]: epoch 193: training loss 0.0945\n",
      "2025-05-29 21:01:37 [INFO]: epoch 194: training loss 0.0952\n",
      "2025-05-29 21:01:37 [INFO]: epoch 195: training loss 0.0947\n",
      "2025-05-29 21:01:37 [INFO]: epoch 196: training loss 0.1097\n",
      "2025-05-29 21:01:37 [INFO]: epoch 197: training loss 0.1009\n",
      "2025-05-29 21:01:37 [INFO]: epoch 198: training loss 0.0986\n",
      "2025-05-29 21:01:37 [INFO]: epoch 199: training loss 0.0899\n",
      "2025-05-29 21:01:37 [INFO]: epoch 200: training loss 0.1040\n",
      "2025-05-29 21:01:37 [INFO]: epoch 201: training loss 0.1037\n",
      "2025-05-29 21:01:37 [INFO]: epoch 202: training loss 0.0938\n",
      "2025-05-29 21:01:37 [INFO]: epoch 203: training loss 0.0964\n",
      "2025-05-29 21:01:38 [INFO]: epoch 204: training loss 0.0926\n",
      "2025-05-29 21:01:38 [INFO]: epoch 205: training loss 0.0950\n",
      "2025-05-29 21:01:38 [INFO]: epoch 206: training loss 0.0840\n",
      "2025-05-29 21:01:38 [INFO]: epoch 207: training loss 0.0876\n",
      "2025-05-29 21:01:38 [INFO]: epoch 208: training loss 0.0941\n",
      "2025-05-29 21:01:38 [INFO]: epoch 209: training loss 0.0847\n",
      "2025-05-29 21:01:38 [INFO]: epoch 210: training loss 0.1017\n",
      "2025-05-29 21:01:38 [INFO]: epoch 211: training loss 0.0905\n",
      "2025-05-29 21:01:38 [INFO]: epoch 212: training loss 0.0818\n",
      "2025-05-29 21:01:38 [INFO]: epoch 213: training loss 0.0959\n",
      "2025-05-29 21:01:38 [INFO]: epoch 214: training loss 0.0859\n",
      "2025-05-29 21:01:38 [INFO]: epoch 215: training loss 0.0814\n",
      "2025-05-29 21:01:38 [INFO]: epoch 216: training loss 0.0810\n",
      "2025-05-29 21:01:38 [INFO]: epoch 217: training loss 0.0916\n",
      "2025-05-29 21:01:38 [INFO]: epoch 218: training loss 0.0889\n",
      "2025-05-29 21:01:38 [INFO]: epoch 219: training loss 0.0957\n",
      "2025-05-29 21:01:38 [INFO]: epoch 220: training loss 0.0830\n",
      "2025-05-29 21:01:38 [INFO]: epoch 221: training loss 0.0802\n",
      "2025-05-29 21:01:38 [INFO]: epoch 222: training loss 0.0806\n",
      "2025-05-29 21:01:38 [INFO]: epoch 223: training loss 0.0808\n",
      "2025-05-29 21:01:38 [INFO]: epoch 224: training loss 0.0806\n",
      "2025-05-29 21:01:38 [INFO]: epoch 225: training loss 0.0697\n",
      "2025-05-29 21:01:38 [INFO]: epoch 226: training loss 0.0789\n",
      "2025-05-29 21:01:38 [INFO]: epoch 227: training loss 0.0810\n",
      "2025-05-29 21:01:38 [INFO]: epoch 228: training loss 0.0699\n",
      "2025-05-29 21:01:38 [INFO]: epoch 229: training loss 0.0679\n",
      "2025-05-29 21:01:38 [INFO]: epoch 230: training loss 0.0737\n",
      "2025-05-29 21:01:38 [INFO]: epoch 231: training loss 0.0875\n",
      "2025-05-29 21:01:38 [INFO]: epoch 232: training loss 0.0700\n",
      "2025-05-29 21:01:38 [INFO]: epoch 233: training loss 0.0795\n",
      "2025-05-29 21:01:38 [INFO]: epoch 234: training loss 0.0881\n",
      "2025-05-29 21:01:38 [INFO]: epoch 235: training loss 0.0660\n",
      "2025-05-29 21:01:38 [INFO]: epoch 236: training loss 0.0818\n",
      "2025-05-29 21:01:38 [INFO]: epoch 237: training loss 0.0768\n",
      "2025-05-29 21:01:38 [INFO]: epoch 238: training loss 0.0807\n",
      "2025-05-29 21:01:38 [INFO]: epoch 239: training loss 0.0691\n",
      "2025-05-29 21:01:38 [INFO]: epoch 240: training loss 0.0733\n",
      "2025-05-29 21:01:38 [INFO]: epoch 241: training loss 0.0849\n",
      "2025-05-29 21:01:38 [INFO]: epoch 242: training loss 0.0802\n",
      "2025-05-29 21:01:38 [INFO]: epoch 243: training loss 0.0726\n",
      "2025-05-29 21:01:38 [INFO]: epoch 244: training loss 0.0706\n",
      "2025-05-29 21:01:38 [INFO]: epoch 245: training loss 0.0753\n",
      "2025-05-29 21:01:38 [INFO]: epoch 246: training loss 0.0767\n",
      "2025-05-29 21:01:38 [INFO]: epoch 247: training loss 0.0632\n",
      "2025-05-29 21:01:38 [INFO]: epoch 248: training loss 0.0653\n",
      "2025-05-29 21:01:38 [INFO]: epoch 249: training loss 0.0675\n",
      "2025-05-29 21:01:38 [INFO]: epoch 250: training loss 0.0785\n",
      "2025-05-29 21:01:38 [INFO]: epoch 251: training loss 0.0753\n",
      "2025-05-29 21:01:38 [INFO]: epoch 252: training loss 0.0633\n",
      "2025-05-29 21:01:38 [INFO]: epoch 253: training loss 0.0715\n",
      "2025-05-29 21:01:38 [INFO]: epoch 254: training loss 0.0619\n",
      "2025-05-29 21:01:38 [INFO]: epoch 255: training loss 0.0721\n",
      "2025-05-29 21:01:38 [INFO]: epoch 256: training loss 0.0694\n",
      "2025-05-29 21:01:38 [INFO]: epoch 257: training loss 0.0534\n",
      "2025-05-29 21:01:38 [INFO]: epoch 258: training loss 0.0741\n",
      "2025-05-29 21:01:38 [INFO]: epoch 259: training loss 0.0689\n",
      "2025-05-29 21:01:38 [INFO]: epoch 260: training loss 0.0672\n",
      "2025-05-29 21:01:38 [INFO]: epoch 261: training loss 0.0702\n",
      "2025-05-29 21:01:38 [INFO]: epoch 262: training loss 0.0816\n",
      "2025-05-29 21:01:38 [INFO]: epoch 263: training loss 0.0790\n",
      "2025-05-29 21:01:38 [INFO]: epoch 264: training loss 0.0679\n",
      "2025-05-29 21:01:38 [INFO]: epoch 265: training loss 0.0793\n",
      "2025-05-29 21:01:38 [INFO]: epoch 266: training loss 0.0802\n",
      "2025-05-29 21:01:38 [INFO]: epoch 267: training loss 0.0635\n",
      "2025-05-29 21:01:38 [INFO]: epoch 268: training loss 0.0602\n",
      "2025-05-29 21:01:38 [INFO]: epoch 269: training loss 0.0778\n",
      "2025-05-29 21:01:38 [INFO]: epoch 270: training loss 0.0866\n",
      "2025-05-29 21:01:38 [INFO]: epoch 271: training loss 0.0598\n",
      "2025-05-29 21:01:38 [INFO]: epoch 272: training loss 0.0650\n",
      "2025-05-29 21:01:38 [INFO]: epoch 273: training loss 0.0735\n",
      "2025-05-29 21:01:38 [INFO]: epoch 274: training loss 0.0809\n",
      "2025-05-29 21:01:38 [INFO]: epoch 275: training loss 0.0742\n",
      "2025-05-29 21:01:38 [INFO]: epoch 276: training loss 0.0700\n",
      "2025-05-29 21:01:38 [INFO]: epoch 277: training loss 0.0607\n",
      "2025-05-29 21:01:38 [INFO]: epoch 278: training loss 0.0607\n",
      "2025-05-29 21:01:38 [INFO]: epoch 279: training loss 0.0640\n",
      "2025-05-29 21:01:38 [INFO]: epoch 280: training loss 0.0628\n",
      "2025-05-29 21:01:38 [INFO]: epoch 281: training loss 0.0581\n",
      "2025-05-29 21:01:38 [INFO]: epoch 282: training loss 0.0507\n",
      "2025-05-29 21:01:38 [INFO]: epoch 283: training loss 0.0642\n",
      "2025-05-29 21:01:39 [INFO]: epoch 284: training loss 0.0661\n",
      "2025-05-29 21:01:39 [INFO]: epoch 285: training loss 0.0631\n",
      "2025-05-29 21:01:39 [INFO]: epoch 286: training loss 0.0621\n",
      "2025-05-29 21:01:39 [INFO]: epoch 287: training loss 0.0549\n",
      "2025-05-29 21:01:39 [INFO]: epoch 288: training loss 0.0527\n",
      "2025-05-29 21:01:39 [INFO]: epoch 289: training loss 0.0569\n",
      "2025-05-29 21:01:39 [INFO]: epoch 290: training loss 0.0529\n",
      "2025-05-29 21:01:39 [INFO]: epoch 291: training loss 0.0504\n",
      "2025-05-29 21:01:39 [INFO]: epoch 292: training loss 0.0494\n",
      "2025-05-29 21:01:39 [INFO]: epoch 293: training loss 0.0491\n",
      "2025-05-29 21:01:39 [INFO]: epoch 294: training loss 0.0504\n",
      "2025-05-29 21:01:39 [INFO]: epoch 295: training loss 0.0536\n",
      "2025-05-29 21:01:39 [INFO]: epoch 296: training loss 0.0528\n",
      "2025-05-29 21:01:39 [INFO]: epoch 297: training loss 0.0563\n",
      "2025-05-29 21:01:39 [INFO]: epoch 298: training loss 0.0582\n",
      "2025-05-29 21:01:39 [INFO]: epoch 299: training loss 0.0554\n",
      "2025-05-29 21:01:39 [INFO]: Finished training.\n",
      "2025-05-29 21:01:39 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:15<00:03,  3.85s/it]2025-05-29 21:01:39 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:01:39 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:01:39 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:01:39 [INFO]: epoch 0: training loss 1.0774\n",
      "2025-05-29 21:01:39 [INFO]: epoch 1: training loss 0.5872\n",
      "2025-05-29 21:01:39 [INFO]: epoch 2: training loss 0.5627\n",
      "2025-05-29 21:01:39 [INFO]: epoch 3: training loss 0.5185\n",
      "2025-05-29 21:01:39 [INFO]: epoch 4: training loss 0.5453\n",
      "2025-05-29 21:01:39 [INFO]: epoch 5: training loss 0.5130\n",
      "2025-05-29 21:01:39 [INFO]: epoch 6: training loss 0.4770\n",
      "2025-05-29 21:01:39 [INFO]: epoch 7: training loss 0.4134\n",
      "2025-05-29 21:01:39 [INFO]: epoch 8: training loss 0.4367\n",
      "2025-05-29 21:01:39 [INFO]: epoch 9: training loss 0.4496\n",
      "2025-05-29 21:01:39 [INFO]: epoch 10: training loss 0.4629\n",
      "2025-05-29 21:01:39 [INFO]: epoch 11: training loss 0.4481\n",
      "2025-05-29 21:01:39 [INFO]: epoch 12: training loss 0.4082\n",
      "2025-05-29 21:01:39 [INFO]: epoch 13: training loss 0.4379\n",
      "2025-05-29 21:01:39 [INFO]: epoch 14: training loss 0.4326\n",
      "2025-05-29 21:01:39 [INFO]: epoch 15: training loss 0.4322\n",
      "2025-05-29 21:01:39 [INFO]: epoch 16: training loss 0.3680\n",
      "2025-05-29 21:01:39 [INFO]: epoch 17: training loss 0.4300\n",
      "2025-05-29 21:01:39 [INFO]: epoch 18: training loss 0.3836\n",
      "2025-05-29 21:01:39 [INFO]: epoch 19: training loss 0.4119\n",
      "2025-05-29 21:01:39 [INFO]: epoch 20: training loss 0.4243\n",
      "2025-05-29 21:01:39 [INFO]: epoch 21: training loss 0.4033\n",
      "2025-05-29 21:01:39 [INFO]: epoch 22: training loss 0.3805\n",
      "2025-05-29 21:01:39 [INFO]: epoch 23: training loss 0.3864\n",
      "2025-05-29 21:01:39 [INFO]: epoch 24: training loss 0.4023\n",
      "2025-05-29 21:01:39 [INFO]: epoch 25: training loss 0.3978\n",
      "2025-05-29 21:01:39 [INFO]: epoch 26: training loss 0.3851\n",
      "2025-05-29 21:01:39 [INFO]: epoch 27: training loss 0.3760\n",
      "2025-05-29 21:01:39 [INFO]: epoch 28: training loss 0.3751\n",
      "2025-05-29 21:01:39 [INFO]: epoch 29: training loss 0.3850\n",
      "2025-05-29 21:01:39 [INFO]: epoch 30: training loss 0.3694\n",
      "2025-05-29 21:01:39 [INFO]: epoch 31: training loss 0.3672\n",
      "2025-05-29 21:01:39 [INFO]: epoch 32: training loss 0.3375\n",
      "2025-05-29 21:01:39 [INFO]: epoch 33: training loss 0.3750\n",
      "2025-05-29 21:01:39 [INFO]: epoch 34: training loss 0.3700\n",
      "2025-05-29 21:01:39 [INFO]: epoch 35: training loss 0.3591\n",
      "2025-05-29 21:01:39 [INFO]: epoch 36: training loss 0.3422\n",
      "2025-05-29 21:01:39 [INFO]: epoch 37: training loss 0.3526\n",
      "2025-05-29 21:01:39 [INFO]: epoch 38: training loss 0.3486\n",
      "2025-05-29 21:01:39 [INFO]: epoch 39: training loss 0.3373\n",
      "2025-05-29 21:01:39 [INFO]: epoch 40: training loss 0.3342\n",
      "2025-05-29 21:01:39 [INFO]: epoch 41: training loss 0.3445\n",
      "2025-05-29 21:01:39 [INFO]: epoch 42: training loss 0.3108\n",
      "2025-05-29 21:01:39 [INFO]: epoch 43: training loss 0.3365\n",
      "2025-05-29 21:01:39 [INFO]: epoch 44: training loss 0.3289\n",
      "2025-05-29 21:01:39 [INFO]: epoch 45: training loss 0.3142\n",
      "2025-05-29 21:01:39 [INFO]: epoch 46: training loss 0.3133\n",
      "2025-05-29 21:01:39 [INFO]: epoch 47: training loss 0.3288\n",
      "2025-05-29 21:01:39 [INFO]: epoch 48: training loss 0.3462\n",
      "2025-05-29 21:01:39 [INFO]: epoch 49: training loss 0.3313\n",
      "2025-05-29 21:01:39 [INFO]: epoch 50: training loss 0.3182\n",
      "2025-05-29 21:01:39 [INFO]: epoch 51: training loss 0.3054\n",
      "2025-05-29 21:01:39 [INFO]: epoch 52: training loss 0.3108\n",
      "2025-05-29 21:01:39 [INFO]: epoch 53: training loss 0.3146\n",
      "2025-05-29 21:01:39 [INFO]: epoch 54: training loss 0.3174\n",
      "2025-05-29 21:01:39 [INFO]: epoch 55: training loss 0.3063\n",
      "2025-05-29 21:01:39 [INFO]: epoch 56: training loss 0.3069\n",
      "2025-05-29 21:01:39 [INFO]: epoch 57: training loss 0.3068\n",
      "2025-05-29 21:01:40 [INFO]: epoch 58: training loss 0.3257\n",
      "2025-05-29 21:01:40 [INFO]: epoch 59: training loss 0.2958\n",
      "2025-05-29 21:01:40 [INFO]: epoch 60: training loss 0.3069\n",
      "2025-05-29 21:01:40 [INFO]: epoch 61: training loss 0.3184\n",
      "2025-05-29 21:01:40 [INFO]: epoch 62: training loss 0.2844\n",
      "2025-05-29 21:01:40 [INFO]: epoch 63: training loss 0.3047\n",
      "2025-05-29 21:01:40 [INFO]: epoch 64: training loss 0.3066\n",
      "2025-05-29 21:01:40 [INFO]: epoch 65: training loss 0.2778\n",
      "2025-05-29 21:01:40 [INFO]: epoch 66: training loss 0.2834\n",
      "2025-05-29 21:01:40 [INFO]: epoch 67: training loss 0.3000\n",
      "2025-05-29 21:01:40 [INFO]: epoch 68: training loss 0.2792\n",
      "2025-05-29 21:01:40 [INFO]: epoch 69: training loss 0.2990\n",
      "2025-05-29 21:01:40 [INFO]: epoch 70: training loss 0.2915\n",
      "2025-05-29 21:01:40 [INFO]: epoch 71: training loss 0.2850\n",
      "2025-05-29 21:01:40 [INFO]: epoch 72: training loss 0.2969\n",
      "2025-05-29 21:01:40 [INFO]: epoch 73: training loss 0.2744\n",
      "2025-05-29 21:01:40 [INFO]: epoch 74: training loss 0.2783\n",
      "2025-05-29 21:01:40 [INFO]: epoch 75: training loss 0.2902\n",
      "2025-05-29 21:01:40 [INFO]: epoch 76: training loss 0.2897\n",
      "2025-05-29 21:01:40 [INFO]: epoch 77: training loss 0.2613\n",
      "2025-05-29 21:01:40 [INFO]: epoch 78: training loss 0.3052\n",
      "2025-05-29 21:01:40 [INFO]: epoch 79: training loss 0.2619\n",
      "2025-05-29 21:01:40 [INFO]: epoch 80: training loss 0.2823\n",
      "2025-05-29 21:01:40 [INFO]: epoch 81: training loss 0.2837\n",
      "2025-05-29 21:01:40 [INFO]: epoch 82: training loss 0.2807\n",
      "2025-05-29 21:01:40 [INFO]: epoch 83: training loss 0.2946\n",
      "2025-05-29 21:01:40 [INFO]: epoch 84: training loss 0.2698\n",
      "2025-05-29 21:01:40 [INFO]: epoch 85: training loss 0.2660\n",
      "2025-05-29 21:01:40 [INFO]: epoch 86: training loss 0.2737\n",
      "2025-05-29 21:01:40 [INFO]: epoch 87: training loss 0.2736\n",
      "2025-05-29 21:01:40 [INFO]: epoch 88: training loss 0.2575\n",
      "2025-05-29 21:01:40 [INFO]: epoch 89: training loss 0.2649\n",
      "2025-05-29 21:01:40 [INFO]: epoch 90: training loss 0.2967\n",
      "2025-05-29 21:01:40 [INFO]: epoch 91: training loss 0.2769\n",
      "2025-05-29 21:01:40 [INFO]: epoch 92: training loss 0.2612\n",
      "2025-05-29 21:01:40 [INFO]: epoch 93: training loss 0.2694\n",
      "2025-05-29 21:01:40 [INFO]: epoch 94: training loss 0.2681\n",
      "2025-05-29 21:01:40 [INFO]: epoch 95: training loss 0.2688\n",
      "2025-05-29 21:01:40 [INFO]: epoch 96: training loss 0.2604\n",
      "2025-05-29 21:01:40 [INFO]: epoch 97: training loss 0.2628\n",
      "2025-05-29 21:01:40 [INFO]: epoch 98: training loss 0.2554\n",
      "2025-05-29 21:01:40 [INFO]: epoch 99: training loss 0.2569\n",
      "2025-05-29 21:01:40 [INFO]: epoch 100: training loss 0.2688\n",
      "2025-05-29 21:01:40 [INFO]: epoch 101: training loss 0.2652\n",
      "2025-05-29 21:01:40 [INFO]: epoch 102: training loss 0.2751\n",
      "2025-05-29 21:01:40 [INFO]: epoch 103: training loss 0.2796\n",
      "2025-05-29 21:01:40 [INFO]: epoch 104: training loss 0.2557\n",
      "2025-05-29 21:01:40 [INFO]: epoch 105: training loss 0.2744\n",
      "2025-05-29 21:01:40 [INFO]: epoch 106: training loss 0.2674\n",
      "2025-05-29 21:01:40 [INFO]: epoch 107: training loss 0.2846\n",
      "2025-05-29 21:01:40 [INFO]: epoch 108: training loss 0.2583\n",
      "2025-05-29 21:01:40 [INFO]: epoch 109: training loss 0.2502\n",
      "2025-05-29 21:01:40 [INFO]: epoch 110: training loss 0.2627\n",
      "2025-05-29 21:01:40 [INFO]: epoch 111: training loss 0.2652\n",
      "2025-05-29 21:01:40 [INFO]: epoch 112: training loss 0.2546\n",
      "2025-05-29 21:01:40 [INFO]: epoch 113: training loss 0.2498\n",
      "2025-05-29 21:01:40 [INFO]: epoch 114: training loss 0.2510\n",
      "2025-05-29 21:01:40 [INFO]: epoch 115: training loss 0.2388\n",
      "2025-05-29 21:01:40 [INFO]: epoch 116: training loss 0.2438\n",
      "2025-05-29 21:01:40 [INFO]: epoch 117: training loss 0.2503\n",
      "2025-05-29 21:01:40 [INFO]: epoch 118: training loss 0.2575\n",
      "2025-05-29 21:01:40 [INFO]: epoch 119: training loss 0.2525\n",
      "2025-05-29 21:01:40 [INFO]: epoch 120: training loss 0.2522\n",
      "2025-05-29 21:01:40 [INFO]: epoch 121: training loss 0.2408\n",
      "2025-05-29 21:01:40 [INFO]: epoch 122: training loss 0.2260\n",
      "2025-05-29 21:01:40 [INFO]: epoch 123: training loss 0.2343\n",
      "2025-05-29 21:01:40 [INFO]: epoch 124: training loss 0.2799\n",
      "2025-05-29 21:01:40 [INFO]: epoch 125: training loss 0.2644\n",
      "2025-05-29 21:01:40 [INFO]: epoch 126: training loss 0.2453\n",
      "2025-05-29 21:01:40 [INFO]: epoch 127: training loss 0.2413\n",
      "2025-05-29 21:01:40 [INFO]: epoch 128: training loss 0.2263\n",
      "2025-05-29 21:01:40 [INFO]: epoch 129: training loss 0.2679\n",
      "2025-05-29 21:01:40 [INFO]: epoch 130: training loss 0.2542\n",
      "2025-05-29 21:01:40 [INFO]: epoch 131: training loss 0.2399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:40 [INFO]: epoch 132: training loss 0.2361\n",
      "2025-05-29 21:01:40 [INFO]: epoch 133: training loss 0.2278\n",
      "2025-05-29 21:01:40 [INFO]: epoch 134: training loss 0.2505\n",
      "2025-05-29 21:01:40 [INFO]: epoch 135: training loss 0.2344\n",
      "2025-05-29 21:01:40 [INFO]: epoch 136: training loss 0.2379\n",
      "2025-05-29 21:01:40 [INFO]: epoch 137: training loss 0.2336\n",
      "2025-05-29 21:01:41 [INFO]: epoch 138: training loss 0.2167\n",
      "2025-05-29 21:01:41 [INFO]: epoch 139: training loss 0.2317\n",
      "2025-05-29 21:01:41 [INFO]: epoch 140: training loss 0.2172\n",
      "2025-05-29 21:01:41 [INFO]: epoch 141: training loss 0.2294\n",
      "2025-05-29 21:01:41 [INFO]: epoch 142: training loss 0.2206\n",
      "2025-05-29 21:01:41 [INFO]: epoch 143: training loss 0.2246\n",
      "2025-05-29 21:01:41 [INFO]: epoch 144: training loss 0.2309\n",
      "2025-05-29 21:01:41 [INFO]: epoch 145: training loss 0.2148\n",
      "2025-05-29 21:01:41 [INFO]: epoch 146: training loss 0.2204\n",
      "2025-05-29 21:01:41 [INFO]: epoch 147: training loss 0.2308\n",
      "2025-05-29 21:01:41 [INFO]: epoch 148: training loss 0.2172\n",
      "2025-05-29 21:01:41 [INFO]: epoch 149: training loss 0.2186\n",
      "2025-05-29 21:01:41 [INFO]: epoch 150: training loss 0.2345\n",
      "2025-05-29 21:01:41 [INFO]: epoch 151: training loss 0.2373\n",
      "2025-05-29 21:01:41 [INFO]: epoch 152: training loss 0.2260\n",
      "2025-05-29 21:01:41 [INFO]: epoch 153: training loss 0.2095\n",
      "2025-05-29 21:01:41 [INFO]: epoch 154: training loss 0.2184\n",
      "2025-05-29 21:01:41 [INFO]: epoch 155: training loss 0.2248\n",
      "2025-05-29 21:01:41 [INFO]: epoch 156: training loss 0.2182\n",
      "2025-05-29 21:01:41 [INFO]: epoch 157: training loss 0.2137\n",
      "2025-05-29 21:01:41 [INFO]: epoch 158: training loss 0.2122\n",
      "2025-05-29 21:01:41 [INFO]: epoch 159: training loss 0.2229\n",
      "2025-05-29 21:01:41 [INFO]: epoch 160: training loss 0.2010\n",
      "2025-05-29 21:01:41 [INFO]: epoch 161: training loss 0.2140\n",
      "2025-05-29 21:01:41 [INFO]: epoch 162: training loss 0.2102\n",
      "2025-05-29 21:01:41 [INFO]: epoch 163: training loss 0.2128\n",
      "2025-05-29 21:01:41 [INFO]: epoch 164: training loss 0.2139\n",
      "2025-05-29 21:01:41 [INFO]: epoch 165: training loss 0.2260\n",
      "2025-05-29 21:01:41 [INFO]: epoch 166: training loss 0.2053\n",
      "2025-05-29 21:01:41 [INFO]: epoch 167: training loss 0.2176\n",
      "2025-05-29 21:01:41 [INFO]: epoch 168: training loss 0.2371\n",
      "2025-05-29 21:01:41 [INFO]: epoch 169: training loss 0.2238\n",
      "2025-05-29 21:01:41 [INFO]: epoch 170: training loss 0.2007\n",
      "2025-05-29 21:01:41 [INFO]: epoch 171: training loss 0.2079\n",
      "2025-05-29 21:01:41 [INFO]: epoch 172: training loss 0.2026\n",
      "2025-05-29 21:01:41 [INFO]: epoch 173: training loss 0.2154\n",
      "2025-05-29 21:01:41 [INFO]: epoch 174: training loss 0.2219\n",
      "2025-05-29 21:01:41 [INFO]: epoch 175: training loss 0.2042\n",
      "2025-05-29 21:01:41 [INFO]: epoch 176: training loss 0.2132\n",
      "2025-05-29 21:01:41 [INFO]: epoch 177: training loss 0.2197\n",
      "2025-05-29 21:01:41 [INFO]: epoch 178: training loss 0.2236\n",
      "2025-05-29 21:01:41 [INFO]: epoch 179: training loss 0.2137\n",
      "2025-05-29 21:01:41 [INFO]: epoch 180: training loss 0.2064\n",
      "2025-05-29 21:01:41 [INFO]: epoch 181: training loss 0.2250\n",
      "2025-05-29 21:01:41 [INFO]: epoch 182: training loss 0.2028\n",
      "2025-05-29 21:01:41 [INFO]: epoch 183: training loss 0.2042\n",
      "2025-05-29 21:01:41 [INFO]: epoch 184: training loss 0.2051\n",
      "2025-05-29 21:01:41 [INFO]: epoch 185: training loss 0.2187\n",
      "2025-05-29 21:01:41 [INFO]: epoch 186: training loss 0.2054\n",
      "2025-05-29 21:01:41 [INFO]: epoch 187: training loss 0.2144\n",
      "2025-05-29 21:01:41 [INFO]: epoch 188: training loss 0.2139\n",
      "2025-05-29 21:01:41 [INFO]: epoch 189: training loss 0.2022\n",
      "2025-05-29 21:01:41 [INFO]: epoch 190: training loss 0.1949\n",
      "2025-05-29 21:01:41 [INFO]: epoch 191: training loss 0.2088\n",
      "2025-05-29 21:01:41 [INFO]: epoch 192: training loss 0.2212\n",
      "2025-05-29 21:01:41 [INFO]: epoch 193: training loss 0.2112\n",
      "2025-05-29 21:01:41 [INFO]: epoch 194: training loss 0.1897\n",
      "2025-05-29 21:01:41 [INFO]: epoch 195: training loss 0.2033\n",
      "2025-05-29 21:01:41 [INFO]: epoch 196: training loss 0.2208\n",
      "2025-05-29 21:01:41 [INFO]: epoch 197: training loss 0.2140\n",
      "2025-05-29 21:01:41 [INFO]: epoch 198: training loss 0.2084\n",
      "2025-05-29 21:01:41 [INFO]: epoch 199: training loss 0.1863\n",
      "2025-05-29 21:01:41 [INFO]: epoch 200: training loss 0.1922\n",
      "2025-05-29 21:01:41 [INFO]: epoch 201: training loss 0.1930\n",
      "2025-05-29 21:01:41 [INFO]: epoch 202: training loss 0.2110\n",
      "2025-05-29 21:01:41 [INFO]: epoch 203: training loss 0.1969\n",
      "2025-05-29 21:01:41 [INFO]: epoch 204: training loss 0.1931\n",
      "2025-05-29 21:01:41 [INFO]: epoch 205: training loss 0.1875\n",
      "2025-05-29 21:01:41 [INFO]: epoch 206: training loss 0.1938\n",
      "2025-05-29 21:01:41 [INFO]: epoch 207: training loss 0.1958\n",
      "2025-05-29 21:01:41 [INFO]: epoch 208: training loss 0.1994\n",
      "2025-05-29 21:01:41 [INFO]: epoch 209: training loss 0.1722\n",
      "2025-05-29 21:01:41 [INFO]: epoch 210: training loss 0.1804\n",
      "2025-05-29 21:01:41 [INFO]: epoch 211: training loss 0.1852\n",
      "2025-05-29 21:01:41 [INFO]: epoch 212: training loss 0.2020\n",
      "2025-05-29 21:01:41 [INFO]: epoch 213: training loss 0.1982\n",
      "2025-05-29 21:01:41 [INFO]: epoch 214: training loss 0.1898\n",
      "2025-05-29 21:01:41 [INFO]: epoch 215: training loss 0.1844\n",
      "2025-05-29 21:01:41 [INFO]: epoch 216: training loss 0.1939\n",
      "2025-05-29 21:01:41 [INFO]: epoch 217: training loss 0.1863\n",
      "2025-05-29 21:01:41 [INFO]: epoch 218: training loss 0.1880\n",
      "2025-05-29 21:01:42 [INFO]: epoch 219: training loss 0.1859\n",
      "2025-05-29 21:01:42 [INFO]: epoch 220: training loss 0.1762\n",
      "2025-05-29 21:01:42 [INFO]: epoch 221: training loss 0.1901\n",
      "2025-05-29 21:01:42 [INFO]: epoch 222: training loss 0.1784\n",
      "2025-05-29 21:01:42 [INFO]: epoch 223: training loss 0.1730\n",
      "2025-05-29 21:01:42 [INFO]: epoch 224: training loss 0.1957\n",
      "2025-05-29 21:01:42 [INFO]: epoch 225: training loss 0.1897\n",
      "2025-05-29 21:01:42 [INFO]: epoch 226: training loss 0.1822\n",
      "2025-05-29 21:01:42 [INFO]: epoch 227: training loss 0.1679\n",
      "2025-05-29 21:01:42 [INFO]: epoch 228: training loss 0.1957\n",
      "2025-05-29 21:01:42 [INFO]: epoch 229: training loss 0.1791\n",
      "2025-05-29 21:01:42 [INFO]: epoch 230: training loss 0.1742\n",
      "2025-05-29 21:01:42 [INFO]: epoch 231: training loss 0.1694\n",
      "2025-05-29 21:01:42 [INFO]: epoch 232: training loss 0.1809\n",
      "2025-05-29 21:01:42 [INFO]: epoch 233: training loss 0.1734\n",
      "2025-05-29 21:01:42 [INFO]: epoch 234: training loss 0.1846\n",
      "2025-05-29 21:01:42 [INFO]: epoch 235: training loss 0.1759\n",
      "2025-05-29 21:01:42 [INFO]: epoch 236: training loss 0.1822\n",
      "2025-05-29 21:01:42 [INFO]: epoch 237: training loss 0.1811\n",
      "2025-05-29 21:01:42 [INFO]: epoch 238: training loss 0.1749\n",
      "2025-05-29 21:01:42 [INFO]: epoch 239: training loss 0.1841\n",
      "2025-05-29 21:01:42 [INFO]: epoch 240: training loss 0.1679\n",
      "2025-05-29 21:01:42 [INFO]: epoch 241: training loss 0.1742\n",
      "2025-05-29 21:01:42 [INFO]: epoch 242: training loss 0.1684\n",
      "2025-05-29 21:01:42 [INFO]: epoch 243: training loss 0.1696\n",
      "2025-05-29 21:01:42 [INFO]: epoch 244: training loss 0.1660\n",
      "2025-05-29 21:01:42 [INFO]: epoch 245: training loss 0.1698\n",
      "2025-05-29 21:01:42 [INFO]: epoch 246: training loss 0.1691\n",
      "2025-05-29 21:01:42 [INFO]: epoch 247: training loss 0.1722\n",
      "2025-05-29 21:01:42 [INFO]: epoch 248: training loss 0.1669\n",
      "2025-05-29 21:01:42 [INFO]: epoch 249: training loss 0.1616\n",
      "2025-05-29 21:01:42 [INFO]: epoch 250: training loss 0.1785\n",
      "2025-05-29 21:01:42 [INFO]: epoch 251: training loss 0.1694\n",
      "2025-05-29 21:01:42 [INFO]: epoch 252: training loss 0.1643\n",
      "2025-05-29 21:01:42 [INFO]: epoch 253: training loss 0.1629\n",
      "2025-05-29 21:01:42 [INFO]: epoch 254: training loss 0.1640\n",
      "2025-05-29 21:01:42 [INFO]: epoch 255: training loss 0.1607\n",
      "2025-05-29 21:01:42 [INFO]: epoch 256: training loss 0.1624\n",
      "2025-05-29 21:01:42 [INFO]: epoch 257: training loss 0.1606\n",
      "2025-05-29 21:01:42 [INFO]: epoch 258: training loss 0.1684\n",
      "2025-05-29 21:01:42 [INFO]: epoch 259: training loss 0.1591\n",
      "2025-05-29 21:01:42 [INFO]: epoch 260: training loss 0.1530\n",
      "2025-05-29 21:01:42 [INFO]: epoch 261: training loss 0.1564\n",
      "2025-05-29 21:01:42 [INFO]: epoch 262: training loss 0.1644\n",
      "2025-05-29 21:01:42 [INFO]: epoch 263: training loss 0.1614\n",
      "2025-05-29 21:01:42 [INFO]: epoch 264: training loss 0.1611\n",
      "2025-05-29 21:01:42 [INFO]: epoch 265: training loss 0.1753\n",
      "2025-05-29 21:01:42 [INFO]: epoch 266: training loss 0.1613\n",
      "2025-05-29 21:01:42 [INFO]: epoch 267: training loss 0.1619\n",
      "2025-05-29 21:01:42 [INFO]: epoch 268: training loss 0.1577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:42 [INFO]: epoch 269: training loss 0.1664\n",
      "2025-05-29 21:01:42 [INFO]: epoch 270: training loss 0.1609\n",
      "2025-05-29 21:01:42 [INFO]: epoch 271: training loss 0.1625\n",
      "2025-05-29 21:01:42 [INFO]: epoch 272: training loss 0.1608\n",
      "2025-05-29 21:01:42 [INFO]: epoch 273: training loss 0.1631\n",
      "2025-05-29 21:01:42 [INFO]: epoch 274: training loss 0.1598\n",
      "2025-05-29 21:01:42 [INFO]: epoch 275: training loss 0.1625\n",
      "2025-05-29 21:01:42 [INFO]: epoch 276: training loss 0.1591\n",
      "2025-05-29 21:01:42 [INFO]: epoch 277: training loss 0.1482\n",
      "2025-05-29 21:01:42 [INFO]: epoch 278: training loss 0.1533\n",
      "2025-05-29 21:01:42 [INFO]: epoch 279: training loss 0.1529\n",
      "2025-05-29 21:01:42 [INFO]: epoch 280: training loss 0.1566\n",
      "2025-05-29 21:01:42 [INFO]: epoch 281: training loss 0.1514\n",
      "2025-05-29 21:01:42 [INFO]: epoch 282: training loss 0.1444\n",
      "2025-05-29 21:01:42 [INFO]: epoch 283: training loss 0.1595\n",
      "2025-05-29 21:01:42 [INFO]: epoch 284: training loss 0.1472\n",
      "2025-05-29 21:01:42 [INFO]: epoch 285: training loss 0.1579\n",
      "2025-05-29 21:01:42 [INFO]: epoch 286: training loss 0.1444\n",
      "2025-05-29 21:01:42 [INFO]: epoch 287: training loss 0.1500\n",
      "2025-05-29 21:01:42 [INFO]: epoch 288: training loss 0.1644\n",
      "2025-05-29 21:01:42 [INFO]: epoch 289: training loss 0.1597\n",
      "2025-05-29 21:01:42 [INFO]: epoch 290: training loss 0.1537\n",
      "2025-05-29 21:01:42 [INFO]: epoch 291: training loss 0.1463\n",
      "2025-05-29 21:01:42 [INFO]: epoch 292: training loss 0.1520\n",
      "2025-05-29 21:01:42 [INFO]: epoch 293: training loss 0.1470\n",
      "2025-05-29 21:01:42 [INFO]: epoch 294: training loss 0.1551\n",
      "2025-05-29 21:01:42 [INFO]: epoch 295: training loss 0.1501\n",
      "2025-05-29 21:01:42 [INFO]: epoch 296: training loss 0.1475\n",
      "2025-05-29 21:01:42 [INFO]: epoch 297: training loss 0.1489\n",
      "2025-05-29 21:01:42 [INFO]: epoch 298: training loss 0.1572\n",
      "2025-05-29 21:01:42 [INFO]: epoch 299: training loss 0.1547\n",
      "2025-05-29 21:01:42 [INFO]: Finished training.\n",
      "2025-05-29 21:01:42 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.85s/it]\n",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]2025-05-29 21:01:43 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:01:43 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:01:43 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:01:43 [INFO]: epoch 0: training loss 1.0210\n",
      "2025-05-29 21:01:43 [INFO]: epoch 1: training loss 0.6844\n",
      "2025-05-29 21:01:43 [INFO]: epoch 2: training loss 0.5720\n",
      "2025-05-29 21:01:43 [INFO]: epoch 3: training loss 0.5474\n",
      "2025-05-29 21:01:43 [INFO]: epoch 4: training loss 0.3920\n",
      "2025-05-29 21:01:43 [INFO]: epoch 5: training loss 0.3717\n",
      "2025-05-29 21:01:43 [INFO]: epoch 6: training loss 0.3846\n",
      "2025-05-29 21:01:43 [INFO]: epoch 7: training loss 0.3821\n",
      "2025-05-29 21:01:43 [INFO]: epoch 8: training loss 0.3420\n",
      "2025-05-29 21:01:43 [INFO]: epoch 9: training loss 0.3343\n",
      "2025-05-29 21:01:43 [INFO]: epoch 10: training loss 0.2939\n",
      "2025-05-29 21:01:43 [INFO]: epoch 11: training loss 0.2898\n",
      "2025-05-29 21:01:43 [INFO]: epoch 12: training loss 0.2721\n",
      "2025-05-29 21:01:43 [INFO]: epoch 13: training loss 0.2850\n",
      "2025-05-29 21:01:43 [INFO]: epoch 14: training loss 0.3245\n",
      "2025-05-29 21:01:43 [INFO]: epoch 15: training loss 0.2853\n",
      "2025-05-29 21:01:43 [INFO]: epoch 16: training loss 0.2814\n",
      "2025-05-29 21:01:43 [INFO]: epoch 17: training loss 0.2200\n",
      "2025-05-29 21:01:43 [INFO]: epoch 18: training loss 0.2029\n",
      "2025-05-29 21:01:43 [INFO]: epoch 19: training loss 0.2498\n",
      "2025-05-29 21:01:43 [INFO]: epoch 20: training loss 0.2314\n",
      "2025-05-29 21:01:43 [INFO]: epoch 21: training loss 0.2325\n",
      "2025-05-29 21:01:43 [INFO]: epoch 22: training loss 0.2072\n",
      "2025-05-29 21:01:43 [INFO]: epoch 23: training loss 0.1588\n",
      "2025-05-29 21:01:43 [INFO]: epoch 24: training loss 0.1899\n",
      "2025-05-29 21:01:43 [INFO]: epoch 25: training loss 0.1941\n",
      "2025-05-29 21:01:43 [INFO]: epoch 26: training loss 0.2129\n",
      "2025-05-29 21:01:43 [INFO]: epoch 27: training loss 0.2035\n",
      "2025-05-29 21:01:43 [INFO]: epoch 28: training loss 0.1710\n",
      "2025-05-29 21:01:43 [INFO]: epoch 29: training loss 0.1655\n",
      "2025-05-29 21:01:43 [INFO]: epoch 30: training loss 0.1719\n",
      "2025-05-29 21:01:43 [INFO]: epoch 31: training loss 0.1819\n",
      "2025-05-29 21:01:43 [INFO]: epoch 32: training loss 0.1785\n",
      "2025-05-29 21:01:43 [INFO]: epoch 33: training loss 0.1633\n",
      "2025-05-29 21:01:43 [INFO]: epoch 34: training loss 0.1600\n",
      "2025-05-29 21:01:43 [INFO]: epoch 35: training loss 0.1876\n",
      "2025-05-29 21:01:43 [INFO]: epoch 36: training loss 0.1924\n",
      "2025-05-29 21:01:43 [INFO]: epoch 37: training loss 0.1471\n",
      "2025-05-29 21:01:43 [INFO]: epoch 38: training loss 0.1401\n",
      "2025-05-29 21:01:43 [INFO]: epoch 39: training loss 0.1613\n",
      "2025-05-29 21:01:43 [INFO]: epoch 40: training loss 0.1739\n",
      "2025-05-29 21:01:43 [INFO]: epoch 41: training loss 0.1760\n",
      "2025-05-29 21:01:43 [INFO]: epoch 42: training loss 0.1341\n",
      "2025-05-29 21:01:43 [INFO]: epoch 43: training loss 0.1470\n",
      "2025-05-29 21:01:43 [INFO]: epoch 44: training loss 0.1619\n",
      "2025-05-29 21:01:43 [INFO]: epoch 45: training loss 0.1640\n",
      "2025-05-29 21:01:43 [INFO]: epoch 46: training loss 0.1494\n",
      "2025-05-29 21:01:43 [INFO]: epoch 47: training loss 0.1336\n",
      "2025-05-29 21:01:43 [INFO]: epoch 48: training loss 0.1510\n",
      "2025-05-29 21:01:43 [INFO]: epoch 49: training loss 0.1498\n",
      "2025-05-29 21:01:43 [INFO]: epoch 50: training loss 0.1221\n",
      "2025-05-29 21:01:43 [INFO]: epoch 51: training loss 0.1416\n",
      "2025-05-29 21:01:43 [INFO]: epoch 52: training loss 0.1606\n",
      "2025-05-29 21:01:43 [INFO]: epoch 53: training loss 0.1521\n",
      "2025-05-29 21:01:43 [INFO]: epoch 54: training loss 0.1153\n",
      "2025-05-29 21:01:43 [INFO]: epoch 55: training loss 0.1217\n",
      "2025-05-29 21:01:43 [INFO]: epoch 56: training loss 0.1209\n",
      "2025-05-29 21:01:43 [INFO]: epoch 57: training loss 0.1326\n",
      "2025-05-29 21:01:43 [INFO]: epoch 58: training loss 0.1232\n",
      "2025-05-29 21:01:43 [INFO]: epoch 59: training loss 0.1293\n",
      "2025-05-29 21:01:43 [INFO]: epoch 60: training loss 0.1177\n",
      "2025-05-29 21:01:43 [INFO]: epoch 61: training loss 0.1239\n",
      "2025-05-29 21:01:43 [INFO]: epoch 62: training loss 0.1137\n",
      "2025-05-29 21:01:43 [INFO]: epoch 63: training loss 0.1103\n",
      "2025-05-29 21:01:43 [INFO]: epoch 64: training loss 0.1265\n",
      "2025-05-29 21:01:43 [INFO]: epoch 65: training loss 0.1342\n",
      "2025-05-29 21:01:43 [INFO]: epoch 66: training loss 0.1102\n",
      "2025-05-29 21:01:43 [INFO]: epoch 67: training loss 0.0997\n",
      "2025-05-29 21:01:43 [INFO]: epoch 68: training loss 0.1180\n",
      "2025-05-29 21:01:43 [INFO]: epoch 69: training loss 0.1206\n",
      "2025-05-29 21:01:43 [INFO]: epoch 70: training loss 0.0994\n",
      "2025-05-29 21:01:43 [INFO]: epoch 71: training loss 0.1038\n",
      "2025-05-29 21:01:43 [INFO]: epoch 72: training loss 0.1116\n",
      "2025-05-29 21:01:44 [INFO]: epoch 73: training loss 0.1226\n",
      "2025-05-29 21:01:44 [INFO]: epoch 74: training loss 0.1118\n",
      "2025-05-29 21:01:44 [INFO]: epoch 75: training loss 0.1133\n",
      "2025-05-29 21:01:44 [INFO]: epoch 76: training loss 0.0979\n",
      "2025-05-29 21:01:44 [INFO]: epoch 77: training loss 0.0989\n",
      "2025-05-29 21:01:44 [INFO]: epoch 78: training loss 0.1137\n",
      "2025-05-29 21:01:44 [INFO]: epoch 79: training loss 0.0941\n",
      "2025-05-29 21:01:44 [INFO]: epoch 80: training loss 0.0961\n",
      "2025-05-29 21:01:44 [INFO]: epoch 81: training loss 0.0919\n",
      "2025-05-29 21:01:44 [INFO]: epoch 82: training loss 0.0965\n",
      "2025-05-29 21:01:44 [INFO]: epoch 83: training loss 0.1051\n",
      "2025-05-29 21:01:44 [INFO]: epoch 84: training loss 0.1003\n",
      "2025-05-29 21:01:44 [INFO]: epoch 85: training loss 0.0972\n",
      "2025-05-29 21:01:44 [INFO]: epoch 86: training loss 0.0867\n",
      "2025-05-29 21:01:44 [INFO]: epoch 87: training loss 0.1200\n",
      "2025-05-29 21:01:44 [INFO]: epoch 88: training loss 0.1100\n",
      "2025-05-29 21:01:44 [INFO]: epoch 89: training loss 0.1018\n",
      "2025-05-29 21:01:44 [INFO]: epoch 90: training loss 0.1040\n",
      "2025-05-29 21:01:44 [INFO]: epoch 91: training loss 0.1227\n",
      "2025-05-29 21:01:44 [INFO]: epoch 92: training loss 0.0996\n",
      "2025-05-29 21:01:44 [INFO]: epoch 93: training loss 0.1110\n",
      "2025-05-29 21:01:44 [INFO]: epoch 94: training loss 0.1052\n",
      "2025-05-29 21:01:44 [INFO]: epoch 95: training loss 0.1079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:44 [INFO]: epoch 96: training loss 0.0926\n",
      "2025-05-29 21:01:44 [INFO]: epoch 97: training loss 0.1165\n",
      "2025-05-29 21:01:44 [INFO]: epoch 98: training loss 0.1071\n",
      "2025-05-29 21:01:44 [INFO]: epoch 99: training loss 0.1067\n",
      "2025-05-29 21:01:44 [INFO]: epoch 100: training loss 0.1117\n",
      "2025-05-29 21:01:44 [INFO]: epoch 101: training loss 0.1237\n",
      "2025-05-29 21:01:44 [INFO]: epoch 102: training loss 0.1346\n",
      "2025-05-29 21:01:44 [INFO]: epoch 103: training loss 0.1127\n",
      "2025-05-29 21:01:44 [INFO]: epoch 104: training loss 0.1085\n",
      "2025-05-29 21:01:44 [INFO]: epoch 105: training loss 0.1302\n",
      "2025-05-29 21:01:44 [INFO]: epoch 106: training loss 0.1215\n",
      "2025-05-29 21:01:44 [INFO]: epoch 107: training loss 0.1000\n",
      "2025-05-29 21:01:44 [INFO]: epoch 108: training loss 0.1035\n",
      "2025-05-29 21:01:44 [INFO]: epoch 109: training loss 0.0998\n",
      "2025-05-29 21:01:44 [INFO]: epoch 110: training loss 0.1005\n",
      "2025-05-29 21:01:44 [INFO]: epoch 111: training loss 0.0968\n",
      "2025-05-29 21:01:44 [INFO]: epoch 112: training loss 0.0962\n",
      "2025-05-29 21:01:44 [INFO]: epoch 113: training loss 0.0871\n",
      "2025-05-29 21:01:44 [INFO]: epoch 114: training loss 0.0897\n",
      "2025-05-29 21:01:44 [INFO]: epoch 115: training loss 0.0875\n",
      "2025-05-29 21:01:44 [INFO]: epoch 116: training loss 0.0826\n",
      "2025-05-29 21:01:44 [INFO]: epoch 117: training loss 0.0970\n",
      "2025-05-29 21:01:44 [INFO]: epoch 118: training loss 0.1003\n",
      "2025-05-29 21:01:44 [INFO]: epoch 119: training loss 0.1025\n",
      "2025-05-29 21:01:44 [INFO]: epoch 120: training loss 0.1025\n",
      "2025-05-29 21:01:44 [INFO]: epoch 121: training loss 0.1025\n",
      "2025-05-29 21:01:44 [INFO]: epoch 122: training loss 0.0894\n",
      "2025-05-29 21:01:44 [INFO]: epoch 123: training loss 0.0954\n",
      "2025-05-29 21:01:44 [INFO]: epoch 124: training loss 0.0950\n",
      "2025-05-29 21:01:44 [INFO]: epoch 125: training loss 0.0872\n",
      "2025-05-29 21:01:44 [INFO]: epoch 126: training loss 0.0940\n",
      "2025-05-29 21:01:44 [INFO]: epoch 127: training loss 0.1088\n",
      "2025-05-29 21:01:44 [INFO]: epoch 128: training loss 0.0871\n",
      "2025-05-29 21:01:44 [INFO]: epoch 129: training loss 0.0935\n",
      "2025-05-29 21:01:44 [INFO]: epoch 130: training loss 0.1077\n",
      "2025-05-29 21:01:44 [INFO]: epoch 131: training loss 0.1035\n",
      "2025-05-29 21:01:44 [INFO]: epoch 132: training loss 0.0953\n",
      "2025-05-29 21:01:44 [INFO]: epoch 133: training loss 0.1076\n",
      "2025-05-29 21:01:44 [INFO]: epoch 134: training loss 0.1100\n",
      "2025-05-29 21:01:44 [INFO]: epoch 135: training loss 0.0984\n",
      "2025-05-29 21:01:44 [INFO]: epoch 136: training loss 0.0921\n",
      "2025-05-29 21:01:44 [INFO]: epoch 137: training loss 0.1017\n",
      "2025-05-29 21:01:44 [INFO]: epoch 138: training loss 0.0992\n",
      "2025-05-29 21:01:44 [INFO]: epoch 139: training loss 0.0885\n",
      "2025-05-29 21:01:44 [INFO]: epoch 140: training loss 0.0870\n",
      "2025-05-29 21:01:44 [INFO]: epoch 141: training loss 0.0848\n",
      "2025-05-29 21:01:44 [INFO]: epoch 142: training loss 0.0959\n",
      "2025-05-29 21:01:44 [INFO]: epoch 143: training loss 0.0733\n",
      "2025-05-29 21:01:44 [INFO]: epoch 144: training loss 0.0748\n",
      "2025-05-29 21:01:44 [INFO]: epoch 145: training loss 0.0960\n",
      "2025-05-29 21:01:44 [INFO]: epoch 146: training loss 0.0852\n",
      "2025-05-29 21:01:44 [INFO]: epoch 147: training loss 0.0773\n",
      "2025-05-29 21:01:44 [INFO]: epoch 148: training loss 0.0978\n",
      "2025-05-29 21:01:44 [INFO]: epoch 149: training loss 0.0950\n",
      "2025-05-29 21:01:44 [INFO]: epoch 150: training loss 0.0827\n",
      "2025-05-29 21:01:44 [INFO]: epoch 151: training loss 0.0983\n",
      "2025-05-29 21:01:44 [INFO]: epoch 152: training loss 0.0874\n",
      "2025-05-29 21:01:45 [INFO]: epoch 153: training loss 0.0879\n",
      "2025-05-29 21:01:45 [INFO]: epoch 154: training loss 0.0712\n",
      "2025-05-29 21:01:45 [INFO]: epoch 155: training loss 0.0788\n",
      "2025-05-29 21:01:45 [INFO]: epoch 156: training loss 0.0724\n",
      "2025-05-29 21:01:45 [INFO]: epoch 157: training loss 0.0738\n",
      "2025-05-29 21:01:45 [INFO]: epoch 158: training loss 0.0916\n",
      "2025-05-29 21:01:45 [INFO]: epoch 159: training loss 0.0769\n",
      "2025-05-29 21:01:45 [INFO]: epoch 160: training loss 0.0885\n",
      "2025-05-29 21:01:45 [INFO]: epoch 161: training loss 0.0678\n",
      "2025-05-29 21:01:45 [INFO]: epoch 162: training loss 0.0880\n",
      "2025-05-29 21:01:45 [INFO]: epoch 163: training loss 0.0706\n",
      "2025-05-29 21:01:45 [INFO]: epoch 164: training loss 0.0870\n",
      "2025-05-29 21:01:45 [INFO]: epoch 165: training loss 0.0813\n",
      "2025-05-29 21:01:45 [INFO]: epoch 166: training loss 0.0707\n",
      "2025-05-29 21:01:45 [INFO]: epoch 167: training loss 0.0754\n",
      "2025-05-29 21:01:45 [INFO]: epoch 168: training loss 0.0833\n",
      "2025-05-29 21:01:45 [INFO]: epoch 169: training loss 0.0775\n",
      "2025-05-29 21:01:45 [INFO]: epoch 170: training loss 0.0699\n",
      "2025-05-29 21:01:45 [INFO]: epoch 171: training loss 0.0805\n",
      "2025-05-29 21:01:45 [INFO]: epoch 172: training loss 0.0701\n",
      "2025-05-29 21:01:45 [INFO]: epoch 173: training loss 0.0902\n",
      "2025-05-29 21:01:45 [INFO]: epoch 174: training loss 0.0963\n",
      "2025-05-29 21:01:45 [INFO]: epoch 175: training loss 0.0682\n",
      "2025-05-29 21:01:45 [INFO]: epoch 176: training loss 0.0770\n",
      "2025-05-29 21:01:45 [INFO]: epoch 177: training loss 0.0756\n",
      "2025-05-29 21:01:45 [INFO]: epoch 178: training loss 0.0684\n",
      "2025-05-29 21:01:45 [INFO]: epoch 179: training loss 0.0764\n",
      "2025-05-29 21:01:45 [INFO]: epoch 180: training loss 0.0707\n",
      "2025-05-29 21:01:45 [INFO]: epoch 181: training loss 0.0712\n",
      "2025-05-29 21:01:45 [INFO]: epoch 182: training loss 0.0849\n",
      "2025-05-29 21:01:45 [INFO]: epoch 183: training loss 0.0739\n",
      "2025-05-29 21:01:45 [INFO]: epoch 184: training loss 0.0731\n",
      "2025-05-29 21:01:45 [INFO]: epoch 185: training loss 0.0750\n",
      "2025-05-29 21:01:45 [INFO]: epoch 186: training loss 0.0649\n",
      "2025-05-29 21:01:45 [INFO]: epoch 187: training loss 0.0622\n",
      "2025-05-29 21:01:45 [INFO]: epoch 188: training loss 0.0678\n",
      "2025-05-29 21:01:45 [INFO]: epoch 189: training loss 0.0664\n",
      "2025-05-29 21:01:45 [INFO]: epoch 190: training loss 0.0616\n",
      "2025-05-29 21:01:45 [INFO]: epoch 191: training loss 0.0759\n",
      "2025-05-29 21:01:45 [INFO]: epoch 192: training loss 0.0640\n",
      "2025-05-29 21:01:45 [INFO]: epoch 193: training loss 0.0863\n",
      "2025-05-29 21:01:45 [INFO]: epoch 194: training loss 0.0657\n",
      "2025-05-29 21:01:45 [INFO]: epoch 195: training loss 0.0671\n",
      "2025-05-29 21:01:45 [INFO]: epoch 196: training loss 0.0821\n",
      "2025-05-29 21:01:45 [INFO]: epoch 197: training loss 0.0599\n",
      "2025-05-29 21:01:45 [INFO]: epoch 198: training loss 0.0734\n",
      "2025-05-29 21:01:45 [INFO]: epoch 199: training loss 0.0763\n",
      "2025-05-29 21:01:45 [INFO]: epoch 200: training loss 0.0649\n",
      "2025-05-29 21:01:45 [INFO]: epoch 201: training loss 0.0756\n",
      "2025-05-29 21:01:45 [INFO]: epoch 202: training loss 0.0760\n",
      "2025-05-29 21:01:45 [INFO]: epoch 203: training loss 0.0742\n",
      "2025-05-29 21:01:45 [INFO]: epoch 204: training loss 0.0875\n",
      "2025-05-29 21:01:45 [INFO]: epoch 205: training loss 0.0782\n",
      "2025-05-29 21:01:45 [INFO]: epoch 206: training loss 0.0570\n",
      "2025-05-29 21:01:45 [INFO]: epoch 207: training loss 0.0622\n",
      "2025-05-29 21:01:45 [INFO]: epoch 208: training loss 0.0651\n",
      "2025-05-29 21:01:45 [INFO]: epoch 209: training loss 0.0730\n",
      "2025-05-29 21:01:45 [INFO]: epoch 210: training loss 0.0768\n",
      "2025-05-29 21:01:45 [INFO]: epoch 211: training loss 0.0760\n",
      "2025-05-29 21:01:45 [INFO]: epoch 212: training loss 0.0733\n",
      "2025-05-29 21:01:45 [INFO]: epoch 213: training loss 0.0635\n",
      "2025-05-29 21:01:45 [INFO]: epoch 214: training loss 0.0652\n",
      "2025-05-29 21:01:45 [INFO]: epoch 215: training loss 0.0731\n",
      "2025-05-29 21:01:45 [INFO]: epoch 216: training loss 0.0774\n",
      "2025-05-29 21:01:45 [INFO]: epoch 217: training loss 0.0651\n",
      "2025-05-29 21:01:45 [INFO]: epoch 218: training loss 0.0800\n",
      "2025-05-29 21:01:45 [INFO]: epoch 219: training loss 0.1023\n",
      "2025-05-29 21:01:45 [INFO]: epoch 220: training loss 0.0767\n",
      "2025-05-29 21:01:45 [INFO]: epoch 221: training loss 0.0692\n",
      "2025-05-29 21:01:45 [INFO]: epoch 222: training loss 0.0904\n",
      "2025-05-29 21:01:45 [INFO]: epoch 223: training loss 0.0883\n",
      "2025-05-29 21:01:45 [INFO]: epoch 224: training loss 0.0749\n",
      "2025-05-29 21:01:45 [INFO]: epoch 225: training loss 0.0627\n",
      "2025-05-29 21:01:45 [INFO]: epoch 226: training loss 0.0880\n",
      "2025-05-29 21:01:45 [INFO]: epoch 227: training loss 0.0673\n",
      "2025-05-29 21:01:45 [INFO]: epoch 228: training loss 0.0642\n",
      "2025-05-29 21:01:45 [INFO]: epoch 229: training loss 0.0817\n",
      "2025-05-29 21:01:45 [INFO]: epoch 230: training loss 0.0871\n",
      "2025-05-29 21:01:45 [INFO]: epoch 231: training loss 0.0789\n",
      "2025-05-29 21:01:45 [INFO]: epoch 232: training loss 0.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:45 [INFO]: epoch 233: training loss 0.0717\n",
      "2025-05-29 21:01:45 [INFO]: epoch 234: training loss 0.0633\n",
      "2025-05-29 21:01:46 [INFO]: epoch 235: training loss 0.0655\n",
      "2025-05-29 21:01:46 [INFO]: epoch 236: training loss 0.0655\n",
      "2025-05-29 21:01:46 [INFO]: epoch 237: training loss 0.0738\n",
      "2025-05-29 21:01:46 [INFO]: epoch 238: training loss 0.0829\n",
      "2025-05-29 21:01:46 [INFO]: epoch 239: training loss 0.0736\n",
      "2025-05-29 21:01:46 [INFO]: epoch 240: training loss 0.0693\n",
      "2025-05-29 21:01:46 [INFO]: epoch 241: training loss 0.0709\n",
      "2025-05-29 21:01:46 [INFO]: epoch 242: training loss 0.0675\n",
      "2025-05-29 21:01:46 [INFO]: epoch 243: training loss 0.0547\n",
      "2025-05-29 21:01:46 [INFO]: epoch 244: training loss 0.0722\n",
      "2025-05-29 21:01:46 [INFO]: epoch 245: training loss 0.0751\n",
      "2025-05-29 21:01:46 [INFO]: epoch 246: training loss 0.0696\n",
      "2025-05-29 21:01:46 [INFO]: epoch 247: training loss 0.0515\n",
      "2025-05-29 21:01:46 [INFO]: epoch 248: training loss 0.0639\n",
      "2025-05-29 21:01:46 [INFO]: epoch 249: training loss 0.0614\n",
      "2025-05-29 21:01:46 [INFO]: epoch 250: training loss 0.0639\n",
      "2025-05-29 21:01:46 [INFO]: epoch 251: training loss 0.0619\n",
      "2025-05-29 21:01:46 [INFO]: epoch 252: training loss 0.0571\n",
      "2025-05-29 21:01:46 [INFO]: epoch 253: training loss 0.0584\n",
      "2025-05-29 21:01:46 [INFO]: epoch 254: training loss 0.0616\n",
      "2025-05-29 21:01:46 [INFO]: epoch 255: training loss 0.0766\n",
      "2025-05-29 21:01:46 [INFO]: epoch 256: training loss 0.0625\n",
      "2025-05-29 21:01:46 [INFO]: epoch 257: training loss 0.0669\n",
      "2025-05-29 21:01:46 [INFO]: epoch 258: training loss 0.0733\n",
      "2025-05-29 21:01:46 [INFO]: epoch 259: training loss 0.0660\n",
      "2025-05-29 21:01:46 [INFO]: epoch 260: training loss 0.0671\n",
      "2025-05-29 21:01:46 [INFO]: epoch 261: training loss 0.0756\n",
      "2025-05-29 21:01:46 [INFO]: epoch 262: training loss 0.0788\n",
      "2025-05-29 21:01:46 [INFO]: epoch 263: training loss 0.0578\n",
      "2025-05-29 21:01:46 [INFO]: epoch 264: training loss 0.0711\n",
      "2025-05-29 21:01:46 [INFO]: epoch 265: training loss 0.0714\n",
      "2025-05-29 21:01:46 [INFO]: epoch 266: training loss 0.0579\n",
      "2025-05-29 21:01:46 [INFO]: epoch 267: training loss 0.0583\n",
      "2025-05-29 21:01:46 [INFO]: epoch 268: training loss 0.0650\n",
      "2025-05-29 21:01:46 [INFO]: epoch 269: training loss 0.0591\n",
      "2025-05-29 21:01:46 [INFO]: epoch 270: training loss 0.0517\n",
      "2025-05-29 21:01:46 [INFO]: epoch 271: training loss 0.0600\n",
      "2025-05-29 21:01:46 [INFO]: epoch 272: training loss 0.0539\n",
      "2025-05-29 21:01:46 [INFO]: epoch 273: training loss 0.0578\n",
      "2025-05-29 21:01:46 [INFO]: epoch 274: training loss 0.0603\n",
      "2025-05-29 21:01:46 [INFO]: epoch 275: training loss 0.0666\n",
      "2025-05-29 21:01:46 [INFO]: epoch 276: training loss 0.0738\n",
      "2025-05-29 21:01:46 [INFO]: epoch 277: training loss 0.0677\n",
      "2025-05-29 21:01:46 [INFO]: epoch 278: training loss 0.0514\n",
      "2025-05-29 21:01:46 [INFO]: epoch 279: training loss 0.0679\n",
      "2025-05-29 21:01:46 [INFO]: epoch 280: training loss 0.0722\n",
      "2025-05-29 21:01:46 [INFO]: epoch 281: training loss 0.0582\n",
      "2025-05-29 21:01:46 [INFO]: epoch 282: training loss 0.0531\n",
      "2025-05-29 21:01:46 [INFO]: epoch 283: training loss 0.0801\n",
      "2025-05-29 21:01:46 [INFO]: epoch 284: training loss 0.0704\n",
      "2025-05-29 21:01:46 [INFO]: epoch 285: training loss 0.0493\n",
      "2025-05-29 21:01:46 [INFO]: epoch 286: training loss 0.0642\n",
      "2025-05-29 21:01:46 [INFO]: epoch 287: training loss 0.0567\n",
      "2025-05-29 21:01:46 [INFO]: epoch 288: training loss 0.0554\n",
      "2025-05-29 21:01:46 [INFO]: epoch 289: training loss 0.0621\n",
      "2025-05-29 21:01:46 [INFO]: epoch 290: training loss 0.0722\n",
      "2025-05-29 21:01:46 [INFO]: epoch 291: training loss 0.0512\n",
      "2025-05-29 21:01:46 [INFO]: epoch 292: training loss 0.0635\n",
      "2025-05-29 21:01:46 [INFO]: epoch 293: training loss 0.0639\n",
      "2025-05-29 21:01:46 [INFO]: epoch 294: training loss 0.0500\n",
      "2025-05-29 21:01:46 [INFO]: epoch 295: training loss 0.0713\n",
      "2025-05-29 21:01:46 [INFO]: epoch 296: training loss 0.0588\n",
      "2025-05-29 21:01:46 [INFO]: epoch 297: training loss 0.0429\n",
      "2025-05-29 21:01:46 [INFO]: epoch 298: training loss 0.0515\n",
      "2025-05-29 21:01:46 [INFO]: epoch 299: training loss 0.0496\n",
      "2025-05-29 21:01:46 [INFO]: epoch 300: training loss 0.0486\n",
      "2025-05-29 21:01:46 [INFO]: epoch 301: training loss 0.0565\n",
      "2025-05-29 21:01:46 [INFO]: epoch 302: training loss 0.0560\n",
      "2025-05-29 21:01:46 [INFO]: epoch 303: training loss 0.0453\n",
      "2025-05-29 21:01:46 [INFO]: epoch 304: training loss 0.0509\n",
      "2025-05-29 21:01:46 [INFO]: epoch 305: training loss 0.0480\n",
      "2025-05-29 21:01:46 [INFO]: epoch 306: training loss 0.0524\n",
      "2025-05-29 21:01:46 [INFO]: epoch 307: training loss 0.0442\n",
      "2025-05-29 21:01:46 [INFO]: epoch 308: training loss 0.0515\n",
      "2025-05-29 21:01:46 [INFO]: epoch 309: training loss 0.0488\n",
      "2025-05-29 21:01:46 [INFO]: epoch 310: training loss 0.0552\n",
      "2025-05-29 21:01:46 [INFO]: epoch 311: training loss 0.0479\n",
      "2025-05-29 21:01:46 [INFO]: epoch 312: training loss 0.0451\n",
      "2025-05-29 21:01:46 [INFO]: epoch 313: training loss 0.0548\n",
      "2025-05-29 21:01:46 [INFO]: epoch 314: training loss 0.0505\n",
      "2025-05-29 21:01:47 [INFO]: epoch 315: training loss 0.0435\n",
      "2025-05-29 21:01:47 [INFO]: epoch 316: training loss 0.0482\n",
      "2025-05-29 21:01:47 [INFO]: epoch 317: training loss 0.0495\n",
      "2025-05-29 21:01:47 [INFO]: epoch 318: training loss 0.0417\n",
      "2025-05-29 21:01:47 [INFO]: epoch 319: training loss 0.0437\n",
      "2025-05-29 21:01:47 [INFO]: epoch 320: training loss 0.0389\n",
      "2025-05-29 21:01:47 [INFO]: epoch 321: training loss 0.0429\n",
      "2025-05-29 21:01:47 [INFO]: epoch 322: training loss 0.0487\n",
      "2025-05-29 21:01:47 [INFO]: epoch 323: training loss 0.0456\n",
      "2025-05-29 21:01:47 [INFO]: epoch 324: training loss 0.0444\n",
      "2025-05-29 21:01:47 [INFO]: epoch 325: training loss 0.0481\n",
      "2025-05-29 21:01:47 [INFO]: epoch 326: training loss 0.0494\n",
      "2025-05-29 21:01:47 [INFO]: epoch 327: training loss 0.0450\n",
      "2025-05-29 21:01:47 [INFO]: epoch 328: training loss 0.0489\n",
      "2025-05-29 21:01:47 [INFO]: epoch 329: training loss 0.0421\n",
      "2025-05-29 21:01:47 [INFO]: epoch 330: training loss 0.0432\n",
      "2025-05-29 21:01:47 [INFO]: epoch 331: training loss 0.0475\n",
      "2025-05-29 21:01:47 [INFO]: epoch 332: training loss 0.0399\n",
      "2025-05-29 21:01:47 [INFO]: epoch 333: training loss 0.0454\n",
      "2025-05-29 21:01:47 [INFO]: epoch 334: training loss 0.0436\n",
      "2025-05-29 21:01:47 [INFO]: epoch 335: training loss 0.0495\n",
      "2025-05-29 21:01:47 [INFO]: epoch 336: training loss 0.0458\n",
      "2025-05-29 21:01:47 [INFO]: epoch 337: training loss 0.0478\n",
      "2025-05-29 21:01:47 [INFO]: epoch 338: training loss 0.0487\n",
      "2025-05-29 21:01:47 [INFO]: epoch 339: training loss 0.0433\n",
      "2025-05-29 21:01:47 [INFO]: epoch 340: training loss 0.0440\n",
      "2025-05-29 21:01:47 [INFO]: epoch 341: training loss 0.0452\n",
      "2025-05-29 21:01:47 [INFO]: epoch 342: training loss 0.0435\n",
      "2025-05-29 21:01:47 [INFO]: epoch 343: training loss 0.0436\n",
      "2025-05-29 21:01:47 [INFO]: epoch 344: training loss 0.0477\n",
      "2025-05-29 21:01:47 [INFO]: epoch 345: training loss 0.0416\n",
      "2025-05-29 21:01:47 [INFO]: epoch 346: training loss 0.0421\n",
      "2025-05-29 21:01:47 [INFO]: epoch 347: training loss 0.0387\n",
      "2025-05-29 21:01:47 [INFO]: epoch 348: training loss 0.0428\n",
      "2025-05-29 21:01:47 [INFO]: epoch 349: training loss 0.0470\n",
      "2025-05-29 21:01:47 [INFO]: epoch 350: training loss 0.0431\n",
      "2025-05-29 21:01:47 [INFO]: epoch 351: training loss 0.0361\n",
      "2025-05-29 21:01:47 [INFO]: epoch 352: training loss 0.0383\n",
      "2025-05-29 21:01:47 [INFO]: epoch 353: training loss 0.0420\n",
      "2025-05-29 21:01:47 [INFO]: epoch 354: training loss 0.0418\n",
      "2025-05-29 21:01:47 [INFO]: epoch 355: training loss 0.0426\n",
      "2025-05-29 21:01:47 [INFO]: epoch 356: training loss 0.0459\n",
      "2025-05-29 21:01:47 [INFO]: epoch 357: training loss 0.0561\n",
      "2025-05-29 21:01:47 [INFO]: epoch 358: training loss 0.0435\n",
      "2025-05-29 21:01:47 [INFO]: epoch 359: training loss 0.0500\n",
      "2025-05-29 21:01:47 [INFO]: epoch 360: training loss 0.0525\n",
      "2025-05-29 21:01:47 [INFO]: epoch 361: training loss 0.0413\n",
      "2025-05-29 21:01:47 [INFO]: epoch 362: training loss 0.0478\n",
      "2025-05-29 21:01:47 [INFO]: epoch 363: training loss 0.0577\n",
      "2025-05-29 21:01:47 [INFO]: epoch 364: training loss 0.0464\n",
      "2025-05-29 21:01:47 [INFO]: epoch 365: training loss 0.0470\n",
      "2025-05-29 21:01:47 [INFO]: epoch 366: training loss 0.0438\n",
      "2025-05-29 21:01:47 [INFO]: epoch 367: training loss 0.0372\n",
      "2025-05-29 21:01:47 [INFO]: epoch 368: training loss 0.0427\n",
      "2025-05-29 21:01:47 [INFO]: epoch 369: training loss 0.0386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:47 [INFO]: epoch 370: training loss 0.0358\n",
      "2025-05-29 21:01:47 [INFO]: epoch 371: training loss 0.0498\n",
      "2025-05-29 21:01:47 [INFO]: epoch 372: training loss 0.0439\n",
      "2025-05-29 21:01:47 [INFO]: epoch 373: training loss 0.0359\n",
      "2025-05-29 21:01:47 [INFO]: epoch 374: training loss 0.0500\n",
      "2025-05-29 21:01:47 [INFO]: epoch 375: training loss 0.0420\n",
      "2025-05-29 21:01:47 [INFO]: epoch 376: training loss 0.0426\n",
      "2025-05-29 21:01:47 [INFO]: epoch 377: training loss 0.0400\n",
      "2025-05-29 21:01:47 [INFO]: epoch 378: training loss 0.0446\n",
      "2025-05-29 21:01:47 [INFO]: epoch 379: training loss 0.0365\n",
      "2025-05-29 21:01:47 [INFO]: epoch 380: training loss 0.0437\n",
      "2025-05-29 21:01:47 [INFO]: epoch 381: training loss 0.0497\n",
      "2025-05-29 21:01:47 [INFO]: epoch 382: training loss 0.0402\n",
      "2025-05-29 21:01:47 [INFO]: epoch 383: training loss 0.0401\n",
      "2025-05-29 21:01:47 [INFO]: epoch 384: training loss 0.0390\n",
      "2025-05-29 21:01:47 [INFO]: epoch 385: training loss 0.0417\n",
      "2025-05-29 21:01:47 [INFO]: epoch 386: training loss 0.0416\n",
      "2025-05-29 21:01:47 [INFO]: epoch 387: training loss 0.0385\n",
      "2025-05-29 21:01:47 [INFO]: epoch 388: training loss 0.0358\n",
      "2025-05-29 21:01:47 [INFO]: epoch 389: training loss 0.0403\n",
      "2025-05-29 21:01:47 [INFO]: epoch 390: training loss 0.0372\n",
      "2025-05-29 21:01:47 [INFO]: epoch 391: training loss 0.0411\n",
      "2025-05-29 21:01:47 [INFO]: epoch 392: training loss 0.0414\n",
      "2025-05-29 21:01:47 [INFO]: epoch 393: training loss 0.0344\n",
      "2025-05-29 21:01:47 [INFO]: epoch 394: training loss 0.0428\n",
      "2025-05-29 21:01:47 [INFO]: epoch 395: training loss 0.0406\n",
      "2025-05-29 21:01:47 [INFO]: epoch 396: training loss 0.0391\n",
      "2025-05-29 21:01:48 [INFO]: epoch 397: training loss 0.0414\n",
      "2025-05-29 21:01:48 [INFO]: epoch 398: training loss 0.0435\n",
      "2025-05-29 21:01:48 [INFO]: epoch 399: training loss 0.0426\n",
      "2025-05-29 21:01:48 [INFO]: epoch 400: training loss 0.0425\n",
      "2025-05-29 21:01:48 [INFO]: epoch 401: training loss 0.0473\n",
      "2025-05-29 21:01:48 [INFO]: epoch 402: training loss 0.0585\n",
      "2025-05-29 21:01:48 [INFO]: epoch 403: training loss 0.0550\n",
      "2025-05-29 21:01:48 [INFO]: epoch 404: training loss 0.0407\n",
      "2025-05-29 21:01:48 [INFO]: epoch 405: training loss 0.0514\n",
      "2025-05-29 21:01:48 [INFO]: epoch 406: training loss 0.0529\n",
      "2025-05-29 21:01:48 [INFO]: epoch 407: training loss 0.0380\n",
      "2025-05-29 21:01:48 [INFO]: epoch 408: training loss 0.0446\n",
      "2025-05-29 21:01:48 [INFO]: epoch 409: training loss 0.0519\n",
      "2025-05-29 21:01:48 [INFO]: epoch 410: training loss 0.0404\n",
      "2025-05-29 21:01:48 [INFO]: epoch 411: training loss 0.0438\n",
      "2025-05-29 21:01:48 [INFO]: epoch 412: training loss 0.0508\n",
      "2025-05-29 21:01:48 [INFO]: epoch 413: training loss 0.0443\n",
      "2025-05-29 21:01:48 [INFO]: epoch 414: training loss 0.0459\n",
      "2025-05-29 21:01:48 [INFO]: epoch 415: training loss 0.0395\n",
      "2025-05-29 21:01:48 [INFO]: epoch 416: training loss 0.0404\n",
      "2025-05-29 21:01:48 [INFO]: epoch 417: training loss 0.0438\n",
      "2025-05-29 21:01:48 [INFO]: epoch 418: training loss 0.0430\n",
      "2025-05-29 21:01:48 [INFO]: epoch 419: training loss 0.0379\n",
      "2025-05-29 21:01:48 [INFO]: epoch 420: training loss 0.0432\n",
      "2025-05-29 21:01:48 [INFO]: epoch 421: training loss 0.0468\n",
      "2025-05-29 21:01:48 [INFO]: epoch 422: training loss 0.0382\n",
      "2025-05-29 21:01:48 [INFO]: epoch 423: training loss 0.0415\n",
      "2025-05-29 21:01:48 [INFO]: epoch 424: training loss 0.0392\n",
      "2025-05-29 21:01:48 [INFO]: epoch 425: training loss 0.0297\n",
      "2025-05-29 21:01:48 [INFO]: epoch 426: training loss 0.0401\n",
      "2025-05-29 21:01:48 [INFO]: epoch 427: training loss 0.0388\n",
      "2025-05-29 21:01:48 [INFO]: epoch 428: training loss 0.0286\n",
      "2025-05-29 21:01:48 [INFO]: epoch 429: training loss 0.0316\n",
      "2025-05-29 21:01:48 [INFO]: epoch 430: training loss 0.0398\n",
      "2025-05-29 21:01:48 [INFO]: epoch 431: training loss 0.0302\n",
      "2025-05-29 21:01:48 [INFO]: epoch 432: training loss 0.0364\n",
      "2025-05-29 21:01:48 [INFO]: epoch 433: training loss 0.0339\n",
      "2025-05-29 21:01:48 [INFO]: epoch 434: training loss 0.0372\n",
      "2025-05-29 21:01:48 [INFO]: epoch 435: training loss 0.0316\n",
      "2025-05-29 21:01:48 [INFO]: epoch 436: training loss 0.0284\n",
      "2025-05-29 21:01:48 [INFO]: epoch 437: training loss 0.0338\n",
      "2025-05-29 21:01:48 [INFO]: epoch 438: training loss 0.0349\n",
      "2025-05-29 21:01:48 [INFO]: epoch 439: training loss 0.0386\n",
      "2025-05-29 21:01:48 [INFO]: epoch 440: training loss 0.0359\n",
      "2025-05-29 21:01:48 [INFO]: epoch 441: training loss 0.0379\n",
      "2025-05-29 21:01:48 [INFO]: epoch 442: training loss 0.0429\n",
      "2025-05-29 21:01:48 [INFO]: epoch 443: training loss 0.0392\n",
      "2025-05-29 21:01:48 [INFO]: epoch 444: training loss 0.0435\n",
      "2025-05-29 21:01:48 [INFO]: epoch 445: training loss 0.0361\n",
      "2025-05-29 21:01:48 [INFO]: epoch 446: training loss 0.0370\n",
      "2025-05-29 21:01:48 [INFO]: epoch 447: training loss 0.0426\n",
      "2025-05-29 21:01:48 [INFO]: epoch 448: training loss 0.0372\n",
      "2025-05-29 21:01:48 [INFO]: epoch 449: training loss 0.0347\n",
      "2025-05-29 21:01:48 [INFO]: epoch 450: training loss 0.0408\n",
      "2025-05-29 21:01:48 [INFO]: epoch 451: training loss 0.0436\n",
      "2025-05-29 21:01:48 [INFO]: epoch 452: training loss 0.0330\n",
      "2025-05-29 21:01:48 [INFO]: epoch 453: training loss 0.0399\n",
      "2025-05-29 21:01:48 [INFO]: epoch 454: training loss 0.0389\n",
      "2025-05-29 21:01:48 [INFO]: epoch 455: training loss 0.0327\n",
      "2025-05-29 21:01:48 [INFO]: epoch 456: training loss 0.0348\n",
      "2025-05-29 21:01:48 [INFO]: epoch 457: training loss 0.0396\n",
      "2025-05-29 21:01:48 [INFO]: epoch 458: training loss 0.0358\n",
      "2025-05-29 21:01:48 [INFO]: epoch 459: training loss 0.0337\n",
      "2025-05-29 21:01:48 [INFO]: epoch 460: training loss 0.0383\n",
      "2025-05-29 21:01:48 [INFO]: epoch 461: training loss 0.0352\n",
      "2025-05-29 21:01:48 [INFO]: epoch 462: training loss 0.0411\n",
      "2025-05-29 21:01:48 [INFO]: epoch 463: training loss 0.0348\n",
      "2025-05-29 21:01:48 [INFO]: epoch 464: training loss 0.0314\n",
      "2025-05-29 21:01:48 [INFO]: epoch 465: training loss 0.0319\n",
      "2025-05-29 21:01:48 [INFO]: epoch 466: training loss 0.0351\n",
      "2025-05-29 21:01:48 [INFO]: epoch 467: training loss 0.0297\n",
      "2025-05-29 21:01:48 [INFO]: epoch 468: training loss 0.0282\n",
      "2025-05-29 21:01:48 [INFO]: epoch 469: training loss 0.0337\n",
      "2025-05-29 21:01:48 [INFO]: epoch 470: training loss 0.0346\n",
      "2025-05-29 21:01:48 [INFO]: epoch 471: training loss 0.0260\n",
      "2025-05-29 21:01:48 [INFO]: epoch 472: training loss 0.0368\n",
      "2025-05-29 21:01:48 [INFO]: epoch 473: training loss 0.0417\n",
      "2025-05-29 21:01:48 [INFO]: epoch 474: training loss 0.0322\n",
      "2025-05-29 21:01:48 [INFO]: epoch 475: training loss 0.0321\n",
      "2025-05-29 21:01:48 [INFO]: epoch 476: training loss 0.0309\n",
      "2025-05-29 21:01:48 [INFO]: epoch 477: training loss 0.0355\n",
      "2025-05-29 21:01:49 [INFO]: epoch 478: training loss 0.0341\n",
      "2025-05-29 21:01:49 [INFO]: epoch 479: training loss 0.0348\n",
      "2025-05-29 21:01:49 [INFO]: epoch 480: training loss 0.0389\n",
      "2025-05-29 21:01:49 [INFO]: epoch 481: training loss 0.0347\n",
      "2025-05-29 21:01:49 [INFO]: epoch 482: training loss 0.0376\n",
      "2025-05-29 21:01:49 [INFO]: epoch 483: training loss 0.0397\n",
      "2025-05-29 21:01:49 [INFO]: epoch 484: training loss 0.0306\n",
      "2025-05-29 21:01:49 [INFO]: epoch 485: training loss 0.0343\n",
      "2025-05-29 21:01:49 [INFO]: epoch 486: training loss 0.0440\n",
      "2025-05-29 21:01:49 [INFO]: epoch 487: training loss 0.0285\n",
      "2025-05-29 21:01:49 [INFO]: epoch 488: training loss 0.0422\n",
      "2025-05-29 21:01:49 [INFO]: epoch 489: training loss 0.0411\n",
      "2025-05-29 21:01:49 [INFO]: epoch 490: training loss 0.0271\n",
      "2025-05-29 21:01:49 [INFO]: epoch 491: training loss 0.0305\n",
      "2025-05-29 21:01:49 [INFO]: epoch 492: training loss 0.0301\n",
      "2025-05-29 21:01:49 [INFO]: epoch 493: training loss 0.0289\n",
      "2025-05-29 21:01:49 [INFO]: epoch 494: training loss 0.0267\n",
      "2025-05-29 21:01:49 [INFO]: epoch 495: training loss 0.0298\n",
      "2025-05-29 21:01:49 [INFO]: epoch 496: training loss 0.0393\n",
      "2025-05-29 21:01:49 [INFO]: epoch 497: training loss 0.0326\n",
      "2025-05-29 21:01:49 [INFO]: epoch 498: training loss 0.0341\n",
      "2025-05-29 21:01:49 [INFO]: epoch 499: training loss 0.0427\n",
      "2025-05-29 21:01:49 [INFO]: epoch 500: training loss 0.0267\n",
      "2025-05-29 21:01:49 [INFO]: epoch 501: training loss 0.0298\n",
      "2025-05-29 21:01:49 [INFO]: epoch 502: training loss 0.0362\n",
      "2025-05-29 21:01:49 [INFO]: epoch 503: training loss 0.0339\n",
      "2025-05-29 21:01:49 [INFO]: epoch 504: training loss 0.0320\n",
      "2025-05-29 21:01:49 [INFO]: epoch 505: training loss 0.0338\n",
      "2025-05-29 21:01:49 [INFO]: epoch 506: training loss 0.0383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:49 [INFO]: epoch 507: training loss 0.0405\n",
      "2025-05-29 21:01:49 [INFO]: epoch 508: training loss 0.0384\n",
      "2025-05-29 21:01:49 [INFO]: epoch 509: training loss 0.0298\n",
      "2025-05-29 21:01:49 [INFO]: epoch 510: training loss 0.0377\n",
      "2025-05-29 21:01:49 [INFO]: epoch 511: training loss 0.0333\n",
      "2025-05-29 21:01:49 [INFO]: epoch 512: training loss 0.0308\n",
      "2025-05-29 21:01:49 [INFO]: epoch 513: training loss 0.0400\n",
      "2025-05-29 21:01:49 [INFO]: epoch 514: training loss 0.0358\n",
      "2025-05-29 21:01:49 [INFO]: epoch 515: training loss 0.0297\n",
      "2025-05-29 21:01:49 [INFO]: epoch 516: training loss 0.0320\n",
      "2025-05-29 21:01:49 [INFO]: epoch 517: training loss 0.0367\n",
      "2025-05-29 21:01:49 [INFO]: epoch 518: training loss 0.0351\n",
      "2025-05-29 21:01:49 [INFO]: epoch 519: training loss 0.0392\n",
      "2025-05-29 21:01:49 [INFO]: epoch 520: training loss 0.0320\n",
      "2025-05-29 21:01:49 [INFO]: epoch 521: training loss 0.0344\n",
      "2025-05-29 21:01:49 [INFO]: epoch 522: training loss 0.0431\n",
      "2025-05-29 21:01:49 [INFO]: epoch 523: training loss 0.0398\n",
      "2025-05-29 21:01:49 [INFO]: epoch 524: training loss 0.0387\n",
      "2025-05-29 21:01:49 [INFO]: epoch 525: training loss 0.0377\n",
      "2025-05-29 21:01:49 [INFO]: epoch 526: training loss 0.0378\n",
      "2025-05-29 21:01:49 [INFO]: epoch 527: training loss 0.0359\n",
      "2025-05-29 21:01:49 [INFO]: epoch 528: training loss 0.0434\n",
      "2025-05-29 21:01:49 [INFO]: epoch 529: training loss 0.0424\n",
      "2025-05-29 21:01:49 [INFO]: epoch 530: training loss 0.0476\n",
      "2025-05-29 21:01:49 [INFO]: epoch 531: training loss 0.0318\n",
      "2025-05-29 21:01:49 [INFO]: epoch 532: training loss 0.0302\n",
      "2025-05-29 21:01:49 [INFO]: epoch 533: training loss 0.0308\n",
      "2025-05-29 21:01:49 [INFO]: epoch 534: training loss 0.0271\n",
      "2025-05-29 21:01:49 [INFO]: epoch 535: training loss 0.0287\n",
      "2025-05-29 21:01:49 [INFO]: epoch 536: training loss 0.0319\n",
      "2025-05-29 21:01:49 [INFO]: epoch 537: training loss 0.0254\n",
      "2025-05-29 21:01:49 [INFO]: epoch 538: training loss 0.0338\n",
      "2025-05-29 21:01:49 [INFO]: epoch 539: training loss 0.0372\n",
      "2025-05-29 21:01:49 [INFO]: epoch 540: training loss 0.0329\n",
      "2025-05-29 21:01:49 [INFO]: epoch 541: training loss 0.0364\n",
      "2025-05-29 21:01:49 [INFO]: epoch 542: training loss 0.0336\n",
      "2025-05-29 21:01:49 [INFO]: epoch 543: training loss 0.0301\n",
      "2025-05-29 21:01:49 [INFO]: epoch 544: training loss 0.0303\n",
      "2025-05-29 21:01:49 [INFO]: epoch 545: training loss 0.0379\n",
      "2025-05-29 21:01:49 [INFO]: epoch 546: training loss 0.0321\n",
      "2025-05-29 21:01:49 [INFO]: epoch 547: training loss 0.0351\n",
      "2025-05-29 21:01:49 [INFO]: epoch 548: training loss 0.0370\n",
      "2025-05-29 21:01:49 [INFO]: epoch 549: training loss 0.0447\n",
      "2025-05-29 21:01:49 [INFO]: epoch 550: training loss 0.0309\n",
      "2025-05-29 21:01:49 [INFO]: epoch 551: training loss 0.0334\n",
      "2025-05-29 21:01:49 [INFO]: epoch 552: training loss 0.0426\n",
      "2025-05-29 21:01:49 [INFO]: epoch 553: training loss 0.0331\n",
      "2025-05-29 21:01:49 [INFO]: epoch 554: training loss 0.0337\n",
      "2025-05-29 21:01:49 [INFO]: epoch 555: training loss 0.0345\n",
      "2025-05-29 21:01:49 [INFO]: epoch 556: training loss 0.0383\n",
      "2025-05-29 21:01:49 [INFO]: epoch 557: training loss 0.0270\n",
      "2025-05-29 21:01:49 [INFO]: epoch 558: training loss 0.0310\n",
      "2025-05-29 21:01:49 [INFO]: epoch 559: training loss 0.0307\n",
      "2025-05-29 21:01:50 [INFO]: epoch 560: training loss 0.0304\n",
      "2025-05-29 21:01:50 [INFO]: epoch 561: training loss 0.0331\n",
      "2025-05-29 21:01:50 [INFO]: epoch 562: training loss 0.0284\n",
      "2025-05-29 21:01:50 [INFO]: epoch 563: training loss 0.0314\n",
      "2025-05-29 21:01:50 [INFO]: epoch 564: training loss 0.0285\n",
      "2025-05-29 21:01:50 [INFO]: epoch 565: training loss 0.0299\n",
      "2025-05-29 21:01:50 [INFO]: epoch 566: training loss 0.0287\n",
      "2025-05-29 21:01:50 [INFO]: epoch 567: training loss 0.0282\n",
      "2025-05-29 21:01:50 [INFO]: epoch 568: training loss 0.0270\n",
      "2025-05-29 21:01:50 [INFO]: epoch 569: training loss 0.0263\n",
      "2025-05-29 21:01:50 [INFO]: epoch 570: training loss 0.0321\n",
      "2025-05-29 21:01:50 [INFO]: epoch 571: training loss 0.0315\n",
      "2025-05-29 21:01:50 [INFO]: epoch 572: training loss 0.0293\n",
      "2025-05-29 21:01:50 [INFO]: epoch 573: training loss 0.0251\n",
      "2025-05-29 21:01:50 [INFO]: epoch 574: training loss 0.0309\n",
      "2025-05-29 21:01:50 [INFO]: epoch 575: training loss 0.0292\n",
      "2025-05-29 21:01:50 [INFO]: epoch 576: training loss 0.0238\n",
      "2025-05-29 21:01:50 [INFO]: epoch 577: training loss 0.0290\n",
      "2025-05-29 21:01:50 [INFO]: epoch 578: training loss 0.0309\n",
      "2025-05-29 21:01:50 [INFO]: epoch 579: training loss 0.0255\n",
      "2025-05-29 21:01:50 [INFO]: epoch 580: training loss 0.0234\n",
      "2025-05-29 21:01:50 [INFO]: epoch 581: training loss 0.0305\n",
      "2025-05-29 21:01:50 [INFO]: epoch 582: training loss 0.0258\n",
      "2025-05-29 21:01:50 [INFO]: epoch 583: training loss 0.0251\n",
      "2025-05-29 21:01:50 [INFO]: epoch 584: training loss 0.0322\n",
      "2025-05-29 21:01:50 [INFO]: epoch 585: training loss 0.0263\n",
      "2025-05-29 21:01:50 [INFO]: epoch 586: training loss 0.0241\n",
      "2025-05-29 21:01:50 [INFO]: epoch 587: training loss 0.0318\n",
      "2025-05-29 21:01:50 [INFO]: epoch 588: training loss 0.0282\n",
      "2025-05-29 21:01:50 [INFO]: epoch 589: training loss 0.0302\n",
      "2025-05-29 21:01:50 [INFO]: epoch 590: training loss 0.0313\n",
      "2025-05-29 21:01:50 [INFO]: epoch 591: training loss 0.0312\n",
      "2025-05-29 21:01:50 [INFO]: epoch 592: training loss 0.0359\n",
      "2025-05-29 21:01:50 [INFO]: epoch 593: training loss 0.0265\n",
      "2025-05-29 21:01:50 [INFO]: epoch 594: training loss 0.0253\n",
      "2025-05-29 21:01:50 [INFO]: epoch 595: training loss 0.0363\n",
      "2025-05-29 21:01:50 [INFO]: epoch 596: training loss 0.0310\n",
      "2025-05-29 21:01:50 [INFO]: epoch 597: training loss 0.0317\n",
      "2025-05-29 21:01:50 [INFO]: epoch 598: training loss 0.0437\n",
      "2025-05-29 21:01:50 [INFO]: epoch 599: training loss 0.0295\n",
      "2025-05-29 21:01:50 [INFO]: Finished training.\n",
      "2025-05-29 21:01:50 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 20%|████████████████▊                                                                   | 1/5 [00:07<00:30,  7.50s/it]2025-05-29 21:01:50 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:01:50 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:01:50 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:01:50 [INFO]: epoch 0: training loss 0.9636\n",
      "2025-05-29 21:01:50 [INFO]: epoch 1: training loss 0.6022\n",
      "2025-05-29 21:01:50 [INFO]: epoch 2: training loss 0.4661\n",
      "2025-05-29 21:01:50 [INFO]: epoch 3: training loss 0.5498\n",
      "2025-05-29 21:01:50 [INFO]: epoch 4: training loss 0.4571\n",
      "2025-05-29 21:01:50 [INFO]: epoch 5: training loss 0.4486\n",
      "2025-05-29 21:01:50 [INFO]: epoch 6: training loss 0.5253\n",
      "2025-05-29 21:01:50 [INFO]: epoch 7: training loss 0.4784\n",
      "2025-05-29 21:01:50 [INFO]: epoch 8: training loss 0.4579\n",
      "2025-05-29 21:01:50 [INFO]: epoch 9: training loss 0.4391\n",
      "2025-05-29 21:01:50 [INFO]: epoch 10: training loss 0.4457\n",
      "2025-05-29 21:01:50 [INFO]: epoch 11: training loss 0.4211\n",
      "2025-05-29 21:01:50 [INFO]: epoch 12: training loss 0.4248\n",
      "2025-05-29 21:01:50 [INFO]: epoch 13: training loss 0.3767\n",
      "2025-05-29 21:01:50 [INFO]: epoch 14: training loss 0.4003\n",
      "2025-05-29 21:01:50 [INFO]: epoch 15: training loss 0.3990\n",
      "2025-05-29 21:01:50 [INFO]: epoch 16: training loss 0.4171\n",
      "2025-05-29 21:01:50 [INFO]: epoch 17: training loss 0.3844\n",
      "2025-05-29 21:01:50 [INFO]: epoch 18: training loss 0.3724\n",
      "2025-05-29 21:01:50 [INFO]: epoch 19: training loss 0.4002\n",
      "2025-05-29 21:01:50 [INFO]: epoch 20: training loss 0.4104\n",
      "2025-05-29 21:01:50 [INFO]: epoch 21: training loss 0.4038\n",
      "2025-05-29 21:01:50 [INFO]: epoch 22: training loss 0.3631\n",
      "2025-05-29 21:01:50 [INFO]: epoch 23: training loss 0.4015\n",
      "2025-05-29 21:01:50 [INFO]: epoch 24: training loss 0.3526\n",
      "2025-05-29 21:01:50 [INFO]: epoch 25: training loss 0.3430\n",
      "2025-05-29 21:01:50 [INFO]: epoch 26: training loss 0.3658\n",
      "2025-05-29 21:01:50 [INFO]: epoch 27: training loss 0.3788\n",
      "2025-05-29 21:01:50 [INFO]: epoch 28: training loss 0.3662\n",
      "2025-05-29 21:01:50 [INFO]: epoch 29: training loss 0.3505\n",
      "2025-05-29 21:01:50 [INFO]: epoch 30: training loss 0.3487\n",
      "2025-05-29 21:01:50 [INFO]: epoch 31: training loss 0.3531\n",
      "2025-05-29 21:01:50 [INFO]: epoch 32: training loss 0.3501\n",
      "2025-05-29 21:01:51 [INFO]: epoch 33: training loss 0.3373\n",
      "2025-05-29 21:01:51 [INFO]: epoch 34: training loss 0.3647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:51 [INFO]: epoch 35: training loss 0.3303\n",
      "2025-05-29 21:01:51 [INFO]: epoch 36: training loss 0.3321\n",
      "2025-05-29 21:01:51 [INFO]: epoch 37: training loss 0.3460\n",
      "2025-05-29 21:01:51 [INFO]: epoch 38: training loss 0.3660\n",
      "2025-05-29 21:01:51 [INFO]: epoch 39: training loss 0.3350\n",
      "2025-05-29 21:01:51 [INFO]: epoch 40: training loss 0.3213\n",
      "2025-05-29 21:01:51 [INFO]: epoch 41: training loss 0.3055\n",
      "2025-05-29 21:01:51 [INFO]: epoch 42: training loss 0.3299\n",
      "2025-05-29 21:01:51 [INFO]: epoch 43: training loss 0.3727\n",
      "2025-05-29 21:01:51 [INFO]: epoch 44: training loss 0.3360\n",
      "2025-05-29 21:01:51 [INFO]: epoch 45: training loss 0.3069\n",
      "2025-05-29 21:01:51 [INFO]: epoch 46: training loss 0.3167\n",
      "2025-05-29 21:01:51 [INFO]: epoch 47: training loss 0.3138\n",
      "2025-05-29 21:01:51 [INFO]: epoch 48: training loss 0.3381\n",
      "2025-05-29 21:01:51 [INFO]: epoch 49: training loss 0.3178\n",
      "2025-05-29 21:01:51 [INFO]: epoch 50: training loss 0.3276\n",
      "2025-05-29 21:01:51 [INFO]: epoch 51: training loss 0.3231\n",
      "2025-05-29 21:01:51 [INFO]: epoch 52: training loss 0.3121\n",
      "2025-05-29 21:01:51 [INFO]: epoch 53: training loss 0.3031\n",
      "2025-05-29 21:01:51 [INFO]: epoch 54: training loss 0.3094\n",
      "2025-05-29 21:01:51 [INFO]: epoch 55: training loss 0.3045\n",
      "2025-05-29 21:01:51 [INFO]: epoch 56: training loss 0.3047\n",
      "2025-05-29 21:01:51 [INFO]: epoch 57: training loss 0.3111\n",
      "2025-05-29 21:01:51 [INFO]: epoch 58: training loss 0.3085\n",
      "2025-05-29 21:01:51 [INFO]: epoch 59: training loss 0.3019\n",
      "2025-05-29 21:01:51 [INFO]: epoch 60: training loss 0.3018\n",
      "2025-05-29 21:01:51 [INFO]: epoch 61: training loss 0.3208\n",
      "2025-05-29 21:01:51 [INFO]: epoch 62: training loss 0.3063\n",
      "2025-05-29 21:01:51 [INFO]: epoch 63: training loss 0.2996\n",
      "2025-05-29 21:01:51 [INFO]: epoch 64: training loss 0.2969\n",
      "2025-05-29 21:01:51 [INFO]: epoch 65: training loss 0.3060\n",
      "2025-05-29 21:01:51 [INFO]: epoch 66: training loss 0.3283\n",
      "2025-05-29 21:01:51 [INFO]: epoch 67: training loss 0.3063\n",
      "2025-05-29 21:01:51 [INFO]: epoch 68: training loss 0.2876\n",
      "2025-05-29 21:01:51 [INFO]: epoch 69: training loss 0.2846\n",
      "2025-05-29 21:01:51 [INFO]: epoch 70: training loss 0.3274\n",
      "2025-05-29 21:01:51 [INFO]: epoch 71: training loss 0.3242\n",
      "2025-05-29 21:01:51 [INFO]: epoch 72: training loss 0.3358\n",
      "2025-05-29 21:01:51 [INFO]: epoch 73: training loss 0.3090\n",
      "2025-05-29 21:01:51 [INFO]: epoch 74: training loss 0.3158\n",
      "2025-05-29 21:01:51 [INFO]: epoch 75: training loss 0.3077\n",
      "2025-05-29 21:01:51 [INFO]: epoch 76: training loss 0.3075\n",
      "2025-05-29 21:01:51 [INFO]: epoch 77: training loss 0.3197\n",
      "2025-05-29 21:01:51 [INFO]: epoch 78: training loss 0.3078\n",
      "2025-05-29 21:01:51 [INFO]: epoch 79: training loss 0.2784\n",
      "2025-05-29 21:01:51 [INFO]: epoch 80: training loss 0.2918\n",
      "2025-05-29 21:01:51 [INFO]: epoch 81: training loss 0.2927\n",
      "2025-05-29 21:01:51 [INFO]: epoch 82: training loss 0.2929\n",
      "2025-05-29 21:01:51 [INFO]: epoch 83: training loss 0.2778\n",
      "2025-05-29 21:01:51 [INFO]: epoch 84: training loss 0.2690\n",
      "2025-05-29 21:01:51 [INFO]: epoch 85: training loss 0.2653\n",
      "2025-05-29 21:01:51 [INFO]: epoch 86: training loss 0.2679\n",
      "2025-05-29 21:01:51 [INFO]: epoch 87: training loss 0.2581\n",
      "2025-05-29 21:01:51 [INFO]: epoch 88: training loss 0.2814\n",
      "2025-05-29 21:01:51 [INFO]: epoch 89: training loss 0.2505\n",
      "2025-05-29 21:01:51 [INFO]: epoch 90: training loss 0.2768\n",
      "2025-05-29 21:01:51 [INFO]: epoch 91: training loss 0.2678\n",
      "2025-05-29 21:01:51 [INFO]: epoch 92: training loss 0.2877\n",
      "2025-05-29 21:01:51 [INFO]: epoch 93: training loss 0.2665\n",
      "2025-05-29 21:01:51 [INFO]: epoch 94: training loss 0.2643\n",
      "2025-05-29 21:01:51 [INFO]: epoch 95: training loss 0.2693\n",
      "2025-05-29 21:01:51 [INFO]: epoch 96: training loss 0.2659\n",
      "2025-05-29 21:01:51 [INFO]: epoch 97: training loss 0.2503\n",
      "2025-05-29 21:01:51 [INFO]: epoch 98: training loss 0.2755\n",
      "2025-05-29 21:01:51 [INFO]: epoch 99: training loss 0.2588\n",
      "2025-05-29 21:01:51 [INFO]: epoch 100: training loss 0.2799\n",
      "2025-05-29 21:01:51 [INFO]: epoch 101: training loss 0.2660\n",
      "2025-05-29 21:01:51 [INFO]: epoch 102: training loss 0.2663\n",
      "2025-05-29 21:01:51 [INFO]: epoch 103: training loss 0.2530\n",
      "2025-05-29 21:01:51 [INFO]: epoch 104: training loss 0.2710\n",
      "2025-05-29 21:01:51 [INFO]: epoch 105: training loss 0.2820\n",
      "2025-05-29 21:01:51 [INFO]: epoch 106: training loss 0.2472\n",
      "2025-05-29 21:01:51 [INFO]: epoch 107: training loss 0.2925\n",
      "2025-05-29 21:01:51 [INFO]: epoch 108: training loss 0.2622\n",
      "2025-05-29 21:01:51 [INFO]: epoch 109: training loss 0.2823\n",
      "2025-05-29 21:01:51 [INFO]: epoch 110: training loss 0.2859\n",
      "2025-05-29 21:01:51 [INFO]: epoch 111: training loss 0.2497\n",
      "2025-05-29 21:01:51 [INFO]: epoch 112: training loss 0.2355\n",
      "2025-05-29 21:01:52 [INFO]: epoch 113: training loss 0.2701\n",
      "2025-05-29 21:01:52 [INFO]: epoch 114: training loss 0.2412\n",
      "2025-05-29 21:01:52 [INFO]: epoch 115: training loss 0.2541\n",
      "2025-05-29 21:01:52 [INFO]: epoch 116: training loss 0.2461\n",
      "2025-05-29 21:01:52 [INFO]: epoch 117: training loss 0.2484\n",
      "2025-05-29 21:01:52 [INFO]: epoch 118: training loss 0.2502\n",
      "2025-05-29 21:01:52 [INFO]: epoch 119: training loss 0.2639\n",
      "2025-05-29 21:01:52 [INFO]: epoch 120: training loss 0.2835\n",
      "2025-05-29 21:01:52 [INFO]: epoch 121: training loss 0.2605\n",
      "2025-05-29 21:01:52 [INFO]: epoch 122: training loss 0.2513\n",
      "2025-05-29 21:01:52 [INFO]: epoch 123: training loss 0.2614\n",
      "2025-05-29 21:01:52 [INFO]: epoch 124: training loss 0.2656\n",
      "2025-05-29 21:01:52 [INFO]: epoch 125: training loss 0.2685\n",
      "2025-05-29 21:01:52 [INFO]: epoch 126: training loss 0.2745\n",
      "2025-05-29 21:01:52 [INFO]: epoch 127: training loss 0.2489\n",
      "2025-05-29 21:01:52 [INFO]: epoch 128: training loss 0.2571\n",
      "2025-05-29 21:01:52 [INFO]: epoch 129: training loss 0.2520\n",
      "2025-05-29 21:01:52 [INFO]: epoch 130: training loss 0.2508\n",
      "2025-05-29 21:01:52 [INFO]: epoch 131: training loss 0.2237\n",
      "2025-05-29 21:01:52 [INFO]: epoch 132: training loss 0.2368\n",
      "2025-05-29 21:01:52 [INFO]: epoch 133: training loss 0.2268\n",
      "2025-05-29 21:01:52 [INFO]: epoch 134: training loss 0.2521\n",
      "2025-05-29 21:01:52 [INFO]: epoch 135: training loss 0.2568\n",
      "2025-05-29 21:01:52 [INFO]: epoch 136: training loss 0.2457\n",
      "2025-05-29 21:01:52 [INFO]: epoch 137: training loss 0.2390\n",
      "2025-05-29 21:01:52 [INFO]: epoch 138: training loss 0.2375\n",
      "2025-05-29 21:01:52 [INFO]: epoch 139: training loss 0.2257\n",
      "2025-05-29 21:01:52 [INFO]: epoch 140: training loss 0.2653\n",
      "2025-05-29 21:01:52 [INFO]: epoch 141: training loss 0.2353\n",
      "2025-05-29 21:01:52 [INFO]: epoch 142: training loss 0.2409\n",
      "2025-05-29 21:01:52 [INFO]: epoch 143: training loss 0.2409\n",
      "2025-05-29 21:01:52 [INFO]: epoch 144: training loss 0.2121\n",
      "2025-05-29 21:01:52 [INFO]: epoch 145: training loss 0.2277\n",
      "2025-05-29 21:01:52 [INFO]: epoch 146: training loss 0.2346\n",
      "2025-05-29 21:01:52 [INFO]: epoch 147: training loss 0.2345\n",
      "2025-05-29 21:01:52 [INFO]: epoch 148: training loss 0.2333\n",
      "2025-05-29 21:01:52 [INFO]: epoch 149: training loss 0.2298\n",
      "2025-05-29 21:01:52 [INFO]: epoch 150: training loss 0.2255\n",
      "2025-05-29 21:01:52 [INFO]: epoch 151: training loss 0.2331\n",
      "2025-05-29 21:01:52 [INFO]: epoch 152: training loss 0.2332\n",
      "2025-05-29 21:01:52 [INFO]: epoch 153: training loss 0.2305\n",
      "2025-05-29 21:01:52 [INFO]: epoch 154: training loss 0.2384\n",
      "2025-05-29 21:01:52 [INFO]: epoch 155: training loss 0.2236\n",
      "2025-05-29 21:01:52 [INFO]: epoch 156: training loss 0.2131\n",
      "2025-05-29 21:01:52 [INFO]: epoch 157: training loss 0.2334\n",
      "2025-05-29 21:01:52 [INFO]: epoch 158: training loss 0.2196\n",
      "2025-05-29 21:01:52 [INFO]: epoch 159: training loss 0.2211\n",
      "2025-05-29 21:01:52 [INFO]: epoch 160: training loss 0.2270\n",
      "2025-05-29 21:01:52 [INFO]: epoch 161: training loss 0.2167\n",
      "2025-05-29 21:01:52 [INFO]: epoch 162: training loss 0.2518\n",
      "2025-05-29 21:01:52 [INFO]: epoch 163: training loss 0.2290\n",
      "2025-05-29 21:01:52 [INFO]: epoch 164: training loss 0.2323\n",
      "2025-05-29 21:01:52 [INFO]: epoch 165: training loss 0.2200\n",
      "2025-05-29 21:01:52 [INFO]: epoch 166: training loss 0.2258\n",
      "2025-05-29 21:01:52 [INFO]: epoch 167: training loss 0.2150\n",
      "2025-05-29 21:01:52 [INFO]: epoch 168: training loss 0.2265\n",
      "2025-05-29 21:01:52 [INFO]: epoch 169: training loss 0.2217\n",
      "2025-05-29 21:01:52 [INFO]: epoch 170: training loss 0.2133\n",
      "2025-05-29 21:01:52 [INFO]: epoch 171: training loss 0.2243\n",
      "2025-05-29 21:01:52 [INFO]: epoch 172: training loss 0.2112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:52 [INFO]: epoch 173: training loss 0.2123\n",
      "2025-05-29 21:01:52 [INFO]: epoch 174: training loss 0.2164\n",
      "2025-05-29 21:01:52 [INFO]: epoch 175: training loss 0.2146\n",
      "2025-05-29 21:01:52 [INFO]: epoch 176: training loss 0.2085\n",
      "2025-05-29 21:01:52 [INFO]: epoch 177: training loss 0.2215\n",
      "2025-05-29 21:01:52 [INFO]: epoch 178: training loss 0.2119\n",
      "2025-05-29 21:01:52 [INFO]: epoch 179: training loss 0.1991\n",
      "2025-05-29 21:01:52 [INFO]: epoch 180: training loss 0.2070\n",
      "2025-05-29 21:01:52 [INFO]: epoch 181: training loss 0.2112\n",
      "2025-05-29 21:01:52 [INFO]: epoch 182: training loss 0.2040\n",
      "2025-05-29 21:01:52 [INFO]: epoch 183: training loss 0.2037\n",
      "2025-05-29 21:01:52 [INFO]: epoch 184: training loss 0.2093\n",
      "2025-05-29 21:01:52 [INFO]: epoch 185: training loss 0.2242\n",
      "2025-05-29 21:01:52 [INFO]: epoch 186: training loss 0.2113\n",
      "2025-05-29 21:01:52 [INFO]: epoch 187: training loss 0.2097\n",
      "2025-05-29 21:01:52 [INFO]: epoch 188: training loss 0.2002\n",
      "2025-05-29 21:01:52 [INFO]: epoch 189: training loss 0.1977\n",
      "2025-05-29 21:01:52 [INFO]: epoch 190: training loss 0.2120\n",
      "2025-05-29 21:01:52 [INFO]: epoch 191: training loss 0.2167\n",
      "2025-05-29 21:01:52 [INFO]: epoch 192: training loss 0.1967\n",
      "2025-05-29 21:01:53 [INFO]: epoch 193: training loss 0.2029\n",
      "2025-05-29 21:01:53 [INFO]: epoch 194: training loss 0.1963\n",
      "2025-05-29 21:01:53 [INFO]: epoch 195: training loss 0.2046\n",
      "2025-05-29 21:01:53 [INFO]: epoch 196: training loss 0.2047\n",
      "2025-05-29 21:01:53 [INFO]: epoch 197: training loss 0.1991\n",
      "2025-05-29 21:01:53 [INFO]: epoch 198: training loss 0.2021\n",
      "2025-05-29 21:01:53 [INFO]: epoch 199: training loss 0.1908\n",
      "2025-05-29 21:01:53 [INFO]: epoch 200: training loss 0.1976\n",
      "2025-05-29 21:01:53 [INFO]: epoch 201: training loss 0.1986\n",
      "2025-05-29 21:01:53 [INFO]: epoch 202: training loss 0.2097\n",
      "2025-05-29 21:01:53 [INFO]: epoch 203: training loss 0.2016\n",
      "2025-05-29 21:01:53 [INFO]: epoch 204: training loss 0.1913\n",
      "2025-05-29 21:01:53 [INFO]: epoch 205: training loss 0.1939\n",
      "2025-05-29 21:01:53 [INFO]: epoch 206: training loss 0.1974\n",
      "2025-05-29 21:01:53 [INFO]: epoch 207: training loss 0.2001\n",
      "2025-05-29 21:01:53 [INFO]: epoch 208: training loss 0.2033\n",
      "2025-05-29 21:01:53 [INFO]: epoch 209: training loss 0.1988\n",
      "2025-05-29 21:01:53 [INFO]: epoch 210: training loss 0.2009\n",
      "2025-05-29 21:01:53 [INFO]: epoch 211: training loss 0.1980\n",
      "2025-05-29 21:01:53 [INFO]: epoch 212: training loss 0.1981\n",
      "2025-05-29 21:01:53 [INFO]: epoch 213: training loss 0.1903\n",
      "2025-05-29 21:01:53 [INFO]: epoch 214: training loss 0.2031\n",
      "2025-05-29 21:01:53 [INFO]: epoch 215: training loss 0.1881\n",
      "2025-05-29 21:01:53 [INFO]: epoch 216: training loss 0.2100\n",
      "2025-05-29 21:01:53 [INFO]: epoch 217: training loss 0.2189\n",
      "2025-05-29 21:01:53 [INFO]: epoch 218: training loss 0.1991\n",
      "2025-05-29 21:01:53 [INFO]: epoch 219: training loss 0.1969\n",
      "2025-05-29 21:01:53 [INFO]: epoch 220: training loss 0.1896\n",
      "2025-05-29 21:01:53 [INFO]: epoch 221: training loss 0.1972\n",
      "2025-05-29 21:01:53 [INFO]: epoch 222: training loss 0.1979\n",
      "2025-05-29 21:01:53 [INFO]: epoch 223: training loss 0.1874\n",
      "2025-05-29 21:01:53 [INFO]: epoch 224: training loss 0.2023\n",
      "2025-05-29 21:01:53 [INFO]: epoch 225: training loss 0.1913\n",
      "2025-05-29 21:01:53 [INFO]: epoch 226: training loss 0.1917\n",
      "2025-05-29 21:01:53 [INFO]: epoch 227: training loss 0.2013\n",
      "2025-05-29 21:01:53 [INFO]: epoch 228: training loss 0.2055\n",
      "2025-05-29 21:01:53 [INFO]: epoch 229: training loss 0.1995\n",
      "2025-05-29 21:01:53 [INFO]: epoch 230: training loss 0.1796\n",
      "2025-05-29 21:01:53 [INFO]: epoch 231: training loss 0.1837\n",
      "2025-05-29 21:01:53 [INFO]: epoch 232: training loss 0.1862\n",
      "2025-05-29 21:01:53 [INFO]: epoch 233: training loss 0.1789\n",
      "2025-05-29 21:01:53 [INFO]: epoch 234: training loss 0.1972\n",
      "2025-05-29 21:01:53 [INFO]: epoch 235: training loss 0.1905\n",
      "2025-05-29 21:01:53 [INFO]: epoch 236: training loss 0.1797\n",
      "2025-05-29 21:01:53 [INFO]: epoch 237: training loss 0.1864\n",
      "2025-05-29 21:01:53 [INFO]: epoch 238: training loss 0.1965\n",
      "2025-05-29 21:01:53 [INFO]: epoch 239: training loss 0.1817\n",
      "2025-05-29 21:01:53 [INFO]: epoch 240: training loss 0.1904\n",
      "2025-05-29 21:01:53 [INFO]: epoch 241: training loss 0.1856\n",
      "2025-05-29 21:01:53 [INFO]: epoch 242: training loss 0.1862\n",
      "2025-05-29 21:01:53 [INFO]: epoch 243: training loss 0.1794\n",
      "2025-05-29 21:01:53 [INFO]: epoch 244: training loss 0.1816\n",
      "2025-05-29 21:01:53 [INFO]: epoch 245: training loss 0.1879\n",
      "2025-05-29 21:01:53 [INFO]: epoch 246: training loss 0.1714\n",
      "2025-05-29 21:01:53 [INFO]: epoch 247: training loss 0.1739\n",
      "2025-05-29 21:01:53 [INFO]: epoch 248: training loss 0.1894\n",
      "2025-05-29 21:01:53 [INFO]: epoch 249: training loss 0.1786\n",
      "2025-05-29 21:01:53 [INFO]: epoch 250: training loss 0.1700\n",
      "2025-05-29 21:01:53 [INFO]: epoch 251: training loss 0.1759\n",
      "2025-05-29 21:01:53 [INFO]: epoch 252: training loss 0.1779\n",
      "2025-05-29 21:01:53 [INFO]: epoch 253: training loss 0.1710\n",
      "2025-05-29 21:01:53 [INFO]: epoch 254: training loss 0.1773\n",
      "2025-05-29 21:01:53 [INFO]: epoch 255: training loss 0.1681\n",
      "2025-05-29 21:01:53 [INFO]: epoch 256: training loss 0.1719\n",
      "2025-05-29 21:01:53 [INFO]: epoch 257: training loss 0.1684\n",
      "2025-05-29 21:01:53 [INFO]: epoch 258: training loss 0.1668\n",
      "2025-05-29 21:01:53 [INFO]: epoch 259: training loss 0.1622\n",
      "2025-05-29 21:01:53 [INFO]: epoch 260: training loss 0.1605\n",
      "2025-05-29 21:01:53 [INFO]: epoch 261: training loss 0.1626\n",
      "2025-05-29 21:01:53 [INFO]: epoch 262: training loss 0.1725\n",
      "2025-05-29 21:01:53 [INFO]: epoch 263: training loss 0.1673\n",
      "2025-05-29 21:01:53 [INFO]: epoch 264: training loss 0.1721\n",
      "2025-05-29 21:01:53 [INFO]: epoch 265: training loss 0.1685\n",
      "2025-05-29 21:01:53 [INFO]: epoch 266: training loss 0.1803\n",
      "2025-05-29 21:01:53 [INFO]: epoch 267: training loss 0.1644\n",
      "2025-05-29 21:01:53 [INFO]: epoch 268: training loss 0.1743\n",
      "2025-05-29 21:01:53 [INFO]: epoch 269: training loss 0.1475\n",
      "2025-05-29 21:01:53 [INFO]: epoch 270: training loss 0.1632\n",
      "2025-05-29 21:01:53 [INFO]: epoch 271: training loss 0.1637\n",
      "2025-05-29 21:01:53 [INFO]: epoch 272: training loss 0.1500\n",
      "2025-05-29 21:01:53 [INFO]: epoch 273: training loss 0.1629\n",
      "2025-05-29 21:01:54 [INFO]: epoch 274: training loss 0.1649\n",
      "2025-05-29 21:01:54 [INFO]: epoch 275: training loss 0.1653\n",
      "2025-05-29 21:01:54 [INFO]: epoch 276: training loss 0.1630\n",
      "2025-05-29 21:01:54 [INFO]: epoch 277: training loss 0.1571\n",
      "2025-05-29 21:01:54 [INFO]: epoch 278: training loss 0.1733\n",
      "2025-05-29 21:01:54 [INFO]: epoch 279: training loss 0.1668\n",
      "2025-05-29 21:01:54 [INFO]: epoch 280: training loss 0.1680\n",
      "2025-05-29 21:01:54 [INFO]: epoch 281: training loss 0.1697\n",
      "2025-05-29 21:01:54 [INFO]: epoch 282: training loss 0.1695\n",
      "2025-05-29 21:01:54 [INFO]: epoch 283: training loss 0.1653\n",
      "2025-05-29 21:01:54 [INFO]: epoch 284: training loss 0.1562\n",
      "2025-05-29 21:01:54 [INFO]: epoch 285: training loss 0.1481\n",
      "2025-05-29 21:01:54 [INFO]: epoch 286: training loss 0.1610\n",
      "2025-05-29 21:01:54 [INFO]: epoch 287: training loss 0.1655\n",
      "2025-05-29 21:01:54 [INFO]: epoch 288: training loss 0.1458\n",
      "2025-05-29 21:01:54 [INFO]: epoch 289: training loss 0.1571\n",
      "2025-05-29 21:01:54 [INFO]: epoch 290: training loss 0.1733\n",
      "2025-05-29 21:01:54 [INFO]: epoch 291: training loss 0.1482\n",
      "2025-05-29 21:01:54 [INFO]: epoch 292: training loss 0.1614\n",
      "2025-05-29 21:01:54 [INFO]: epoch 293: training loss 0.1573\n",
      "2025-05-29 21:01:54 [INFO]: epoch 294: training loss 0.1565\n",
      "2025-05-29 21:01:54 [INFO]: epoch 295: training loss 0.1584\n",
      "2025-05-29 21:01:54 [INFO]: epoch 296: training loss 0.1581\n",
      "2025-05-29 21:01:54 [INFO]: epoch 297: training loss 0.1589\n",
      "2025-05-29 21:01:54 [INFO]: epoch 298: training loss 0.1525\n",
      "2025-05-29 21:01:54 [INFO]: epoch 299: training loss 0.1605\n",
      "2025-05-29 21:01:54 [INFO]: epoch 300: training loss 0.1623\n",
      "2025-05-29 21:01:54 [INFO]: epoch 301: training loss 0.1654\n",
      "2025-05-29 21:01:54 [INFO]: epoch 302: training loss 0.1580\n",
      "2025-05-29 21:01:54 [INFO]: epoch 303: training loss 0.1577\n",
      "2025-05-29 21:01:54 [INFO]: epoch 304: training loss 0.1473\n",
      "2025-05-29 21:01:54 [INFO]: epoch 305: training loss 0.1567\n",
      "2025-05-29 21:01:54 [INFO]: epoch 306: training loss 0.1710\n",
      "2025-05-29 21:01:54 [INFO]: epoch 307: training loss 0.1883\n",
      "2025-05-29 21:01:54 [INFO]: epoch 308: training loss 0.1704\n",
      "2025-05-29 21:01:54 [INFO]: epoch 309: training loss 0.1511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:54 [INFO]: epoch 310: training loss 0.1684\n",
      "2025-05-29 21:01:54 [INFO]: epoch 311: training loss 0.1705\n",
      "2025-05-29 21:01:54 [INFO]: epoch 312: training loss 0.1551\n",
      "2025-05-29 21:01:54 [INFO]: epoch 313: training loss 0.1565\n",
      "2025-05-29 21:01:54 [INFO]: epoch 314: training loss 0.1518\n",
      "2025-05-29 21:01:54 [INFO]: epoch 315: training loss 0.1508\n",
      "2025-05-29 21:01:54 [INFO]: epoch 316: training loss 0.1549\n",
      "2025-05-29 21:01:54 [INFO]: epoch 317: training loss 0.1587\n",
      "2025-05-29 21:01:54 [INFO]: epoch 318: training loss 0.1504\n",
      "2025-05-29 21:01:54 [INFO]: epoch 319: training loss 0.1453\n",
      "2025-05-29 21:01:54 [INFO]: epoch 320: training loss 0.1466\n",
      "2025-05-29 21:01:54 [INFO]: epoch 321: training loss 0.1530\n",
      "2025-05-29 21:01:54 [INFO]: epoch 322: training loss 0.1558\n",
      "2025-05-29 21:01:54 [INFO]: epoch 323: training loss 0.1566\n",
      "2025-05-29 21:01:54 [INFO]: epoch 324: training loss 0.1481\n",
      "2025-05-29 21:01:54 [INFO]: epoch 325: training loss 0.1562\n",
      "2025-05-29 21:01:54 [INFO]: epoch 326: training loss 0.1538\n",
      "2025-05-29 21:01:54 [INFO]: epoch 327: training loss 0.1465\n",
      "2025-05-29 21:01:54 [INFO]: epoch 328: training loss 0.1536\n",
      "2025-05-29 21:01:54 [INFO]: epoch 329: training loss 0.1545\n",
      "2025-05-29 21:01:54 [INFO]: epoch 330: training loss 0.1485\n",
      "2025-05-29 21:01:54 [INFO]: epoch 331: training loss 0.1547\n",
      "2025-05-29 21:01:54 [INFO]: epoch 332: training loss 0.1455\n",
      "2025-05-29 21:01:54 [INFO]: epoch 333: training loss 0.1494\n",
      "2025-05-29 21:01:54 [INFO]: epoch 334: training loss 0.1499\n",
      "2025-05-29 21:01:54 [INFO]: epoch 335: training loss 0.1395\n",
      "2025-05-29 21:01:54 [INFO]: epoch 336: training loss 0.1345\n",
      "2025-05-29 21:01:54 [INFO]: epoch 337: training loss 0.1422\n",
      "2025-05-29 21:01:54 [INFO]: epoch 338: training loss 0.1513\n",
      "2025-05-29 21:01:54 [INFO]: epoch 339: training loss 0.1434\n",
      "2025-05-29 21:01:54 [INFO]: epoch 340: training loss 0.1411\n",
      "2025-05-29 21:01:54 [INFO]: epoch 341: training loss 0.1408\n",
      "2025-05-29 21:01:54 [INFO]: epoch 342: training loss 0.1391\n",
      "2025-05-29 21:01:54 [INFO]: epoch 343: training loss 0.1380\n",
      "2025-05-29 21:01:54 [INFO]: epoch 344: training loss 0.1409\n",
      "2025-05-29 21:01:54 [INFO]: epoch 345: training loss 0.1437\n",
      "2025-05-29 21:01:54 [INFO]: epoch 346: training loss 0.1379\n",
      "2025-05-29 21:01:54 [INFO]: epoch 347: training loss 0.1367\n",
      "2025-05-29 21:01:54 [INFO]: epoch 348: training loss 0.1328\n",
      "2025-05-29 21:01:54 [INFO]: epoch 349: training loss 0.1291\n",
      "2025-05-29 21:01:54 [INFO]: epoch 350: training loss 0.1327\n",
      "2025-05-29 21:01:54 [INFO]: epoch 351: training loss 0.1306\n",
      "2025-05-29 21:01:54 [INFO]: epoch 352: training loss 0.1361\n",
      "2025-05-29 21:01:54 [INFO]: epoch 353: training loss 0.1283\n",
      "2025-05-29 21:01:54 [INFO]: epoch 354: training loss 0.1410\n",
      "2025-05-29 21:01:55 [INFO]: epoch 355: training loss 0.1255\n",
      "2025-05-29 21:01:55 [INFO]: epoch 356: training loss 0.1262\n",
      "2025-05-29 21:01:55 [INFO]: epoch 357: training loss 0.1316\n",
      "2025-05-29 21:01:55 [INFO]: epoch 358: training loss 0.1317\n",
      "2025-05-29 21:01:55 [INFO]: epoch 359: training loss 0.1422\n",
      "2025-05-29 21:01:55 [INFO]: epoch 360: training loss 0.1360\n",
      "2025-05-29 21:01:55 [INFO]: epoch 361: training loss 0.1247\n",
      "2025-05-29 21:01:55 [INFO]: epoch 362: training loss 0.1340\n",
      "2025-05-29 21:01:55 [INFO]: epoch 363: training loss 0.1349\n",
      "2025-05-29 21:01:55 [INFO]: epoch 364: training loss 0.1338\n",
      "2025-05-29 21:01:55 [INFO]: epoch 365: training loss 0.1372\n",
      "2025-05-29 21:01:55 [INFO]: epoch 366: training loss 0.1340\n",
      "2025-05-29 21:01:55 [INFO]: epoch 367: training loss 0.1256\n",
      "2025-05-29 21:01:55 [INFO]: epoch 368: training loss 0.1404\n",
      "2025-05-29 21:01:55 [INFO]: epoch 369: training loss 0.1397\n",
      "2025-05-29 21:01:55 [INFO]: epoch 370: training loss 0.1333\n",
      "2025-05-29 21:01:55 [INFO]: epoch 371: training loss 0.1294\n",
      "2025-05-29 21:01:55 [INFO]: epoch 372: training loss 0.1355\n",
      "2025-05-29 21:01:55 [INFO]: epoch 373: training loss 0.1363\n",
      "2025-05-29 21:01:55 [INFO]: epoch 374: training loss 0.1368\n",
      "2025-05-29 21:01:55 [INFO]: epoch 375: training loss 0.1276\n",
      "2025-05-29 21:01:55 [INFO]: epoch 376: training loss 0.1306\n",
      "2025-05-29 21:01:55 [INFO]: epoch 377: training loss 0.1344\n",
      "2025-05-29 21:01:55 [INFO]: epoch 378: training loss 0.1349\n",
      "2025-05-29 21:01:55 [INFO]: epoch 379: training loss 0.1355\n",
      "2025-05-29 21:01:55 [INFO]: epoch 380: training loss 0.1361\n",
      "2025-05-29 21:01:55 [INFO]: epoch 381: training loss 0.1413\n",
      "2025-05-29 21:01:55 [INFO]: epoch 382: training loss 0.1272\n",
      "2025-05-29 21:01:55 [INFO]: epoch 383: training loss 0.1263\n",
      "2025-05-29 21:01:55 [INFO]: epoch 384: training loss 0.1346\n",
      "2025-05-29 21:01:55 [INFO]: epoch 385: training loss 0.1324\n",
      "2025-05-29 21:01:55 [INFO]: epoch 386: training loss 0.1195\n",
      "2025-05-29 21:01:55 [INFO]: epoch 387: training loss 0.1255\n",
      "2025-05-29 21:01:55 [INFO]: epoch 388: training loss 0.1296\n",
      "2025-05-29 21:01:55 [INFO]: epoch 389: training loss 0.1296\n",
      "2025-05-29 21:01:55 [INFO]: epoch 390: training loss 0.1223\n",
      "2025-05-29 21:01:55 [INFO]: epoch 391: training loss 0.1201\n",
      "2025-05-29 21:01:55 [INFO]: epoch 392: training loss 0.1276\n",
      "2025-05-29 21:01:55 [INFO]: epoch 393: training loss 0.1204\n",
      "2025-05-29 21:01:55 [INFO]: epoch 394: training loss 0.1159\n",
      "2025-05-29 21:01:55 [INFO]: epoch 395: training loss 0.1137\n",
      "2025-05-29 21:01:55 [INFO]: epoch 396: training loss 0.1187\n",
      "2025-05-29 21:01:55 [INFO]: epoch 397: training loss 0.1301\n",
      "2025-05-29 21:01:55 [INFO]: epoch 398: training loss 0.1172\n",
      "2025-05-29 21:01:55 [INFO]: epoch 399: training loss 0.1231\n",
      "2025-05-29 21:01:55 [INFO]: epoch 400: training loss 0.1088\n",
      "2025-05-29 21:01:55 [INFO]: epoch 401: training loss 0.1066\n",
      "2025-05-29 21:01:55 [INFO]: epoch 402: training loss 0.1221\n",
      "2025-05-29 21:01:55 [INFO]: epoch 403: training loss 0.1220\n",
      "2025-05-29 21:01:55 [INFO]: epoch 404: training loss 0.1250\n",
      "2025-05-29 21:01:55 [INFO]: epoch 405: training loss 0.1231\n",
      "2025-05-29 21:01:55 [INFO]: epoch 406: training loss 0.1159\n",
      "2025-05-29 21:01:55 [INFO]: epoch 407: training loss 0.1263\n",
      "2025-05-29 21:01:55 [INFO]: epoch 408: training loss 0.1271\n",
      "2025-05-29 21:01:55 [INFO]: epoch 409: training loss 0.1242\n",
      "2025-05-29 21:01:55 [INFO]: epoch 410: training loss 0.1304\n",
      "2025-05-29 21:01:55 [INFO]: epoch 411: training loss 0.1182\n",
      "2025-05-29 21:01:55 [INFO]: epoch 412: training loss 0.1293\n",
      "2025-05-29 21:01:55 [INFO]: epoch 413: training loss 0.1195\n",
      "2025-05-29 21:01:55 [INFO]: epoch 414: training loss 0.1166\n",
      "2025-05-29 21:01:55 [INFO]: epoch 415: training loss 0.1098\n",
      "2025-05-29 21:01:55 [INFO]: epoch 416: training loss 0.1200\n",
      "2025-05-29 21:01:55 [INFO]: epoch 417: training loss 0.1212\n",
      "2025-05-29 21:01:55 [INFO]: epoch 418: training loss 0.1266\n",
      "2025-05-29 21:01:55 [INFO]: epoch 419: training loss 0.1282\n",
      "2025-05-29 21:01:55 [INFO]: epoch 420: training loss 0.1332\n",
      "2025-05-29 21:01:55 [INFO]: epoch 421: training loss 0.1169\n",
      "2025-05-29 21:01:55 [INFO]: epoch 422: training loss 0.1292\n",
      "2025-05-29 21:01:55 [INFO]: epoch 423: training loss 0.1255\n",
      "2025-05-29 21:01:55 [INFO]: epoch 424: training loss 0.1185\n",
      "2025-05-29 21:01:55 [INFO]: epoch 425: training loss 0.1157\n",
      "2025-05-29 21:01:55 [INFO]: epoch 426: training loss 0.1062\n",
      "2025-05-29 21:01:55 [INFO]: epoch 427: training loss 0.1139\n",
      "2025-05-29 21:01:55 [INFO]: epoch 428: training loss 0.1188\n",
      "2025-05-29 21:01:55 [INFO]: epoch 429: training loss 0.1134\n",
      "2025-05-29 21:01:55 [INFO]: epoch 430: training loss 0.1202\n",
      "2025-05-29 21:01:55 [INFO]: epoch 431: training loss 0.1101\n",
      "2025-05-29 21:01:55 [INFO]: epoch 432: training loss 0.1176\n",
      "2025-05-29 21:01:55 [INFO]: epoch 433: training loss 0.1231\n",
      "2025-05-29 21:01:55 [INFO]: epoch 434: training loss 0.1145\n",
      "2025-05-29 21:01:55 [INFO]: epoch 435: training loss 0.1174\n",
      "2025-05-29 21:01:56 [INFO]: epoch 436: training loss 0.1135\n",
      "2025-05-29 21:01:56 [INFO]: epoch 437: training loss 0.1222\n",
      "2025-05-29 21:01:56 [INFO]: epoch 438: training loss 0.1189\n",
      "2025-05-29 21:01:56 [INFO]: epoch 439: training loss 0.1129\n",
      "2025-05-29 21:01:56 [INFO]: epoch 440: training loss 0.1172\n",
      "2025-05-29 21:01:56 [INFO]: epoch 441: training loss 0.1084\n",
      "2025-05-29 21:01:56 [INFO]: epoch 442: training loss 0.1051\n",
      "2025-05-29 21:01:56 [INFO]: epoch 443: training loss 0.1108\n",
      "2025-05-29 21:01:56 [INFO]: epoch 444: training loss 0.1110\n",
      "2025-05-29 21:01:56 [INFO]: epoch 445: training loss 0.1117\n",
      "2025-05-29 21:01:56 [INFO]: epoch 446: training loss 0.1042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:56 [INFO]: epoch 447: training loss 0.1122\n",
      "2025-05-29 21:01:56 [INFO]: epoch 448: training loss 0.1142\n",
      "2025-05-29 21:01:56 [INFO]: epoch 449: training loss 0.1040\n",
      "2025-05-29 21:01:56 [INFO]: epoch 450: training loss 0.1063\n",
      "2025-05-29 21:01:56 [INFO]: epoch 451: training loss 0.1132\n",
      "2025-05-29 21:01:56 [INFO]: epoch 452: training loss 0.1113\n",
      "2025-05-29 21:01:56 [INFO]: epoch 453: training loss 0.1082\n",
      "2025-05-29 21:01:56 [INFO]: epoch 454: training loss 0.1103\n",
      "2025-05-29 21:01:56 [INFO]: epoch 455: training loss 0.1128\n",
      "2025-05-29 21:01:56 [INFO]: epoch 456: training loss 0.1033\n",
      "2025-05-29 21:01:56 [INFO]: epoch 457: training loss 0.1122\n",
      "2025-05-29 21:01:56 [INFO]: epoch 458: training loss 0.1088\n",
      "2025-05-29 21:01:56 [INFO]: epoch 459: training loss 0.1146\n",
      "2025-05-29 21:01:56 [INFO]: epoch 460: training loss 0.1161\n",
      "2025-05-29 21:01:56 [INFO]: epoch 461: training loss 0.1083\n",
      "2025-05-29 21:01:56 [INFO]: epoch 462: training loss 0.1067\n",
      "2025-05-29 21:01:56 [INFO]: epoch 463: training loss 0.1095\n",
      "2025-05-29 21:01:56 [INFO]: epoch 464: training loss 0.1087\n",
      "2025-05-29 21:01:56 [INFO]: epoch 465: training loss 0.1130\n",
      "2025-05-29 21:01:56 [INFO]: epoch 466: training loss 0.1116\n",
      "2025-05-29 21:01:56 [INFO]: epoch 467: training loss 0.1029\n",
      "2025-05-29 21:01:56 [INFO]: epoch 468: training loss 0.1096\n",
      "2025-05-29 21:01:56 [INFO]: epoch 469: training loss 0.1123\n",
      "2025-05-29 21:01:56 [INFO]: epoch 470: training loss 0.1140\n",
      "2025-05-29 21:01:56 [INFO]: epoch 471: training loss 0.1054\n",
      "2025-05-29 21:01:56 [INFO]: epoch 472: training loss 0.1004\n",
      "2025-05-29 21:01:56 [INFO]: epoch 473: training loss 0.1114\n",
      "2025-05-29 21:01:56 [INFO]: epoch 474: training loss 0.1045\n",
      "2025-05-29 21:01:56 [INFO]: epoch 475: training loss 0.1033\n",
      "2025-05-29 21:01:56 [INFO]: epoch 476: training loss 0.1053\n",
      "2025-05-29 21:01:56 [INFO]: epoch 477: training loss 0.1011\n",
      "2025-05-29 21:01:56 [INFO]: epoch 478: training loss 0.1085\n",
      "2025-05-29 21:01:56 [INFO]: epoch 479: training loss 0.1029\n",
      "2025-05-29 21:01:56 [INFO]: epoch 480: training loss 0.1022\n",
      "2025-05-29 21:01:56 [INFO]: epoch 481: training loss 0.0987\n",
      "2025-05-29 21:01:56 [INFO]: epoch 482: training loss 0.1005\n",
      "2025-05-29 21:01:56 [INFO]: epoch 483: training loss 0.1059\n",
      "2025-05-29 21:01:56 [INFO]: epoch 484: training loss 0.1067\n",
      "2025-05-29 21:01:56 [INFO]: epoch 485: training loss 0.0982\n",
      "2025-05-29 21:01:56 [INFO]: epoch 486: training loss 0.1054\n",
      "2025-05-29 21:01:56 [INFO]: epoch 487: training loss 0.0976\n",
      "2025-05-29 21:01:56 [INFO]: epoch 488: training loss 0.1063\n",
      "2025-05-29 21:01:56 [INFO]: epoch 489: training loss 0.1063\n",
      "2025-05-29 21:01:56 [INFO]: epoch 490: training loss 0.1096\n",
      "2025-05-29 21:01:56 [INFO]: epoch 491: training loss 0.1034\n",
      "2025-05-29 21:01:56 [INFO]: epoch 492: training loss 0.1023\n",
      "2025-05-29 21:01:56 [INFO]: epoch 493: training loss 0.1053\n",
      "2025-05-29 21:01:56 [INFO]: epoch 494: training loss 0.1000\n",
      "2025-05-29 21:01:56 [INFO]: epoch 495: training loss 0.1035\n",
      "2025-05-29 21:01:56 [INFO]: epoch 496: training loss 0.1017\n",
      "2025-05-29 21:01:56 [INFO]: epoch 497: training loss 0.1012\n",
      "2025-05-29 21:01:56 [INFO]: epoch 498: training loss 0.0948\n",
      "2025-05-29 21:01:56 [INFO]: epoch 499: training loss 0.1095\n",
      "2025-05-29 21:01:56 [INFO]: epoch 500: training loss 0.1165\n",
      "2025-05-29 21:01:56 [INFO]: epoch 501: training loss 0.1048\n",
      "2025-05-29 21:01:56 [INFO]: epoch 502: training loss 0.0948\n",
      "2025-05-29 21:01:56 [INFO]: epoch 503: training loss 0.1036\n",
      "2025-05-29 21:01:56 [INFO]: epoch 504: training loss 0.1053\n",
      "2025-05-29 21:01:56 [INFO]: epoch 505: training loss 0.0967\n",
      "2025-05-29 21:01:56 [INFO]: epoch 506: training loss 0.1027\n",
      "2025-05-29 21:01:56 [INFO]: epoch 507: training loss 0.1013\n",
      "2025-05-29 21:01:56 [INFO]: epoch 508: training loss 0.0940\n",
      "2025-05-29 21:01:56 [INFO]: epoch 509: training loss 0.0991\n",
      "2025-05-29 21:01:56 [INFO]: epoch 510: training loss 0.0963\n",
      "2025-05-29 21:01:56 [INFO]: epoch 511: training loss 0.1001\n",
      "2025-05-29 21:01:56 [INFO]: epoch 512: training loss 0.0990\n",
      "2025-05-29 21:01:56 [INFO]: epoch 513: training loss 0.0953\n",
      "2025-05-29 21:01:56 [INFO]: epoch 514: training loss 0.0898\n",
      "2025-05-29 21:01:56 [INFO]: epoch 515: training loss 0.0955\n",
      "2025-05-29 21:01:56 [INFO]: epoch 516: training loss 0.1003\n",
      "2025-05-29 21:01:57 [INFO]: epoch 517: training loss 0.0944\n",
      "2025-05-29 21:01:57 [INFO]: epoch 518: training loss 0.1026\n",
      "2025-05-29 21:01:57 [INFO]: epoch 519: training loss 0.0941\n",
      "2025-05-29 21:01:57 [INFO]: epoch 520: training loss 0.1026\n",
      "2025-05-29 21:01:57 [INFO]: epoch 521: training loss 0.1096\n",
      "2025-05-29 21:01:57 [INFO]: epoch 522: training loss 0.0940\n",
      "2025-05-29 21:01:57 [INFO]: epoch 523: training loss 0.0906\n",
      "2025-05-29 21:01:57 [INFO]: epoch 524: training loss 0.1018\n",
      "2025-05-29 21:01:57 [INFO]: epoch 525: training loss 0.1063\n",
      "2025-05-29 21:01:57 [INFO]: epoch 526: training loss 0.1051\n",
      "2025-05-29 21:01:57 [INFO]: epoch 527: training loss 0.1024\n",
      "2025-05-29 21:01:57 [INFO]: epoch 528: training loss 0.1028\n",
      "2025-05-29 21:01:57 [INFO]: epoch 529: training loss 0.1039\n",
      "2025-05-29 21:01:57 [INFO]: epoch 530: training loss 0.1103\n",
      "2025-05-29 21:01:57 [INFO]: epoch 531: training loss 0.1111\n",
      "2025-05-29 21:01:57 [INFO]: epoch 532: training loss 0.1040\n",
      "2025-05-29 21:01:57 [INFO]: epoch 533: training loss 0.0984\n",
      "2025-05-29 21:01:57 [INFO]: epoch 534: training loss 0.0984\n",
      "2025-05-29 21:01:57 [INFO]: epoch 535: training loss 0.1146\n",
      "2025-05-29 21:01:57 [INFO]: epoch 536: training loss 0.1083\n",
      "2025-05-29 21:01:57 [INFO]: epoch 537: training loss 0.0990\n",
      "2025-05-29 21:01:57 [INFO]: epoch 538: training loss 0.0903\n",
      "2025-05-29 21:01:57 [INFO]: epoch 539: training loss 0.1012\n",
      "2025-05-29 21:01:57 [INFO]: epoch 540: training loss 0.1002\n",
      "2025-05-29 21:01:57 [INFO]: epoch 541: training loss 0.0923\n",
      "2025-05-29 21:01:57 [INFO]: epoch 542: training loss 0.0943\n",
      "2025-05-29 21:01:57 [INFO]: epoch 543: training loss 0.0923\n",
      "2025-05-29 21:01:57 [INFO]: epoch 544: training loss 0.0942\n",
      "2025-05-29 21:01:57 [INFO]: epoch 545: training loss 0.0913\n",
      "2025-05-29 21:01:57 [INFO]: epoch 546: training loss 0.0963\n",
      "2025-05-29 21:01:57 [INFO]: epoch 547: training loss 0.0923\n",
      "2025-05-29 21:01:57 [INFO]: epoch 548: training loss 0.0964\n",
      "2025-05-29 21:01:57 [INFO]: epoch 549: training loss 0.0933\n",
      "2025-05-29 21:01:57 [INFO]: epoch 550: training loss 0.0884\n",
      "2025-05-29 21:01:57 [INFO]: epoch 551: training loss 0.0864\n",
      "2025-05-29 21:01:57 [INFO]: epoch 552: training loss 0.0884\n",
      "2025-05-29 21:01:57 [INFO]: epoch 553: training loss 0.0910\n",
      "2025-05-29 21:01:57 [INFO]: epoch 554: training loss 0.0942\n",
      "2025-05-29 21:01:57 [INFO]: epoch 555: training loss 0.0846\n",
      "2025-05-29 21:01:57 [INFO]: epoch 556: training loss 0.0891\n",
      "2025-05-29 21:01:57 [INFO]: epoch 557: training loss 0.0870\n",
      "2025-05-29 21:01:57 [INFO]: epoch 558: training loss 0.0958\n",
      "2025-05-29 21:01:57 [INFO]: epoch 559: training loss 0.0912\n",
      "2025-05-29 21:01:57 [INFO]: epoch 560: training loss 0.0966\n",
      "2025-05-29 21:01:57 [INFO]: epoch 561: training loss 0.0878\n",
      "2025-05-29 21:01:57 [INFO]: epoch 562: training loss 0.0915\n",
      "2025-05-29 21:01:57 [INFO]: epoch 563: training loss 0.0934\n",
      "2025-05-29 21:01:57 [INFO]: epoch 564: training loss 0.0933\n",
      "2025-05-29 21:01:57 [INFO]: epoch 565: training loss 0.0851\n",
      "2025-05-29 21:01:57 [INFO]: epoch 566: training loss 0.0805\n",
      "2025-05-29 21:01:57 [INFO]: epoch 567: training loss 0.0863\n",
      "2025-05-29 21:01:57 [INFO]: epoch 568: training loss 0.0863\n",
      "2025-05-29 21:01:57 [INFO]: epoch 569: training loss 0.0864\n",
      "2025-05-29 21:01:57 [INFO]: epoch 570: training loss 0.0849\n",
      "2025-05-29 21:01:57 [INFO]: epoch 571: training loss 0.0905\n",
      "2025-05-29 21:01:57 [INFO]: epoch 572: training loss 0.0862\n",
      "2025-05-29 21:01:57 [INFO]: epoch 573: training loss 0.1010\n",
      "2025-05-29 21:01:57 [INFO]: epoch 574: training loss 0.0934\n",
      "2025-05-29 21:01:57 [INFO]: epoch 575: training loss 0.0968\n",
      "2025-05-29 21:01:57 [INFO]: epoch 576: training loss 0.0877\n",
      "2025-05-29 21:01:57 [INFO]: epoch 577: training loss 0.0919\n",
      "2025-05-29 21:01:57 [INFO]: epoch 578: training loss 0.0913\n",
      "2025-05-29 21:01:57 [INFO]: epoch 579: training loss 0.0960\n",
      "2025-05-29 21:01:57 [INFO]: epoch 580: training loss 0.0904\n",
      "2025-05-29 21:01:57 [INFO]: epoch 581: training loss 0.0884\n",
      "2025-05-29 21:01:57 [INFO]: epoch 582: training loss 0.0867\n",
      "2025-05-29 21:01:57 [INFO]: epoch 583: training loss 0.0886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:57 [INFO]: epoch 584: training loss 0.0820\n",
      "2025-05-29 21:01:57 [INFO]: epoch 585: training loss 0.0834\n",
      "2025-05-29 21:01:57 [INFO]: epoch 586: training loss 0.0875\n",
      "2025-05-29 21:01:57 [INFO]: epoch 587: training loss 0.0879\n",
      "2025-05-29 21:01:57 [INFO]: epoch 588: training loss 0.0810\n",
      "2025-05-29 21:01:57 [INFO]: epoch 589: training loss 0.0868\n",
      "2025-05-29 21:01:57 [INFO]: epoch 590: training loss 0.0864\n",
      "2025-05-29 21:01:57 [INFO]: epoch 591: training loss 0.0911\n",
      "2025-05-29 21:01:57 [INFO]: epoch 592: training loss 0.0819\n",
      "2025-05-29 21:01:57 [INFO]: epoch 593: training loss 0.0919\n",
      "2025-05-29 21:01:57 [INFO]: epoch 594: training loss 0.0856\n",
      "2025-05-29 21:01:57 [INFO]: epoch 595: training loss 0.0821\n",
      "2025-05-29 21:01:57 [INFO]: epoch 596: training loss 0.0884\n",
      "2025-05-29 21:01:57 [INFO]: epoch 597: training loss 0.0864\n",
      "2025-05-29 21:01:57 [INFO]: epoch 598: training loss 0.0804\n",
      "2025-05-29 21:01:58 [INFO]: epoch 599: training loss 0.0830\n",
      "2025-05-29 21:01:58 [INFO]: Finished training.\n",
      "2025-05-29 21:01:58 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:15<00:22,  7.51s/it]2025-05-29 21:01:58 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:01:58 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:01:58 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:01:58 [INFO]: epoch 0: training loss 1.1826\n",
      "2025-05-29 21:01:58 [INFO]: epoch 1: training loss 0.7382\n",
      "2025-05-29 21:01:58 [INFO]: epoch 2: training loss 0.7513\n",
      "2025-05-29 21:01:58 [INFO]: epoch 3: training loss 0.6905\n",
      "2025-05-29 21:01:58 [INFO]: epoch 4: training loss 0.6199\n",
      "2025-05-29 21:01:58 [INFO]: epoch 5: training loss 0.6175\n",
      "2025-05-29 21:01:58 [INFO]: epoch 6: training loss 0.5053\n",
      "2025-05-29 21:01:58 [INFO]: epoch 7: training loss 0.5849\n",
      "2025-05-29 21:01:58 [INFO]: epoch 8: training loss 0.5502\n",
      "2025-05-29 21:01:58 [INFO]: epoch 9: training loss 0.5812\n",
      "2025-05-29 21:01:58 [INFO]: epoch 10: training loss 0.5043\n",
      "2025-05-29 21:01:58 [INFO]: epoch 11: training loss 0.4746\n",
      "2025-05-29 21:01:58 [INFO]: epoch 12: training loss 0.4313\n",
      "2025-05-29 21:01:58 [INFO]: epoch 13: training loss 0.4576\n",
      "2025-05-29 21:01:58 [INFO]: epoch 14: training loss 0.4599\n",
      "2025-05-29 21:01:58 [INFO]: epoch 15: training loss 0.4618\n",
      "2025-05-29 21:01:58 [INFO]: epoch 16: training loss 0.4820\n",
      "2025-05-29 21:01:58 [INFO]: epoch 17: training loss 0.4848\n",
      "2025-05-29 21:01:58 [INFO]: epoch 18: training loss 0.4634\n",
      "2025-05-29 21:01:58 [INFO]: epoch 19: training loss 0.4243\n",
      "2025-05-29 21:01:58 [INFO]: epoch 20: training loss 0.4121\n",
      "2025-05-29 21:01:58 [INFO]: epoch 21: training loss 0.4127\n",
      "2025-05-29 21:01:58 [INFO]: epoch 22: training loss 0.3778\n",
      "2025-05-29 21:01:58 [INFO]: epoch 23: training loss 0.4318\n",
      "2025-05-29 21:01:58 [INFO]: epoch 24: training loss 0.3953\n",
      "2025-05-29 21:01:58 [INFO]: epoch 25: training loss 0.4369\n",
      "2025-05-29 21:01:58 [INFO]: epoch 26: training loss 0.4277\n",
      "2025-05-29 21:01:58 [INFO]: epoch 27: training loss 0.4096\n",
      "2025-05-29 21:01:58 [INFO]: epoch 28: training loss 0.3638\n",
      "2025-05-29 21:01:58 [INFO]: epoch 29: training loss 0.3866\n",
      "2025-05-29 21:01:58 [INFO]: epoch 30: training loss 0.3794\n",
      "2025-05-29 21:01:58 [INFO]: epoch 31: training loss 0.4189\n",
      "2025-05-29 21:01:58 [INFO]: epoch 32: training loss 0.3704\n",
      "2025-05-29 21:01:58 [INFO]: epoch 33: training loss 0.3597\n",
      "2025-05-29 21:01:58 [INFO]: epoch 34: training loss 0.3837\n",
      "2025-05-29 21:01:58 [INFO]: epoch 35: training loss 0.3634\n",
      "2025-05-29 21:01:58 [INFO]: epoch 36: training loss 0.3361\n",
      "2025-05-29 21:01:58 [INFO]: epoch 37: training loss 0.3330\n",
      "2025-05-29 21:01:58 [INFO]: epoch 38: training loss 0.3351\n",
      "2025-05-29 21:01:58 [INFO]: epoch 39: training loss 0.3276\n",
      "2025-05-29 21:01:58 [INFO]: epoch 40: training loss 0.3508\n",
      "2025-05-29 21:01:58 [INFO]: epoch 41: training loss 0.3379\n",
      "2025-05-29 21:01:58 [INFO]: epoch 42: training loss 0.3135\n",
      "2025-05-29 21:01:58 [INFO]: epoch 43: training loss 0.3213\n",
      "2025-05-29 21:01:58 [INFO]: epoch 44: training loss 0.3197\n",
      "2025-05-29 21:01:58 [INFO]: epoch 45: training loss 0.3287\n",
      "2025-05-29 21:01:58 [INFO]: epoch 46: training loss 0.3136\n",
      "2025-05-29 21:01:58 [INFO]: epoch 47: training loss 0.2806\n",
      "2025-05-29 21:01:58 [INFO]: epoch 48: training loss 0.3380\n",
      "2025-05-29 21:01:58 [INFO]: epoch 49: training loss 0.3482\n",
      "2025-05-29 21:01:58 [INFO]: epoch 50: training loss 0.2926\n",
      "2025-05-29 21:01:58 [INFO]: epoch 51: training loss 0.3229\n",
      "2025-05-29 21:01:58 [INFO]: epoch 52: training loss 0.3145\n",
      "2025-05-29 21:01:58 [INFO]: epoch 53: training loss 0.3112\n",
      "2025-05-29 21:01:58 [INFO]: epoch 54: training loss 0.2831\n",
      "2025-05-29 21:01:58 [INFO]: epoch 55: training loss 0.3058\n",
      "2025-05-29 21:01:58 [INFO]: epoch 56: training loss 0.2941\n",
      "2025-05-29 21:01:58 [INFO]: epoch 57: training loss 0.3109\n",
      "2025-05-29 21:01:58 [INFO]: epoch 58: training loss 0.2845\n",
      "2025-05-29 21:01:58 [INFO]: epoch 59: training loss 0.2744\n",
      "2025-05-29 21:01:58 [INFO]: epoch 60: training loss 0.2942\n",
      "2025-05-29 21:01:58 [INFO]: epoch 61: training loss 0.3050\n",
      "2025-05-29 21:01:58 [INFO]: epoch 62: training loss 0.2828\n",
      "2025-05-29 21:01:58 [INFO]: epoch 63: training loss 0.2971\n",
      "2025-05-29 21:01:58 [INFO]: epoch 64: training loss 0.2741\n",
      "2025-05-29 21:01:58 [INFO]: epoch 65: training loss 0.2425\n",
      "2025-05-29 21:01:58 [INFO]: epoch 66: training loss 0.2914\n",
      "2025-05-29 21:01:58 [INFO]: epoch 67: training loss 0.2806\n",
      "2025-05-29 21:01:58 [INFO]: epoch 68: training loss 0.2615\n",
      "2025-05-29 21:01:58 [INFO]: epoch 69: training loss 0.2522\n",
      "2025-05-29 21:01:58 [INFO]: epoch 70: training loss 0.2816\n",
      "2025-05-29 21:01:58 [INFO]: epoch 71: training loss 0.2807\n",
      "2025-05-29 21:01:59 [INFO]: epoch 72: training loss 0.2699\n",
      "2025-05-29 21:01:59 [INFO]: epoch 73: training loss 0.2348\n",
      "2025-05-29 21:01:59 [INFO]: epoch 74: training loss 0.2876\n",
      "2025-05-29 21:01:59 [INFO]: epoch 75: training loss 0.2953\n",
      "2025-05-29 21:01:59 [INFO]: epoch 76: training loss 0.2434\n",
      "2025-05-29 21:01:59 [INFO]: epoch 77: training loss 0.2383\n",
      "2025-05-29 21:01:59 [INFO]: epoch 78: training loss 0.2482\n",
      "2025-05-29 21:01:59 [INFO]: epoch 79: training loss 0.2054\n",
      "2025-05-29 21:01:59 [INFO]: epoch 80: training loss 0.2056\n",
      "2025-05-29 21:01:59 [INFO]: epoch 81: training loss 0.2073\n",
      "2025-05-29 21:01:59 [INFO]: epoch 82: training loss 0.2184\n",
      "2025-05-29 21:01:59 [INFO]: epoch 83: training loss 0.2078\n",
      "2025-05-29 21:01:59 [INFO]: epoch 84: training loss 0.1834\n",
      "2025-05-29 21:01:59 [INFO]: epoch 85: training loss 0.2134\n",
      "2025-05-29 21:01:59 [INFO]: epoch 86: training loss 0.1995\n",
      "2025-05-29 21:01:59 [INFO]: epoch 87: training loss 0.2034\n",
      "2025-05-29 21:01:59 [INFO]: epoch 88: training loss 0.2028\n",
      "2025-05-29 21:01:59 [INFO]: epoch 89: training loss 0.1910\n",
      "2025-05-29 21:01:59 [INFO]: epoch 90: training loss 0.1956\n",
      "2025-05-29 21:01:59 [INFO]: epoch 91: training loss 0.1846\n",
      "2025-05-29 21:01:59 [INFO]: epoch 92: training loss 0.1856\n",
      "2025-05-29 21:01:59 [INFO]: epoch 93: training loss 0.1950\n",
      "2025-05-29 21:01:59 [INFO]: epoch 94: training loss 0.1768\n",
      "2025-05-29 21:01:59 [INFO]: epoch 95: training loss 0.1757\n",
      "2025-05-29 21:01:59 [INFO]: epoch 96: training loss 0.1679\n",
      "2025-05-29 21:01:59 [INFO]: epoch 97: training loss 0.1584\n",
      "2025-05-29 21:01:59 [INFO]: epoch 98: training loss 0.1755\n",
      "2025-05-29 21:01:59 [INFO]: epoch 99: training loss 0.1655\n",
      "2025-05-29 21:01:59 [INFO]: epoch 100: training loss 0.1602\n",
      "2025-05-29 21:01:59 [INFO]: epoch 101: training loss 0.1551\n",
      "2025-05-29 21:01:59 [INFO]: epoch 102: training loss 0.1898\n",
      "2025-05-29 21:01:59 [INFO]: epoch 103: training loss 0.1455\n",
      "2025-05-29 21:01:59 [INFO]: epoch 104: training loss 0.1753\n",
      "2025-05-29 21:01:59 [INFO]: epoch 105: training loss 0.1917\n",
      "2025-05-29 21:01:59 [INFO]: epoch 106: training loss 0.1626\n",
      "2025-05-29 21:01:59 [INFO]: epoch 107: training loss 0.1499\n",
      "2025-05-29 21:01:59 [INFO]: epoch 108: training loss 0.1643\n",
      "2025-05-29 21:01:59 [INFO]: epoch 109: training loss 0.1718\n",
      "2025-05-29 21:01:59 [INFO]: epoch 110: training loss 0.1524\n",
      "2025-05-29 21:01:59 [INFO]: epoch 111: training loss 0.1395\n",
      "2025-05-29 21:01:59 [INFO]: epoch 112: training loss 0.1447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:01:59 [INFO]: epoch 113: training loss 0.1999\n",
      "2025-05-29 21:01:59 [INFO]: epoch 114: training loss 0.1418\n",
      "2025-05-29 21:01:59 [INFO]: epoch 115: training loss 0.1380\n",
      "2025-05-29 21:01:59 [INFO]: epoch 116: training loss 0.1521\n",
      "2025-05-29 21:01:59 [INFO]: epoch 117: training loss 0.1638\n",
      "2025-05-29 21:01:59 [INFO]: epoch 118: training loss 0.1491\n",
      "2025-05-29 21:01:59 [INFO]: epoch 119: training loss 0.1397\n",
      "2025-05-29 21:01:59 [INFO]: epoch 120: training loss 0.1645\n",
      "2025-05-29 21:01:59 [INFO]: epoch 121: training loss 0.1429\n",
      "2025-05-29 21:01:59 [INFO]: epoch 122: training loss 0.1528\n",
      "2025-05-29 21:01:59 [INFO]: epoch 123: training loss 0.1736\n",
      "2025-05-29 21:01:59 [INFO]: epoch 124: training loss 0.1740\n",
      "2025-05-29 21:01:59 [INFO]: epoch 125: training loss 0.1392\n",
      "2025-05-29 21:01:59 [INFO]: epoch 126: training loss 0.1552\n",
      "2025-05-29 21:01:59 [INFO]: epoch 127: training loss 0.1925\n",
      "2025-05-29 21:01:59 [INFO]: epoch 128: training loss 0.1250\n",
      "2025-05-29 21:01:59 [INFO]: epoch 129: training loss 0.1250\n",
      "2025-05-29 21:01:59 [INFO]: epoch 130: training loss 0.1760\n",
      "2025-05-29 21:01:59 [INFO]: epoch 131: training loss 0.1535\n",
      "2025-05-29 21:01:59 [INFO]: epoch 132: training loss 0.1087\n",
      "2025-05-29 21:01:59 [INFO]: epoch 133: training loss 0.1605\n",
      "2025-05-29 21:01:59 [INFO]: epoch 134: training loss 0.1618\n",
      "2025-05-29 21:01:59 [INFO]: epoch 135: training loss 0.1239\n",
      "2025-05-29 21:01:59 [INFO]: epoch 136: training loss 0.0933\n",
      "2025-05-29 21:01:59 [INFO]: epoch 137: training loss 0.1277\n",
      "2025-05-29 21:01:59 [INFO]: epoch 138: training loss 0.1195\n",
      "2025-05-29 21:01:59 [INFO]: epoch 139: training loss 0.1061\n",
      "2025-05-29 21:01:59 [INFO]: epoch 140: training loss 0.1130\n",
      "2025-05-29 21:01:59 [INFO]: epoch 141: training loss 0.1149\n",
      "2025-05-29 21:01:59 [INFO]: epoch 142: training loss 0.1178\n",
      "2025-05-29 21:01:59 [INFO]: epoch 143: training loss 0.0944\n",
      "2025-05-29 21:01:59 [INFO]: epoch 144: training loss 0.0924\n",
      "2025-05-29 21:01:59 [INFO]: epoch 145: training loss 0.0946\n",
      "2025-05-29 21:01:59 [INFO]: epoch 146: training loss 0.0935\n",
      "2025-05-29 21:01:59 [INFO]: epoch 147: training loss 0.1151\n",
      "2025-05-29 21:01:59 [INFO]: epoch 148: training loss 0.1011\n",
      "2025-05-29 21:01:59 [INFO]: epoch 149: training loss 0.0980\n",
      "2025-05-29 21:01:59 [INFO]: epoch 150: training loss 0.0851\n",
      "2025-05-29 21:01:59 [INFO]: epoch 151: training loss 0.0815\n",
      "2025-05-29 21:01:59 [INFO]: epoch 152: training loss 0.0914\n",
      "2025-05-29 21:02:00 [INFO]: epoch 153: training loss 0.1054\n",
      "2025-05-29 21:02:00 [INFO]: epoch 154: training loss 0.0858\n",
      "2025-05-29 21:02:00 [INFO]: epoch 155: training loss 0.0930\n",
      "2025-05-29 21:02:00 [INFO]: epoch 156: training loss 0.0833\n",
      "2025-05-29 21:02:00 [INFO]: epoch 157: training loss 0.1015\n",
      "2025-05-29 21:02:00 [INFO]: epoch 158: training loss 0.0969\n",
      "2025-05-29 21:02:00 [INFO]: epoch 159: training loss 0.0848\n",
      "2025-05-29 21:02:00 [INFO]: epoch 160: training loss 0.0805\n",
      "2025-05-29 21:02:00 [INFO]: epoch 161: training loss 0.0943\n",
      "2025-05-29 21:02:00 [INFO]: epoch 162: training loss 0.0865\n",
      "2025-05-29 21:02:00 [INFO]: epoch 163: training loss 0.0760\n",
      "2025-05-29 21:02:00 [INFO]: epoch 164: training loss 0.0775\n",
      "2025-05-29 21:02:00 [INFO]: epoch 165: training loss 0.0741\n",
      "2025-05-29 21:02:00 [INFO]: epoch 166: training loss 0.0834\n",
      "2025-05-29 21:02:00 [INFO]: epoch 167: training loss 0.0933\n",
      "2025-05-29 21:02:00 [INFO]: epoch 168: training loss 0.0911\n",
      "2025-05-29 21:02:00 [INFO]: epoch 169: training loss 0.0837\n",
      "2025-05-29 21:02:00 [INFO]: epoch 170: training loss 0.0801\n",
      "2025-05-29 21:02:00 [INFO]: epoch 171: training loss 0.1056\n",
      "2025-05-29 21:02:00 [INFO]: epoch 172: training loss 0.0856\n",
      "2025-05-29 21:02:00 [INFO]: epoch 173: training loss 0.0841\n",
      "2025-05-29 21:02:00 [INFO]: epoch 174: training loss 0.0784\n",
      "2025-05-29 21:02:00 [INFO]: epoch 175: training loss 0.0963\n",
      "2025-05-29 21:02:00 [INFO]: epoch 176: training loss 0.0730\n",
      "2025-05-29 21:02:00 [INFO]: epoch 177: training loss 0.0767\n",
      "2025-05-29 21:02:00 [INFO]: epoch 178: training loss 0.0658\n",
      "2025-05-29 21:02:00 [INFO]: epoch 179: training loss 0.0762\n",
      "2025-05-29 21:02:00 [INFO]: epoch 180: training loss 0.0726\n",
      "2025-05-29 21:02:00 [INFO]: epoch 181: training loss 0.0654\n",
      "2025-05-29 21:02:00 [INFO]: epoch 182: training loss 0.0764\n",
      "2025-05-29 21:02:00 [INFO]: epoch 183: training loss 0.0731\n",
      "2025-05-29 21:02:00 [INFO]: epoch 184: training loss 0.0773\n",
      "2025-05-29 21:02:00 [INFO]: epoch 185: training loss 0.0799\n",
      "2025-05-29 21:02:00 [INFO]: epoch 186: training loss 0.0800\n",
      "2025-05-29 21:02:00 [INFO]: epoch 187: training loss 0.0841\n",
      "2025-05-29 21:02:00 [INFO]: epoch 188: training loss 0.0773\n",
      "2025-05-29 21:02:00 [INFO]: epoch 189: training loss 0.0919\n",
      "2025-05-29 21:02:00 [INFO]: epoch 190: training loss 0.0729\n",
      "2025-05-29 21:02:00 [INFO]: epoch 191: training loss 0.0971\n",
      "2025-05-29 21:02:00 [INFO]: epoch 192: training loss 0.0778\n",
      "2025-05-29 21:02:00 [INFO]: epoch 193: training loss 0.0864\n",
      "2025-05-29 21:02:00 [INFO]: epoch 194: training loss 0.0776\n",
      "2025-05-29 21:02:00 [INFO]: epoch 195: training loss 0.0863\n",
      "2025-05-29 21:02:00 [INFO]: epoch 196: training loss 0.0748\n",
      "2025-05-29 21:02:00 [INFO]: epoch 197: training loss 0.0809\n",
      "2025-05-29 21:02:00 [INFO]: epoch 198: training loss 0.0727\n",
      "2025-05-29 21:02:00 [INFO]: epoch 199: training loss 0.0724\n",
      "2025-05-29 21:02:00 [INFO]: epoch 200: training loss 0.0688\n",
      "2025-05-29 21:02:00 [INFO]: epoch 201: training loss 0.0815\n",
      "2025-05-29 21:02:00 [INFO]: epoch 202: training loss 0.0682\n",
      "2025-05-29 21:02:00 [INFO]: epoch 203: training loss 0.0701\n",
      "2025-05-29 21:02:00 [INFO]: epoch 204: training loss 0.0711\n",
      "2025-05-29 21:02:00 [INFO]: epoch 205: training loss 0.0744\n",
      "2025-05-29 21:02:00 [INFO]: epoch 206: training loss 0.0739\n",
      "2025-05-29 21:02:00 [INFO]: epoch 207: training loss 0.0875\n",
      "2025-05-29 21:02:00 [INFO]: epoch 208: training loss 0.0822\n",
      "2025-05-29 21:02:00 [INFO]: epoch 209: training loss 0.0710\n",
      "2025-05-29 21:02:00 [INFO]: epoch 210: training loss 0.0779\n",
      "2025-05-29 21:02:00 [INFO]: epoch 211: training loss 0.0860\n",
      "2025-05-29 21:02:00 [INFO]: epoch 212: training loss 0.0639\n",
      "2025-05-29 21:02:00 [INFO]: epoch 213: training loss 0.0673\n",
      "2025-05-29 21:02:00 [INFO]: epoch 214: training loss 0.0680\n",
      "2025-05-29 21:02:00 [INFO]: epoch 215: training loss 0.0665\n",
      "2025-05-29 21:02:00 [INFO]: epoch 216: training loss 0.0733\n",
      "2025-05-29 21:02:00 [INFO]: epoch 217: training loss 0.0798\n",
      "2025-05-29 21:02:00 [INFO]: epoch 218: training loss 0.0661\n",
      "2025-05-29 21:02:00 [INFO]: epoch 219: training loss 0.0761\n",
      "2025-05-29 21:02:00 [INFO]: epoch 220: training loss 0.0724\n",
      "2025-05-29 21:02:00 [INFO]: epoch 221: training loss 0.0593\n",
      "2025-05-29 21:02:00 [INFO]: epoch 222: training loss 0.0619\n",
      "2025-05-29 21:02:00 [INFO]: epoch 223: training loss 0.0680\n",
      "2025-05-29 21:02:00 [INFO]: epoch 224: training loss 0.0661\n",
      "2025-05-29 21:02:00 [INFO]: epoch 225: training loss 0.0563\n",
      "2025-05-29 21:02:00 [INFO]: epoch 226: training loss 0.0636\n",
      "2025-05-29 21:02:00 [INFO]: epoch 227: training loss 0.0689\n",
      "2025-05-29 21:02:00 [INFO]: epoch 228: training loss 0.0718\n",
      "2025-05-29 21:02:00 [INFO]: epoch 229: training loss 0.0663\n",
      "2025-05-29 21:02:00 [INFO]: epoch 230: training loss 0.0575\n",
      "2025-05-29 21:02:00 [INFO]: epoch 231: training loss 0.0660\n",
      "2025-05-29 21:02:01 [INFO]: epoch 232: training loss 0.0679\n",
      "2025-05-29 21:02:01 [INFO]: epoch 233: training loss 0.0552\n",
      "2025-05-29 21:02:01 [INFO]: epoch 234: training loss 0.0551\n",
      "2025-05-29 21:02:01 [INFO]: epoch 235: training loss 0.0653\n",
      "2025-05-29 21:02:01 [INFO]: epoch 236: training loss 0.0584\n",
      "2025-05-29 21:02:01 [INFO]: epoch 237: training loss 0.0650\n",
      "2025-05-29 21:02:01 [INFO]: epoch 238: training loss 0.0589\n",
      "2025-05-29 21:02:01 [INFO]: epoch 239: training loss 0.0574\n",
      "2025-05-29 21:02:01 [INFO]: epoch 240: training loss 0.0780\n",
      "2025-05-29 21:02:01 [INFO]: epoch 241: training loss 0.0657\n",
      "2025-05-29 21:02:01 [INFO]: epoch 242: training loss 0.0678\n",
      "2025-05-29 21:02:01 [INFO]: epoch 243: training loss 0.0632\n",
      "2025-05-29 21:02:01 [INFO]: epoch 244: training loss 0.0705\n",
      "2025-05-29 21:02:01 [INFO]: epoch 245: training loss 0.0550\n",
      "2025-05-29 21:02:01 [INFO]: epoch 246: training loss 0.0597\n",
      "2025-05-29 21:02:01 [INFO]: epoch 247: training loss 0.0718\n",
      "2025-05-29 21:02:01 [INFO]: epoch 248: training loss 0.0543\n",
      "2025-05-29 21:02:01 [INFO]: epoch 249: training loss 0.0617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:02:01 [INFO]: epoch 250: training loss 0.0842\n",
      "2025-05-29 21:02:01 [INFO]: epoch 251: training loss 0.0618\n",
      "2025-05-29 21:02:01 [INFO]: epoch 252: training loss 0.0771\n",
      "2025-05-29 21:02:01 [INFO]: epoch 253: training loss 0.0710\n",
      "2025-05-29 21:02:01 [INFO]: epoch 254: training loss 0.0640\n",
      "2025-05-29 21:02:01 [INFO]: epoch 255: training loss 0.0568\n",
      "2025-05-29 21:02:01 [INFO]: epoch 256: training loss 0.0519\n",
      "2025-05-29 21:02:01 [INFO]: epoch 257: training loss 0.0541\n",
      "2025-05-29 21:02:01 [INFO]: epoch 258: training loss 0.0671\n",
      "2025-05-29 21:02:01 [INFO]: epoch 259: training loss 0.0697\n",
      "2025-05-29 21:02:01 [INFO]: epoch 260: training loss 0.0557\n",
      "2025-05-29 21:02:01 [INFO]: epoch 261: training loss 0.0610\n",
      "2025-05-29 21:02:01 [INFO]: epoch 262: training loss 0.0605\n",
      "2025-05-29 21:02:01 [INFO]: epoch 263: training loss 0.0538\n",
      "2025-05-29 21:02:01 [INFO]: epoch 264: training loss 0.0567\n",
      "2025-05-29 21:02:01 [INFO]: epoch 265: training loss 0.0500\n",
      "2025-05-29 21:02:01 [INFO]: epoch 266: training loss 0.0642\n",
      "2025-05-29 21:02:01 [INFO]: epoch 267: training loss 0.0511\n",
      "2025-05-29 21:02:01 [INFO]: epoch 268: training loss 0.0534\n",
      "2025-05-29 21:02:01 [INFO]: epoch 269: training loss 0.0485\n",
      "2025-05-29 21:02:01 [INFO]: epoch 270: training loss 0.0543\n",
      "2025-05-29 21:02:01 [INFO]: epoch 271: training loss 0.0423\n",
      "2025-05-29 21:02:01 [INFO]: epoch 272: training loss 0.0496\n",
      "2025-05-29 21:02:01 [INFO]: epoch 273: training loss 0.0541\n",
      "2025-05-29 21:02:01 [INFO]: epoch 274: training loss 0.0456\n",
      "2025-05-29 21:02:01 [INFO]: epoch 275: training loss 0.0558\n",
      "2025-05-29 21:02:01 [INFO]: epoch 276: training loss 0.0413\n",
      "2025-05-29 21:02:01 [INFO]: epoch 277: training loss 0.0464\n",
      "2025-05-29 21:02:01 [INFO]: epoch 278: training loss 0.0504\n",
      "2025-05-29 21:02:01 [INFO]: epoch 279: training loss 0.0512\n",
      "2025-05-29 21:02:01 [INFO]: epoch 280: training loss 0.0523\n",
      "2025-05-29 21:02:01 [INFO]: epoch 281: training loss 0.0546\n",
      "2025-05-29 21:02:01 [INFO]: epoch 282: training loss 0.0501\n",
      "2025-05-29 21:02:01 [INFO]: epoch 283: training loss 0.0608\n",
      "2025-05-29 21:02:01 [INFO]: epoch 284: training loss 0.0525\n",
      "2025-05-29 21:02:01 [INFO]: epoch 285: training loss 0.0458\n",
      "2025-05-29 21:02:01 [INFO]: epoch 286: training loss 0.0469\n",
      "2025-05-29 21:02:01 [INFO]: epoch 287: training loss 0.0584\n",
      "2025-05-29 21:02:01 [INFO]: epoch 288: training loss 0.0514\n",
      "2025-05-29 21:02:01 [INFO]: epoch 289: training loss 0.0575\n",
      "2025-05-29 21:02:01 [INFO]: epoch 290: training loss 0.0550\n",
      "2025-05-29 21:02:01 [INFO]: epoch 291: training loss 0.0534\n",
      "2025-05-29 21:02:01 [INFO]: epoch 292: training loss 0.0550\n",
      "2025-05-29 21:02:01 [INFO]: epoch 293: training loss 0.0419\n",
      "2025-05-29 21:02:01 [INFO]: epoch 294: training loss 0.0456\n",
      "2025-05-29 21:02:01 [INFO]: epoch 295: training loss 0.0518\n",
      "2025-05-29 21:02:01 [INFO]: epoch 296: training loss 0.0449\n",
      "2025-05-29 21:02:01 [INFO]: epoch 297: training loss 0.0592\n",
      "2025-05-29 21:02:01 [INFO]: epoch 298: training loss 0.0464\n",
      "2025-05-29 21:02:01 [INFO]: epoch 299: training loss 0.0425\n",
      "2025-05-29 21:02:01 [INFO]: epoch 300: training loss 0.0567\n",
      "2025-05-29 21:02:01 [INFO]: epoch 301: training loss 0.0553\n",
      "2025-05-29 21:02:01 [INFO]: epoch 302: training loss 0.0416\n",
      "2025-05-29 21:02:01 [INFO]: epoch 303: training loss 0.0477\n",
      "2025-05-29 21:02:01 [INFO]: epoch 304: training loss 0.0535\n",
      "2025-05-29 21:02:01 [INFO]: epoch 305: training loss 0.0508\n",
      "2025-05-29 21:02:01 [INFO]: epoch 306: training loss 0.0526\n",
      "2025-05-29 21:02:01 [INFO]: epoch 307: training loss 0.0516\n",
      "2025-05-29 21:02:01 [INFO]: epoch 308: training loss 0.0504\n",
      "2025-05-29 21:02:01 [INFO]: epoch 309: training loss 0.0636\n",
      "2025-05-29 21:02:01 [INFO]: epoch 310: training loss 0.0501\n",
      "2025-05-29 21:02:01 [INFO]: epoch 311: training loss 0.0480\n",
      "2025-05-29 21:02:01 [INFO]: epoch 312: training loss 0.0529\n",
      "2025-05-29 21:02:01 [INFO]: epoch 313: training loss 0.0532\n",
      "2025-05-29 21:02:02 [INFO]: epoch 314: training loss 0.0437\n",
      "2025-05-29 21:02:02 [INFO]: epoch 315: training loss 0.0529\n",
      "2025-05-29 21:02:02 [INFO]: epoch 316: training loss 0.0632\n",
      "2025-05-29 21:02:02 [INFO]: epoch 317: training loss 0.0492\n",
      "2025-05-29 21:02:02 [INFO]: epoch 318: training loss 0.0522\n",
      "2025-05-29 21:02:02 [INFO]: epoch 319: training loss 0.0524\n",
      "2025-05-29 21:02:02 [INFO]: epoch 320: training loss 0.0449\n",
      "2025-05-29 21:02:02 [INFO]: epoch 321: training loss 0.0459\n",
      "2025-05-29 21:02:02 [INFO]: epoch 322: training loss 0.0544\n",
      "2025-05-29 21:02:02 [INFO]: epoch 323: training loss 0.0631\n",
      "2025-05-29 21:02:02 [INFO]: epoch 324: training loss 0.0418\n",
      "2025-05-29 21:02:02 [INFO]: epoch 325: training loss 0.0501\n",
      "2025-05-29 21:02:02 [INFO]: epoch 326: training loss 0.0568\n",
      "2025-05-29 21:02:02 [INFO]: epoch 327: training loss 0.0567\n",
      "2025-05-29 21:02:02 [INFO]: epoch 328: training loss 0.0636\n",
      "2025-05-29 21:02:02 [INFO]: epoch 329: training loss 0.0512\n",
      "2025-05-29 21:02:02 [INFO]: epoch 330: training loss 0.0549\n",
      "2025-05-29 21:02:02 [INFO]: epoch 331: training loss 0.0518\n",
      "2025-05-29 21:02:02 [INFO]: epoch 332: training loss 0.0558\n",
      "2025-05-29 21:02:02 [INFO]: epoch 333: training loss 0.0545\n",
      "2025-05-29 21:02:02 [INFO]: epoch 334: training loss 0.0550\n",
      "2025-05-29 21:02:02 [INFO]: epoch 335: training loss 0.0532\n",
      "2025-05-29 21:02:02 [INFO]: epoch 336: training loss 0.0446\n",
      "2025-05-29 21:02:02 [INFO]: epoch 337: training loss 0.0463\n",
      "2025-05-29 21:02:02 [INFO]: epoch 338: training loss 0.0491\n",
      "2025-05-29 21:02:02 [INFO]: epoch 339: training loss 0.0445\n",
      "2025-05-29 21:02:02 [INFO]: epoch 340: training loss 0.0485\n",
      "2025-05-29 21:02:02 [INFO]: epoch 341: training loss 0.0559\n",
      "2025-05-29 21:02:02 [INFO]: epoch 342: training loss 0.0339\n",
      "2025-05-29 21:02:02 [INFO]: epoch 343: training loss 0.0517\n",
      "2025-05-29 21:02:02 [INFO]: epoch 344: training loss 0.0602\n",
      "2025-05-29 21:02:02 [INFO]: epoch 345: training loss 0.0403\n",
      "2025-05-29 21:02:02 [INFO]: epoch 346: training loss 0.0388\n",
      "2025-05-29 21:02:02 [INFO]: epoch 347: training loss 0.0640\n",
      "2025-05-29 21:02:02 [INFO]: epoch 348: training loss 0.0430\n",
      "2025-05-29 21:02:02 [INFO]: epoch 349: training loss 0.0424\n",
      "2025-05-29 21:02:02 [INFO]: epoch 350: training loss 0.0499\n",
      "2025-05-29 21:02:02 [INFO]: epoch 351: training loss 0.0400\n",
      "2025-05-29 21:02:02 [INFO]: epoch 352: training loss 0.0497\n",
      "2025-05-29 21:02:02 [INFO]: epoch 353: training loss 0.0477\n",
      "2025-05-29 21:02:02 [INFO]: epoch 354: training loss 0.0526\n",
      "2025-05-29 21:02:02 [INFO]: epoch 355: training loss 0.0506\n",
      "2025-05-29 21:02:02 [INFO]: epoch 356: training loss 0.0438\n",
      "2025-05-29 21:02:02 [INFO]: epoch 357: training loss 0.0473\n",
      "2025-05-29 21:02:02 [INFO]: epoch 358: training loss 0.0489\n",
      "2025-05-29 21:02:02 [INFO]: epoch 359: training loss 0.0507\n",
      "2025-05-29 21:02:02 [INFO]: epoch 360: training loss 0.0401\n",
      "2025-05-29 21:02:02 [INFO]: epoch 361: training loss 0.0356\n",
      "2025-05-29 21:02:02 [INFO]: epoch 362: training loss 0.0410\n",
      "2025-05-29 21:02:02 [INFO]: epoch 363: training loss 0.0358\n",
      "2025-05-29 21:02:02 [INFO]: epoch 364: training loss 0.0446\n",
      "2025-05-29 21:02:02 [INFO]: epoch 365: training loss 0.0397\n",
      "2025-05-29 21:02:02 [INFO]: epoch 366: training loss 0.0434\n",
      "2025-05-29 21:02:02 [INFO]: epoch 367: training loss 0.0440\n",
      "2025-05-29 21:02:02 [INFO]: epoch 368: training loss 0.0454\n",
      "2025-05-29 21:02:02 [INFO]: epoch 369: training loss 0.0468\n",
      "2025-05-29 21:02:02 [INFO]: epoch 370: training loss 0.0351\n",
      "2025-05-29 21:02:02 [INFO]: epoch 371: training loss 0.0369\n",
      "2025-05-29 21:02:02 [INFO]: epoch 372: training loss 0.0431\n",
      "2025-05-29 21:02:02 [INFO]: epoch 373: training loss 0.0392\n",
      "2025-05-29 21:02:02 [INFO]: epoch 374: training loss 0.0365\n",
      "2025-05-29 21:02:02 [INFO]: epoch 375: training loss 0.0345\n",
      "2025-05-29 21:02:02 [INFO]: epoch 376: training loss 0.0385\n",
      "2025-05-29 21:02:02 [INFO]: epoch 377: training loss 0.0401\n",
      "2025-05-29 21:02:02 [INFO]: epoch 378: training loss 0.0357\n",
      "2025-05-29 21:02:02 [INFO]: epoch 379: training loss 0.0331\n",
      "2025-05-29 21:02:02 [INFO]: epoch 380: training loss 0.0367\n",
      "2025-05-29 21:02:02 [INFO]: epoch 381: training loss 0.0324\n",
      "2025-05-29 21:02:02 [INFO]: epoch 382: training loss 0.0449\n",
      "2025-05-29 21:02:02 [INFO]: epoch 383: training loss 0.0448\n",
      "2025-05-29 21:02:02 [INFO]: epoch 384: training loss 0.0384\n",
      "2025-05-29 21:02:02 [INFO]: epoch 385: training loss 0.0409\n",
      "2025-05-29 21:02:02 [INFO]: epoch 386: training loss 0.0379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:02:02 [INFO]: epoch 387: training loss 0.0395\n",
      "2025-05-29 21:02:02 [INFO]: epoch 388: training loss 0.0367\n",
      "2025-05-29 21:02:02 [INFO]: epoch 389: training loss 0.0343\n",
      "2025-05-29 21:02:02 [INFO]: epoch 390: training loss 0.0409\n",
      "2025-05-29 21:02:02 [INFO]: epoch 391: training loss 0.0358\n",
      "2025-05-29 21:02:02 [INFO]: epoch 392: training loss 0.0408\n",
      "2025-05-29 21:02:03 [INFO]: epoch 393: training loss 0.0343\n",
      "2025-05-29 21:02:03 [INFO]: epoch 394: training loss 0.0386\n",
      "2025-05-29 21:02:03 [INFO]: epoch 395: training loss 0.0441\n",
      "2025-05-29 21:02:03 [INFO]: epoch 396: training loss 0.0373\n",
      "2025-05-29 21:02:03 [INFO]: epoch 397: training loss 0.0405\n",
      "2025-05-29 21:02:03 [INFO]: epoch 398: training loss 0.0460\n",
      "2025-05-29 21:02:03 [INFO]: epoch 399: training loss 0.0379\n",
      "2025-05-29 21:02:03 [INFO]: epoch 400: training loss 0.0321\n",
      "2025-05-29 21:02:03 [INFO]: epoch 401: training loss 0.0433\n",
      "2025-05-29 21:02:03 [INFO]: epoch 402: training loss 0.0374\n",
      "2025-05-29 21:02:03 [INFO]: epoch 403: training loss 0.0341\n",
      "2025-05-29 21:02:03 [INFO]: epoch 404: training loss 0.0378\n",
      "2025-05-29 21:02:03 [INFO]: epoch 405: training loss 0.0384\n",
      "2025-05-29 21:02:03 [INFO]: epoch 406: training loss 0.0390\n",
      "2025-05-29 21:02:03 [INFO]: epoch 407: training loss 0.0393\n",
      "2025-05-29 21:02:03 [INFO]: epoch 408: training loss 0.0384\n",
      "2025-05-29 21:02:03 [INFO]: epoch 409: training loss 0.0389\n",
      "2025-05-29 21:02:03 [INFO]: epoch 410: training loss 0.0333\n",
      "2025-05-29 21:02:03 [INFO]: epoch 411: training loss 0.0443\n",
      "2025-05-29 21:02:03 [INFO]: epoch 412: training loss 0.0353\n",
      "2025-05-29 21:02:03 [INFO]: epoch 413: training loss 0.0329\n",
      "2025-05-29 21:02:03 [INFO]: epoch 414: training loss 0.0393\n",
      "2025-05-29 21:02:03 [INFO]: epoch 415: training loss 0.0374\n",
      "2025-05-29 21:02:03 [INFO]: epoch 416: training loss 0.0335\n",
      "2025-05-29 21:02:03 [INFO]: epoch 417: training loss 0.0363\n",
      "2025-05-29 21:02:03 [INFO]: epoch 418: training loss 0.0285\n",
      "2025-05-29 21:02:03 [INFO]: epoch 419: training loss 0.0324\n",
      "2025-05-29 21:02:03 [INFO]: epoch 420: training loss 0.0353\n",
      "2025-05-29 21:02:03 [INFO]: epoch 421: training loss 0.0317\n",
      "2025-05-29 21:02:03 [INFO]: epoch 422: training loss 0.0307\n",
      "2025-05-29 21:02:03 [INFO]: epoch 423: training loss 0.0337\n",
      "2025-05-29 21:02:03 [INFO]: epoch 424: training loss 0.0407\n",
      "2025-05-29 21:02:03 [INFO]: epoch 425: training loss 0.0342\n",
      "2025-05-29 21:02:03 [INFO]: epoch 426: training loss 0.0378\n",
      "2025-05-29 21:02:03 [INFO]: epoch 427: training loss 0.0329\n",
      "2025-05-29 21:02:03 [INFO]: epoch 428: training loss 0.0346\n",
      "2025-05-29 21:02:03 [INFO]: epoch 429: training loss 0.0309\n",
      "2025-05-29 21:02:03 [INFO]: epoch 430: training loss 0.0390\n",
      "2025-05-29 21:02:03 [INFO]: epoch 431: training loss 0.0316\n",
      "2025-05-29 21:02:03 [INFO]: epoch 432: training loss 0.0311\n",
      "2025-05-29 21:02:03 [INFO]: epoch 433: training loss 0.0380\n",
      "2025-05-29 21:02:03 [INFO]: epoch 434: training loss 0.0301\n",
      "2025-05-29 21:02:03 [INFO]: epoch 435: training loss 0.0300\n",
      "2025-05-29 21:02:03 [INFO]: epoch 436: training loss 0.0306\n",
      "2025-05-29 21:02:03 [INFO]: epoch 437: training loss 0.0279\n",
      "2025-05-29 21:02:03 [INFO]: epoch 438: training loss 0.0354\n",
      "2025-05-29 21:02:03 [INFO]: epoch 439: training loss 0.0374\n",
      "2025-05-29 21:02:03 [INFO]: epoch 440: training loss 0.0282\n",
      "2025-05-29 21:02:03 [INFO]: epoch 441: training loss 0.0350\n",
      "2025-05-29 21:02:03 [INFO]: epoch 442: training loss 0.0447\n",
      "2025-05-29 21:02:03 [INFO]: epoch 443: training loss 0.0352\n",
      "2025-05-29 21:02:03 [INFO]: epoch 444: training loss 0.0342\n",
      "2025-05-29 21:02:03 [INFO]: epoch 445: training loss 0.0337\n",
      "2025-05-29 21:02:03 [INFO]: epoch 446: training loss 0.0321\n",
      "2025-05-29 21:02:03 [INFO]: epoch 447: training loss 0.0315\n",
      "2025-05-29 21:02:03 [INFO]: epoch 448: training loss 0.0370\n",
      "2025-05-29 21:02:03 [INFO]: epoch 449: training loss 0.0338\n",
      "2025-05-29 21:02:03 [INFO]: epoch 450: training loss 0.0358\n",
      "2025-05-29 21:02:03 [INFO]: epoch 451: training loss 0.0372\n",
      "2025-05-29 21:02:03 [INFO]: epoch 452: training loss 0.0421\n",
      "2025-05-29 21:02:03 [INFO]: epoch 453: training loss 0.0381\n",
      "2025-05-29 21:02:03 [INFO]: epoch 454: training loss 0.0416\n",
      "2025-05-29 21:02:03 [INFO]: epoch 455: training loss 0.0333\n",
      "2025-05-29 21:02:03 [INFO]: epoch 456: training loss 0.0379\n",
      "2025-05-29 21:02:03 [INFO]: epoch 457: training loss 0.0388\n",
      "2025-05-29 21:02:03 [INFO]: epoch 458: training loss 0.0339\n",
      "2025-05-29 21:02:03 [INFO]: epoch 459: training loss 0.0383\n",
      "2025-05-29 21:02:03 [INFO]: epoch 460: training loss 0.0397\n",
      "2025-05-29 21:02:03 [INFO]: epoch 461: training loss 0.0324\n",
      "2025-05-29 21:02:03 [INFO]: epoch 462: training loss 0.0349\n",
      "2025-05-29 21:02:03 [INFO]: epoch 463: training loss 0.0321\n",
      "2025-05-29 21:02:03 [INFO]: epoch 464: training loss 0.0415\n",
      "2025-05-29 21:02:03 [INFO]: epoch 465: training loss 0.0372\n",
      "2025-05-29 21:02:03 [INFO]: epoch 466: training loss 0.0299\n",
      "2025-05-29 21:02:03 [INFO]: epoch 467: training loss 0.0351\n",
      "2025-05-29 21:02:03 [INFO]: epoch 468: training loss 0.0442\n",
      "2025-05-29 21:02:03 [INFO]: epoch 469: training loss 0.0358\n",
      "2025-05-29 21:02:03 [INFO]: epoch 470: training loss 0.0375\n",
      "2025-05-29 21:02:03 [INFO]: epoch 471: training loss 0.0299\n",
      "2025-05-29 21:02:03 [INFO]: epoch 472: training loss 0.0357\n",
      "2025-05-29 21:02:03 [INFO]: epoch 473: training loss 0.0384\n",
      "2025-05-29 21:02:04 [INFO]: epoch 474: training loss 0.0306\n",
      "2025-05-29 21:02:04 [INFO]: epoch 475: training loss 0.0378\n",
      "2025-05-29 21:02:04 [INFO]: epoch 476: training loss 0.0341\n",
      "2025-05-29 21:02:04 [INFO]: epoch 477: training loss 0.0367\n",
      "2025-05-29 21:02:04 [INFO]: epoch 478: training loss 0.0315\n",
      "2025-05-29 21:02:04 [INFO]: epoch 479: training loss 0.0256\n",
      "2025-05-29 21:02:04 [INFO]: epoch 480: training loss 0.0302\n",
      "2025-05-29 21:02:04 [INFO]: epoch 481: training loss 0.0332\n",
      "2025-05-29 21:02:04 [INFO]: epoch 482: training loss 0.0348\n",
      "2025-05-29 21:02:04 [INFO]: epoch 483: training loss 0.0314\n",
      "2025-05-29 21:02:04 [INFO]: epoch 484: training loss 0.0383\n",
      "2025-05-29 21:02:04 [INFO]: epoch 485: training loss 0.0333\n",
      "2025-05-29 21:02:04 [INFO]: epoch 486: training loss 0.0337\n",
      "2025-05-29 21:02:04 [INFO]: epoch 487: training loss 0.0330\n",
      "2025-05-29 21:02:04 [INFO]: epoch 488: training loss 0.0343\n",
      "2025-05-29 21:02:04 [INFO]: epoch 489: training loss 0.0316\n",
      "2025-05-29 21:02:04 [INFO]: epoch 490: training loss 0.0325\n",
      "2025-05-29 21:02:04 [INFO]: epoch 491: training loss 0.0371\n",
      "2025-05-29 21:02:04 [INFO]: epoch 492: training loss 0.0345\n",
      "2025-05-29 21:02:04 [INFO]: epoch 493: training loss 0.0305\n",
      "2025-05-29 21:02:04 [INFO]: epoch 494: training loss 0.0346\n",
      "2025-05-29 21:02:04 [INFO]: epoch 495: training loss 0.0320\n",
      "2025-05-29 21:02:04 [INFO]: epoch 496: training loss 0.0312\n",
      "2025-05-29 21:02:04 [INFO]: epoch 497: training loss 0.0359\n",
      "2025-05-29 21:02:04 [INFO]: epoch 498: training loss 0.0376\n",
      "2025-05-29 21:02:04 [INFO]: epoch 499: training loss 0.0241\n",
      "2025-05-29 21:02:04 [INFO]: epoch 500: training loss 0.0318\n",
      "2025-05-29 21:02:04 [INFO]: epoch 501: training loss 0.0364\n",
      "2025-05-29 21:02:04 [INFO]: epoch 502: training loss 0.0289\n",
      "2025-05-29 21:02:04 [INFO]: epoch 503: training loss 0.0275\n",
      "2025-05-29 21:02:04 [INFO]: epoch 504: training loss 0.0411\n",
      "2025-05-29 21:02:04 [INFO]: epoch 505: training loss 0.0271\n",
      "2025-05-29 21:02:04 [INFO]: epoch 506: training loss 0.0308\n",
      "2025-05-29 21:02:04 [INFO]: epoch 507: training loss 0.0332\n",
      "2025-05-29 21:02:04 [INFO]: epoch 508: training loss 0.0295\n",
      "2025-05-29 21:02:04 [INFO]: epoch 509: training loss 0.0252\n",
      "2025-05-29 21:02:04 [INFO]: epoch 510: training loss 0.0324\n",
      "2025-05-29 21:02:04 [INFO]: epoch 511: training loss 0.0302\n",
      "2025-05-29 21:02:04 [INFO]: epoch 512: training loss 0.0350\n",
      "2025-05-29 21:02:04 [INFO]: epoch 513: training loss 0.0283\n",
      "2025-05-29 21:02:04 [INFO]: epoch 514: training loss 0.0287\n",
      "2025-05-29 21:02:04 [INFO]: epoch 515: training loss 0.0245\n",
      "2025-05-29 21:02:04 [INFO]: epoch 516: training loss 0.0276\n",
      "2025-05-29 21:02:04 [INFO]: epoch 517: training loss 0.0345\n",
      "2025-05-29 21:02:04 [INFO]: epoch 518: training loss 0.0259\n",
      "2025-05-29 21:02:04 [INFO]: epoch 519: training loss 0.0317\n",
      "2025-05-29 21:02:04 [INFO]: epoch 520: training loss 0.0322\n",
      "2025-05-29 21:02:04 [INFO]: epoch 521: training loss 0.0279\n",
      "2025-05-29 21:02:04 [INFO]: epoch 522: training loss 0.0330\n",
      "2025-05-29 21:02:04 [INFO]: epoch 523: training loss 0.0245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:02:04 [INFO]: epoch 524: training loss 0.0300\n",
      "2025-05-29 21:02:04 [INFO]: epoch 525: training loss 0.0271\n",
      "2025-05-29 21:02:04 [INFO]: epoch 526: training loss 0.0287\n",
      "2025-05-29 21:02:04 [INFO]: epoch 527: training loss 0.0326\n",
      "2025-05-29 21:02:04 [INFO]: epoch 528: training loss 0.0343\n",
      "2025-05-29 21:02:04 [INFO]: epoch 529: training loss 0.0272\n",
      "2025-05-29 21:02:04 [INFO]: epoch 530: training loss 0.0369\n",
      "2025-05-29 21:02:04 [INFO]: epoch 531: training loss 0.0393\n",
      "2025-05-29 21:02:04 [INFO]: epoch 532: training loss 0.0268\n",
      "2025-05-29 21:02:04 [INFO]: epoch 533: training loss 0.0328\n",
      "2025-05-29 21:02:04 [INFO]: epoch 534: training loss 0.0386\n",
      "2025-05-29 21:02:04 [INFO]: epoch 535: training loss 0.0273\n",
      "2025-05-29 21:02:04 [INFO]: epoch 536: training loss 0.0307\n",
      "2025-05-29 21:02:04 [INFO]: epoch 537: training loss 0.0290\n",
      "2025-05-29 21:02:04 [INFO]: epoch 538: training loss 0.0285\n",
      "2025-05-29 21:02:04 [INFO]: epoch 539: training loss 0.0261\n",
      "2025-05-29 21:02:04 [INFO]: epoch 540: training loss 0.0275\n",
      "2025-05-29 21:02:04 [INFO]: epoch 541: training loss 0.0318\n",
      "2025-05-29 21:02:04 [INFO]: epoch 542: training loss 0.0316\n",
      "2025-05-29 21:02:04 [INFO]: epoch 543: training loss 0.0222\n",
      "2025-05-29 21:02:04 [INFO]: epoch 544: training loss 0.0364\n",
      "2025-05-29 21:02:04 [INFO]: epoch 545: training loss 0.0350\n",
      "2025-05-29 21:02:04 [INFO]: epoch 546: training loss 0.0296\n",
      "2025-05-29 21:02:04 [INFO]: epoch 547: training loss 0.0315\n",
      "2025-05-29 21:02:04 [INFO]: epoch 548: training loss 0.0329\n",
      "2025-05-29 21:02:04 [INFO]: epoch 549: training loss 0.0277\n",
      "2025-05-29 21:02:04 [INFO]: epoch 550: training loss 0.0287\n",
      "2025-05-29 21:02:04 [INFO]: epoch 551: training loss 0.0312\n",
      "2025-05-29 21:02:04 [INFO]: epoch 552: training loss 0.0273\n",
      "2025-05-29 21:02:04 [INFO]: epoch 553: training loss 0.0247\n",
      "2025-05-29 21:02:05 [INFO]: epoch 554: training loss 0.0257\n",
      "2025-05-29 21:02:05 [INFO]: epoch 555: training loss 0.0273\n",
      "2025-05-29 21:02:05 [INFO]: epoch 556: training loss 0.0239\n",
      "2025-05-29 21:02:05 [INFO]: epoch 557: training loss 0.0252\n",
      "2025-05-29 21:02:05 [INFO]: epoch 558: training loss 0.0288\n",
      "2025-05-29 21:02:05 [INFO]: epoch 559: training loss 0.0373\n",
      "2025-05-29 21:02:05 [INFO]: epoch 560: training loss 0.0356\n",
      "2025-05-29 21:02:05 [INFO]: epoch 561: training loss 0.0252\n",
      "2025-05-29 21:02:05 [INFO]: epoch 562: training loss 0.0315\n",
      "2025-05-29 21:02:05 [INFO]: epoch 563: training loss 0.0373\n",
      "2025-05-29 21:02:05 [INFO]: epoch 564: training loss 0.0376\n",
      "2025-05-29 21:02:05 [INFO]: epoch 565: training loss 0.0282\n",
      "2025-05-29 21:02:05 [INFO]: epoch 566: training loss 0.0342\n",
      "2025-05-29 21:02:05 [INFO]: epoch 567: training loss 0.0316\n",
      "2025-05-29 21:02:05 [INFO]: epoch 568: training loss 0.0323\n",
      "2025-05-29 21:02:05 [INFO]: epoch 569: training loss 0.0386\n",
      "2025-05-29 21:02:05 [INFO]: epoch 570: training loss 0.0380\n",
      "2025-05-29 21:02:05 [INFO]: epoch 571: training loss 0.0360\n",
      "2025-05-29 21:02:05 [INFO]: epoch 572: training loss 0.0408\n",
      "2025-05-29 21:02:05 [INFO]: epoch 573: training loss 0.0416\n",
      "2025-05-29 21:02:05 [INFO]: epoch 574: training loss 0.0330\n",
      "2025-05-29 21:02:05 [INFO]: epoch 575: training loss 0.0338\n",
      "2025-05-29 21:02:05 [INFO]: epoch 576: training loss 0.0353\n",
      "2025-05-29 21:02:05 [INFO]: epoch 577: training loss 0.0302\n",
      "2025-05-29 21:02:05 [INFO]: epoch 578: training loss 0.0326\n",
      "2025-05-29 21:02:05 [INFO]: epoch 579: training loss 0.0340\n",
      "2025-05-29 21:02:05 [INFO]: epoch 580: training loss 0.0329\n",
      "2025-05-29 21:02:05 [INFO]: epoch 581: training loss 0.0303\n",
      "2025-05-29 21:02:05 [INFO]: epoch 582: training loss 0.0327\n",
      "2025-05-29 21:02:05 [INFO]: epoch 583: training loss 0.0263\n",
      "2025-05-29 21:02:05 [INFO]: epoch 584: training loss 0.0244\n",
      "2025-05-29 21:02:05 [INFO]: epoch 585: training loss 0.0278\n",
      "2025-05-29 21:02:05 [INFO]: epoch 586: training loss 0.0239\n",
      "2025-05-29 21:02:05 [INFO]: epoch 587: training loss 0.0244\n",
      "2025-05-29 21:02:05 [INFO]: epoch 588: training loss 0.0280\n",
      "2025-05-29 21:02:05 [INFO]: epoch 589: training loss 0.0278\n",
      "2025-05-29 21:02:05 [INFO]: epoch 590: training loss 0.0288\n",
      "2025-05-29 21:02:05 [INFO]: epoch 591: training loss 0.0243\n",
      "2025-05-29 21:02:05 [INFO]: epoch 592: training loss 0.0276\n",
      "2025-05-29 21:02:05 [INFO]: epoch 593: training loss 0.0243\n",
      "2025-05-29 21:02:05 [INFO]: epoch 594: training loss 0.0239\n",
      "2025-05-29 21:02:05 [INFO]: epoch 595: training loss 0.0272\n",
      "2025-05-29 21:02:05 [INFO]: epoch 596: training loss 0.0272\n",
      "2025-05-29 21:02:05 [INFO]: epoch 597: training loss 0.0250\n",
      "2025-05-29 21:02:05 [INFO]: epoch 598: training loss 0.0311\n",
      "2025-05-29 21:02:05 [INFO]: epoch 599: training loss 0.0218\n",
      "2025-05-29 21:02:05 [INFO]: Finished training.\n",
      "2025-05-29 21:02:05 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:22<00:15,  7.54s/it]2025-05-29 21:02:05 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:02:05 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:02:05 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:02:05 [INFO]: epoch 0: training loss 1.0150\n",
      "2025-05-29 21:02:05 [INFO]: epoch 1: training loss 0.5980\n",
      "2025-05-29 21:02:05 [INFO]: epoch 2: training loss 0.6502\n",
      "2025-05-29 21:02:05 [INFO]: epoch 3: training loss 0.6027\n",
      "2025-05-29 21:02:05 [INFO]: epoch 4: training loss 0.5256\n",
      "2025-05-29 21:02:05 [INFO]: epoch 5: training loss 0.5020\n",
      "2025-05-29 21:02:05 [INFO]: epoch 6: training loss 0.5005\n",
      "2025-05-29 21:02:05 [INFO]: epoch 7: training loss 0.4171\n",
      "2025-05-29 21:02:05 [INFO]: epoch 8: training loss 0.4301\n",
      "2025-05-29 21:02:05 [INFO]: epoch 9: training loss 0.4033\n",
      "2025-05-29 21:02:05 [INFO]: epoch 10: training loss 0.4201\n",
      "2025-05-29 21:02:05 [INFO]: epoch 11: training loss 0.4156\n",
      "2025-05-29 21:02:05 [INFO]: epoch 12: training loss 0.4142\n",
      "2025-05-29 21:02:05 [INFO]: epoch 13: training loss 0.3546\n",
      "2025-05-29 21:02:05 [INFO]: epoch 14: training loss 0.3667\n",
      "2025-05-29 21:02:05 [INFO]: epoch 15: training loss 0.3684\n",
      "2025-05-29 21:02:05 [INFO]: epoch 16: training loss 0.3507\n",
      "2025-05-29 21:02:05 [INFO]: epoch 17: training loss 0.2942\n",
      "2025-05-29 21:02:05 [INFO]: epoch 18: training loss 0.2919\n",
      "2025-05-29 21:02:05 [INFO]: epoch 19: training loss 0.3198\n",
      "2025-05-29 21:02:05 [INFO]: epoch 20: training loss 0.3127\n",
      "2025-05-29 21:02:05 [INFO]: epoch 21: training loss 0.3032\n",
      "2025-05-29 21:02:05 [INFO]: epoch 22: training loss 0.2792\n",
      "2025-05-29 21:02:05 [INFO]: epoch 23: training loss 0.2826\n",
      "2025-05-29 21:02:05 [INFO]: epoch 24: training loss 0.2844\n",
      "2025-05-29 21:02:05 [INFO]: epoch 25: training loss 0.2941\n",
      "2025-05-29 21:02:05 [INFO]: epoch 26: training loss 0.3147\n",
      "2025-05-29 21:02:06 [INFO]: epoch 27: training loss 0.2648\n",
      "2025-05-29 21:02:06 [INFO]: epoch 28: training loss 0.2929\n",
      "2025-05-29 21:02:06 [INFO]: epoch 29: training loss 0.2903\n",
      "2025-05-29 21:02:06 [INFO]: epoch 30: training loss 0.3012\n",
      "2025-05-29 21:02:06 [INFO]: epoch 31: training loss 0.2754\n",
      "2025-05-29 21:02:06 [INFO]: epoch 32: training loss 0.3065\n",
      "2025-05-29 21:02:06 [INFO]: epoch 33: training loss 0.2675\n",
      "2025-05-29 21:02:06 [INFO]: epoch 34: training loss 0.2581\n",
      "2025-05-29 21:02:06 [INFO]: epoch 35: training loss 0.2910\n",
      "2025-05-29 21:02:06 [INFO]: epoch 36: training loss 0.2842\n",
      "2025-05-29 21:02:06 [INFO]: epoch 37: training loss 0.2606\n",
      "2025-05-29 21:02:06 [INFO]: epoch 38: training loss 0.2439\n",
      "2025-05-29 21:02:06 [INFO]: epoch 39: training loss 0.2784\n",
      "2025-05-29 21:02:06 [INFO]: epoch 40: training loss 0.2658\n",
      "2025-05-29 21:02:06 [INFO]: epoch 41: training loss 0.2560\n",
      "2025-05-29 21:02:06 [INFO]: epoch 42: training loss 0.2445\n",
      "2025-05-29 21:02:06 [INFO]: epoch 43: training loss 0.2669\n",
      "2025-05-29 21:02:06 [INFO]: epoch 44: training loss 0.2536\n",
      "2025-05-29 21:02:06 [INFO]: epoch 45: training loss 0.2552\n",
      "2025-05-29 21:02:06 [INFO]: epoch 46: training loss 0.2225\n",
      "2025-05-29 21:02:06 [INFO]: epoch 47: training loss 0.2394\n",
      "2025-05-29 21:02:06 [INFO]: epoch 48: training loss 0.2274\n",
      "2025-05-29 21:02:06 [INFO]: epoch 49: training loss 0.2363\n",
      "2025-05-29 21:02:06 [INFO]: epoch 50: training loss 0.2227\n",
      "2025-05-29 21:02:06 [INFO]: epoch 51: training loss 0.2279\n",
      "2025-05-29 21:02:06 [INFO]: epoch 52: training loss 0.2101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:02:06 [INFO]: epoch 53: training loss 0.2261\n",
      "2025-05-29 21:02:06 [INFO]: epoch 54: training loss 0.2208\n",
      "2025-05-29 21:02:06 [INFO]: epoch 55: training loss 0.2208\n",
      "2025-05-29 21:02:06 [INFO]: epoch 56: training loss 0.1962\n",
      "2025-05-29 21:02:06 [INFO]: epoch 57: training loss 0.2122\n",
      "2025-05-29 21:02:06 [INFO]: epoch 58: training loss 0.2070\n",
      "2025-05-29 21:02:06 [INFO]: epoch 59: training loss 0.1901\n",
      "2025-05-29 21:02:06 [INFO]: epoch 60: training loss 0.1918\n",
      "2025-05-29 21:02:06 [INFO]: epoch 61: training loss 0.1914\n",
      "2025-05-29 21:02:06 [INFO]: epoch 62: training loss 0.1900\n",
      "2025-05-29 21:02:06 [INFO]: epoch 63: training loss 0.1771\n",
      "2025-05-29 21:02:06 [INFO]: epoch 64: training loss 0.1738\n",
      "2025-05-29 21:02:06 [INFO]: epoch 65: training loss 0.1838\n",
      "2025-05-29 21:02:06 [INFO]: epoch 66: training loss 0.1804\n",
      "2025-05-29 21:02:06 [INFO]: epoch 67: training loss 0.1635\n",
      "2025-05-29 21:02:06 [INFO]: epoch 68: training loss 0.1703\n",
      "2025-05-29 21:02:06 [INFO]: epoch 69: training loss 0.1954\n",
      "2025-05-29 21:02:06 [INFO]: epoch 70: training loss 0.1808\n",
      "2025-05-29 21:02:06 [INFO]: epoch 71: training loss 0.1710\n",
      "2025-05-29 21:02:06 [INFO]: epoch 72: training loss 0.1800\n",
      "2025-05-29 21:02:06 [INFO]: epoch 73: training loss 0.1815\n",
      "2025-05-29 21:02:06 [INFO]: epoch 74: training loss 0.1734\n",
      "2025-05-29 21:02:06 [INFO]: epoch 75: training loss 0.1732\n",
      "2025-05-29 21:02:06 [INFO]: epoch 76: training loss 0.1711\n",
      "2025-05-29 21:02:06 [INFO]: epoch 77: training loss 0.1589\n",
      "2025-05-29 21:02:06 [INFO]: epoch 78: training loss 0.1718\n",
      "2025-05-29 21:02:06 [INFO]: epoch 79: training loss 0.1886\n",
      "2025-05-29 21:02:06 [INFO]: epoch 80: training loss 0.1893\n",
      "2025-05-29 21:02:06 [INFO]: epoch 81: training loss 0.1685\n",
      "2025-05-29 21:02:06 [INFO]: epoch 82: training loss 0.1806\n",
      "2025-05-29 21:02:06 [INFO]: epoch 83: training loss 0.1771\n",
      "2025-05-29 21:02:06 [INFO]: epoch 84: training loss 0.1869\n",
      "2025-05-29 21:02:06 [INFO]: epoch 85: training loss 0.1761\n",
      "2025-05-29 21:02:06 [INFO]: epoch 86: training loss 0.1485\n",
      "2025-05-29 21:02:06 [INFO]: epoch 87: training loss 0.1804\n",
      "2025-05-29 21:02:06 [INFO]: epoch 88: training loss 0.1740\n",
      "2025-05-29 21:02:06 [INFO]: epoch 89: training loss 0.1543\n",
      "2025-05-29 21:02:06 [INFO]: epoch 90: training loss 0.1565\n",
      "2025-05-29 21:02:06 [INFO]: epoch 91: training loss 0.1771\n",
      "2025-05-29 21:02:06 [INFO]: epoch 92: training loss 0.1726\n",
      "2025-05-29 21:02:06 [INFO]: epoch 93: training loss 0.1470\n",
      "2025-05-29 21:02:06 [INFO]: epoch 94: training loss 0.1419\n",
      "2025-05-29 21:02:06 [INFO]: epoch 95: training loss 0.1609\n",
      "2025-05-29 21:02:06 [INFO]: epoch 96: training loss 0.1441\n",
      "2025-05-29 21:02:06 [INFO]: epoch 97: training loss 0.1702\n",
      "2025-05-29 21:02:06 [INFO]: epoch 98: training loss 0.1487\n",
      "2025-05-29 21:02:06 [INFO]: epoch 99: training loss 0.1275\n",
      "2025-05-29 21:02:06 [INFO]: epoch 100: training loss 0.1431\n",
      "2025-05-29 21:02:06 [INFO]: epoch 101: training loss 0.1389\n",
      "2025-05-29 21:02:06 [INFO]: epoch 102: training loss 0.1407\n",
      "2025-05-29 21:02:06 [INFO]: epoch 103: training loss 0.1412\n",
      "2025-05-29 21:02:06 [INFO]: epoch 104: training loss 0.1416\n",
      "2025-05-29 21:02:06 [INFO]: epoch 105: training loss 0.1374\n",
      "2025-05-29 21:02:06 [INFO]: epoch 106: training loss 0.1433\n",
      "2025-05-29 21:02:07 [INFO]: epoch 107: training loss 0.1359\n",
      "2025-05-29 21:02:07 [INFO]: epoch 108: training loss 0.1325\n",
      "2025-05-29 21:02:07 [INFO]: epoch 109: training loss 0.1277\n",
      "2025-05-29 21:02:07 [INFO]: epoch 110: training loss 0.1456\n",
      "2025-05-29 21:02:07 [INFO]: epoch 111: training loss 0.1329\n",
      "2025-05-29 21:02:07 [INFO]: epoch 112: training loss 0.1298\n",
      "2025-05-29 21:02:07 [INFO]: epoch 113: training loss 0.1260\n",
      "2025-05-29 21:02:07 [INFO]: epoch 114: training loss 0.1408\n",
      "2025-05-29 21:02:07 [INFO]: epoch 115: training loss 0.1403\n",
      "2025-05-29 21:02:07 [INFO]: epoch 116: training loss 0.1191\n",
      "2025-05-29 21:02:07 [INFO]: epoch 117: training loss 0.1338\n",
      "2025-05-29 21:02:07 [INFO]: epoch 118: training loss 0.1288\n",
      "2025-05-29 21:02:07 [INFO]: epoch 119: training loss 0.1414\n",
      "2025-05-29 21:02:07 [INFO]: epoch 120: training loss 0.1270\n",
      "2025-05-29 21:02:07 [INFO]: epoch 121: training loss 0.1287\n",
      "2025-05-29 21:02:07 [INFO]: epoch 122: training loss 0.1286\n",
      "2025-05-29 21:02:07 [INFO]: epoch 123: training loss 0.1045\n",
      "2025-05-29 21:02:07 [INFO]: epoch 124: training loss 0.1037\n",
      "2025-05-29 21:02:07 [INFO]: epoch 125: training loss 0.1171\n",
      "2025-05-29 21:02:07 [INFO]: epoch 126: training loss 0.1018\n",
      "2025-05-29 21:02:07 [INFO]: epoch 127: training loss 0.1051\n",
      "2025-05-29 21:02:07 [INFO]: epoch 128: training loss 0.1208\n",
      "2025-05-29 21:02:07 [INFO]: epoch 129: training loss 0.1121\n",
      "2025-05-29 21:02:07 [INFO]: epoch 130: training loss 0.1000\n",
      "2025-05-29 21:02:07 [INFO]: epoch 131: training loss 0.0970\n",
      "2025-05-29 21:02:07 [INFO]: epoch 132: training loss 0.1109\n",
      "2025-05-29 21:02:07 [INFO]: epoch 133: training loss 0.1197\n",
      "2025-05-29 21:02:07 [INFO]: epoch 134: training loss 0.1038\n",
      "2025-05-29 21:02:07 [INFO]: epoch 135: training loss 0.1099\n",
      "2025-05-29 21:02:07 [INFO]: epoch 136: training loss 0.0962\n",
      "2025-05-29 21:02:07 [INFO]: epoch 137: training loss 0.1148\n",
      "2025-05-29 21:02:07 [INFO]: epoch 138: training loss 0.1172\n",
      "2025-05-29 21:02:07 [INFO]: epoch 139: training loss 0.1129\n",
      "2025-05-29 21:02:07 [INFO]: epoch 140: training loss 0.1072\n",
      "2025-05-29 21:02:07 [INFO]: epoch 141: training loss 0.1126\n",
      "2025-05-29 21:02:07 [INFO]: epoch 142: training loss 0.1014\n",
      "2025-05-29 21:02:07 [INFO]: epoch 143: training loss 0.1121\n",
      "2025-05-29 21:02:07 [INFO]: epoch 144: training loss 0.1101\n",
      "2025-05-29 21:02:07 [INFO]: epoch 145: training loss 0.0974\n",
      "2025-05-29 21:02:07 [INFO]: epoch 146: training loss 0.1075\n",
      "2025-05-29 21:02:07 [INFO]: epoch 147: training loss 0.0998\n",
      "2025-05-29 21:02:07 [INFO]: epoch 148: training loss 0.0903\n",
      "2025-05-29 21:02:07 [INFO]: epoch 149: training loss 0.0982\n",
      "2025-05-29 21:02:07 [INFO]: epoch 150: training loss 0.1117\n",
      "2025-05-29 21:02:07 [INFO]: epoch 151: training loss 0.1034\n",
      "2025-05-29 21:02:07 [INFO]: epoch 152: training loss 0.1136\n",
      "2025-05-29 21:02:07 [INFO]: epoch 153: training loss 0.0983\n",
      "2025-05-29 21:02:07 [INFO]: epoch 154: training loss 0.0907\n",
      "2025-05-29 21:02:07 [INFO]: epoch 155: training loss 0.1124\n",
      "2025-05-29 21:02:07 [INFO]: epoch 156: training loss 0.1181\n",
      "2025-05-29 21:02:07 [INFO]: epoch 157: training loss 0.0919\n",
      "2025-05-29 21:02:07 [INFO]: epoch 158: training loss 0.0848\n",
      "2025-05-29 21:02:07 [INFO]: epoch 159: training loss 0.0943\n",
      "2025-05-29 21:02:07 [INFO]: epoch 160: training loss 0.0903\n",
      "2025-05-29 21:02:07 [INFO]: epoch 161: training loss 0.0870\n",
      "2025-05-29 21:02:07 [INFO]: epoch 162: training loss 0.0865\n",
      "2025-05-29 21:02:07 [INFO]: epoch 163: training loss 0.0779\n",
      "2025-05-29 21:02:07 [INFO]: epoch 164: training loss 0.0960\n",
      "2025-05-29 21:02:07 [INFO]: epoch 165: training loss 0.0804\n",
      "2025-05-29 21:02:07 [INFO]: epoch 166: training loss 0.0808\n",
      "2025-05-29 21:02:07 [INFO]: epoch 167: training loss 0.0860\n",
      "2025-05-29 21:02:07 [INFO]: epoch 168: training loss 0.0992\n",
      "2025-05-29 21:02:07 [INFO]: epoch 169: training loss 0.0763\n",
      "2025-05-29 21:02:07 [INFO]: epoch 170: training loss 0.0868\n",
      "2025-05-29 21:02:07 [INFO]: epoch 171: training loss 0.0893\n",
      "2025-05-29 21:02:07 [INFO]: epoch 172: training loss 0.0794\n",
      "2025-05-29 21:02:07 [INFO]: epoch 173: training loss 0.0786\n",
      "2025-05-29 21:02:07 [INFO]: epoch 174: training loss 0.0900\n",
      "2025-05-29 21:02:07 [INFO]: epoch 175: training loss 0.0839\n",
      "2025-05-29 21:02:07 [INFO]: epoch 176: training loss 0.0904\n",
      "2025-05-29 21:02:07 [INFO]: epoch 177: training loss 0.0879\n",
      "2025-05-29 21:02:07 [INFO]: epoch 178: training loss 0.0727\n",
      "2025-05-29 21:02:07 [INFO]: epoch 179: training loss 0.0737\n",
      "2025-05-29 21:02:07 [INFO]: epoch 180: training loss 0.0854\n",
      "2025-05-29 21:02:07 [INFO]: epoch 181: training loss 0.0828\n",
      "2025-05-29 21:02:07 [INFO]: epoch 182: training loss 0.0599\n",
      "2025-05-29 21:02:07 [INFO]: epoch 183: training loss 0.0753\n",
      "2025-05-29 21:02:07 [INFO]: epoch 184: training loss 0.0703\n",
      "2025-05-29 21:02:07 [INFO]: epoch 185: training loss 0.0783\n",
      "2025-05-29 21:02:07 [INFO]: epoch 186: training loss 0.0745\n",
      "2025-05-29 21:02:07 [INFO]: epoch 187: training loss 0.0815\n",
      "2025-05-29 21:02:08 [INFO]: epoch 188: training loss 0.0862\n",
      "2025-05-29 21:02:08 [INFO]: epoch 189: training loss 0.0776\n",
      "2025-05-29 21:02:08 [INFO]: epoch 190: training loss 0.0763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:02:08 [INFO]: epoch 191: training loss 0.0877\n",
      "2025-05-29 21:02:08 [INFO]: epoch 192: training loss 0.0873\n",
      "2025-05-29 21:02:08 [INFO]: epoch 193: training loss 0.0834\n",
      "2025-05-29 21:02:08 [INFO]: epoch 194: training loss 0.0704\n",
      "2025-05-29 21:02:08 [INFO]: epoch 195: training loss 0.0798\n",
      "2025-05-29 21:02:08 [INFO]: epoch 196: training loss 0.0690\n",
      "2025-05-29 21:02:08 [INFO]: epoch 197: training loss 0.0616\n",
      "2025-05-29 21:02:08 [INFO]: epoch 198: training loss 0.0789\n",
      "2025-05-29 21:02:08 [INFO]: epoch 199: training loss 0.0874\n",
      "2025-05-29 21:02:08 [INFO]: epoch 200: training loss 0.0719\n",
      "2025-05-29 21:02:08 [INFO]: epoch 201: training loss 0.0719\n",
      "2025-05-29 21:02:08 [INFO]: epoch 202: training loss 0.0878\n",
      "2025-05-29 21:02:08 [INFO]: epoch 203: training loss 0.0822\n",
      "2025-05-29 21:02:08 [INFO]: epoch 204: training loss 0.0660\n",
      "2025-05-29 21:02:08 [INFO]: epoch 205: training loss 0.0856\n",
      "2025-05-29 21:02:08 [INFO]: epoch 206: training loss 0.0769\n",
      "2025-05-29 21:02:08 [INFO]: epoch 207: training loss 0.0590\n",
      "2025-05-29 21:02:08 [INFO]: epoch 208: training loss 0.0714\n",
      "2025-05-29 21:02:08 [INFO]: epoch 209: training loss 0.0720\n",
      "2025-05-29 21:02:08 [INFO]: epoch 210: training loss 0.0686\n",
      "2025-05-29 21:02:08 [INFO]: epoch 211: training loss 0.0695\n",
      "2025-05-29 21:02:08 [INFO]: epoch 212: training loss 0.0736\n",
      "2025-05-29 21:02:08 [INFO]: epoch 213: training loss 0.0609\n",
      "2025-05-29 21:02:08 [INFO]: epoch 214: training loss 0.0665\n",
      "2025-05-29 21:02:08 [INFO]: epoch 215: training loss 0.0812\n",
      "2025-05-29 21:02:08 [INFO]: epoch 216: training loss 0.0645\n",
      "2025-05-29 21:02:08 [INFO]: epoch 217: training loss 0.0581\n",
      "2025-05-29 21:02:08 [INFO]: epoch 218: training loss 0.0736\n",
      "2025-05-29 21:02:08 [INFO]: epoch 219: training loss 0.0562\n",
      "2025-05-29 21:02:08 [INFO]: epoch 220: training loss 0.0601\n",
      "2025-05-29 21:02:08 [INFO]: epoch 221: training loss 0.0710\n",
      "2025-05-29 21:02:08 [INFO]: epoch 222: training loss 0.0650\n",
      "2025-05-29 21:02:08 [INFO]: epoch 223: training loss 0.0640\n",
      "2025-05-29 21:02:08 [INFO]: epoch 224: training loss 0.0614\n",
      "2025-05-29 21:02:08 [INFO]: epoch 225: training loss 0.0655\n",
      "2025-05-29 21:02:08 [INFO]: epoch 226: training loss 0.0607\n",
      "2025-05-29 21:02:08 [INFO]: epoch 227: training loss 0.0691\n",
      "2025-05-29 21:02:08 [INFO]: epoch 228: training loss 0.0666\n",
      "2025-05-29 21:02:08 [INFO]: epoch 229: training loss 0.0705\n",
      "2025-05-29 21:02:08 [INFO]: epoch 230: training loss 0.0697\n",
      "2025-05-29 21:02:08 [INFO]: epoch 231: training loss 0.0690\n",
      "2025-05-29 21:02:08 [INFO]: epoch 232: training loss 0.0587\n",
      "2025-05-29 21:02:08 [INFO]: epoch 233: training loss 0.0625\n",
      "2025-05-29 21:02:08 [INFO]: epoch 234: training loss 0.0706\n",
      "2025-05-29 21:02:08 [INFO]: epoch 235: training loss 0.0569\n",
      "2025-05-29 21:02:08 [INFO]: epoch 236: training loss 0.0593\n",
      "2025-05-29 21:02:08 [INFO]: epoch 237: training loss 0.0650\n",
      "2025-05-29 21:02:08 [INFO]: epoch 238: training loss 0.0565\n",
      "2025-05-29 21:02:08 [INFO]: epoch 239: training loss 0.0562\n",
      "2025-05-29 21:02:08 [INFO]: epoch 240: training loss 0.0569\n",
      "2025-05-29 21:02:08 [INFO]: epoch 241: training loss 0.0567\n",
      "2025-05-29 21:02:08 [INFO]: epoch 242: training loss 0.0480\n",
      "2025-05-29 21:02:08 [INFO]: epoch 243: training loss 0.0563\n",
      "2025-05-29 21:02:08 [INFO]: epoch 244: training loss 0.0444\n",
      "2025-05-29 21:02:08 [INFO]: epoch 245: training loss 0.0634\n",
      "2025-05-29 21:02:08 [INFO]: epoch 246: training loss 0.0606\n",
      "2025-05-29 21:02:08 [INFO]: epoch 247: training loss 0.0636\n",
      "2025-05-29 21:02:08 [INFO]: epoch 248: training loss 0.0628\n",
      "2025-05-29 21:02:08 [INFO]: epoch 249: training loss 0.0553\n",
      "2025-05-29 21:02:08 [INFO]: epoch 250: training loss 0.0728\n",
      "2025-05-29 21:02:08 [INFO]: epoch 251: training loss 0.0701\n",
      "2025-05-29 21:02:08 [INFO]: epoch 252: training loss 0.0612\n",
      "2025-05-29 21:02:08 [INFO]: epoch 253: training loss 0.0574\n",
      "2025-05-29 21:02:08 [INFO]: epoch 254: training loss 0.0695\n",
      "2025-05-29 21:02:08 [INFO]: epoch 255: training loss 0.0531\n",
      "2025-05-29 21:02:08 [INFO]: epoch 256: training loss 0.0699\n",
      "2025-05-29 21:02:08 [INFO]: epoch 257: training loss 0.0678\n",
      "2025-05-29 21:02:08 [INFO]: epoch 258: training loss 0.0560\n",
      "2025-05-29 21:02:08 [INFO]: epoch 259: training loss 0.0570\n",
      "2025-05-29 21:02:08 [INFO]: epoch 260: training loss 0.0599\n",
      "2025-05-29 21:02:08 [INFO]: epoch 261: training loss 0.0538\n",
      "2025-05-29 21:02:08 [INFO]: epoch 262: training loss 0.0598\n",
      "2025-05-29 21:02:08 [INFO]: epoch 263: training loss 0.0536\n",
      "2025-05-29 21:02:08 [INFO]: epoch 264: training loss 0.0584\n",
      "2025-05-29 21:02:08 [INFO]: epoch 265: training loss 0.0667\n",
      "2025-05-29 21:02:08 [INFO]: epoch 266: training loss 0.0624\n",
      "2025-05-29 21:02:08 [INFO]: epoch 267: training loss 0.0582\n",
      "2025-05-29 21:02:09 [INFO]: epoch 268: training loss 0.0540\n",
      "2025-05-29 21:02:09 [INFO]: epoch 269: training loss 0.0707\n",
      "2025-05-29 21:02:09 [INFO]: epoch 270: training loss 0.0628\n",
      "2025-05-29 21:02:09 [INFO]: epoch 271: training loss 0.0555\n",
      "2025-05-29 21:02:09 [INFO]: epoch 272: training loss 0.0526\n",
      "2025-05-29 21:02:09 [INFO]: epoch 273: training loss 0.0566\n",
      "2025-05-29 21:02:09 [INFO]: epoch 274: training loss 0.0599\n",
      "2025-05-29 21:02:09 [INFO]: epoch 275: training loss 0.0591\n",
      "2025-05-29 21:02:09 [INFO]: epoch 276: training loss 0.0474\n",
      "2025-05-29 21:02:09 [INFO]: epoch 277: training loss 0.0492\n",
      "2025-05-29 21:02:09 [INFO]: epoch 278: training loss 0.0583\n",
      "2025-05-29 21:02:09 [INFO]: epoch 279: training loss 0.0511\n",
      "2025-05-29 21:02:09 [INFO]: epoch 280: training loss 0.0499\n",
      "2025-05-29 21:02:09 [INFO]: epoch 281: training loss 0.0452\n",
      "2025-05-29 21:02:09 [INFO]: epoch 282: training loss 0.0444\n",
      "2025-05-29 21:02:09 [INFO]: epoch 283: training loss 0.0566\n",
      "2025-05-29 21:02:09 [INFO]: epoch 284: training loss 0.0453\n",
      "2025-05-29 21:02:09 [INFO]: epoch 285: training loss 0.0478\n",
      "2025-05-29 21:02:09 [INFO]: epoch 286: training loss 0.0486\n",
      "2025-05-29 21:02:09 [INFO]: epoch 287: training loss 0.0467\n",
      "2025-05-29 21:02:09 [INFO]: epoch 288: training loss 0.0516\n",
      "2025-05-29 21:02:09 [INFO]: epoch 289: training loss 0.0501\n",
      "2025-05-29 21:02:09 [INFO]: epoch 290: training loss 0.0494\n",
      "2025-05-29 21:02:09 [INFO]: epoch 291: training loss 0.0440\n",
      "2025-05-29 21:02:09 [INFO]: epoch 292: training loss 0.0494\n",
      "2025-05-29 21:02:09 [INFO]: epoch 293: training loss 0.0573\n",
      "2025-05-29 21:02:09 [INFO]: epoch 294: training loss 0.0537\n",
      "2025-05-29 21:02:09 [INFO]: epoch 295: training loss 0.0477\n",
      "2025-05-29 21:02:09 [INFO]: epoch 296: training loss 0.0474\n",
      "2025-05-29 21:02:09 [INFO]: epoch 297: training loss 0.0536\n",
      "2025-05-29 21:02:09 [INFO]: epoch 298: training loss 0.0538\n",
      "2025-05-29 21:02:09 [INFO]: epoch 299: training loss 0.0553\n",
      "2025-05-29 21:02:09 [INFO]: epoch 300: training loss 0.0554\n",
      "2025-05-29 21:02:09 [INFO]: epoch 301: training loss 0.0637\n",
      "2025-05-29 21:02:09 [INFO]: epoch 302: training loss 0.0481\n",
      "2025-05-29 21:02:09 [INFO]: epoch 303: training loss 0.0401\n",
      "2025-05-29 21:02:09 [INFO]: epoch 304: training loss 0.0551\n",
      "2025-05-29 21:02:09 [INFO]: epoch 305: training loss 0.0617\n",
      "2025-05-29 21:02:09 [INFO]: epoch 306: training loss 0.0500\n",
      "2025-05-29 21:02:09 [INFO]: epoch 307: training loss 0.0459\n",
      "2025-05-29 21:02:09 [INFO]: epoch 308: training loss 0.0533\n",
      "2025-05-29 21:02:09 [INFO]: epoch 309: training loss 0.0699\n",
      "2025-05-29 21:02:09 [INFO]: epoch 310: training loss 0.0590\n",
      "2025-05-29 21:02:09 [INFO]: epoch 311: training loss 0.0447\n",
      "2025-05-29 21:02:09 [INFO]: epoch 312: training loss 0.0516\n",
      "2025-05-29 21:02:09 [INFO]: epoch 313: training loss 0.0498\n",
      "2025-05-29 21:02:09 [INFO]: epoch 314: training loss 0.0435\n",
      "2025-05-29 21:02:09 [INFO]: epoch 315: training loss 0.0466\n",
      "2025-05-29 21:02:09 [INFO]: epoch 316: training loss 0.0511\n",
      "2025-05-29 21:02:09 [INFO]: epoch 317: training loss 0.0555\n",
      "2025-05-29 21:02:09 [INFO]: epoch 318: training loss 0.0459\n",
      "2025-05-29 21:02:09 [INFO]: epoch 319: training loss 0.0561\n",
      "2025-05-29 21:02:09 [INFO]: epoch 320: training loss 0.0575\n",
      "2025-05-29 21:02:09 [INFO]: epoch 321: training loss 0.0564\n",
      "2025-05-29 21:02:09 [INFO]: epoch 322: training loss 0.0581\n",
      "2025-05-29 21:02:09 [INFO]: epoch 323: training loss 0.0601\n",
      "2025-05-29 21:02:09 [INFO]: epoch 324: training loss 0.0415\n",
      "2025-05-29 21:02:09 [INFO]: epoch 325: training loss 0.0550\n",
      "2025-05-29 21:02:09 [INFO]: epoch 326: training loss 0.0532\n",
      "2025-05-29 21:02:09 [INFO]: epoch 327: training loss 0.0439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:02:09 [INFO]: epoch 328: training loss 0.0565\n",
      "2025-05-29 21:02:09 [INFO]: epoch 329: training loss 0.0508\n",
      "2025-05-29 21:02:09 [INFO]: epoch 330: training loss 0.0445\n",
      "2025-05-29 21:02:09 [INFO]: epoch 331: training loss 0.0487\n",
      "2025-05-29 21:02:09 [INFO]: epoch 332: training loss 0.0454\n",
      "2025-05-29 21:02:09 [INFO]: epoch 333: training loss 0.0454\n",
      "2025-05-29 21:02:09 [INFO]: epoch 334: training loss 0.0510\n",
      "2025-05-29 21:02:09 [INFO]: epoch 335: training loss 0.0451\n",
      "2025-05-29 21:02:09 [INFO]: epoch 336: training loss 0.0399\n",
      "2025-05-29 21:02:09 [INFO]: epoch 337: training loss 0.0477\n",
      "2025-05-29 21:02:09 [INFO]: epoch 338: training loss 0.0559\n",
      "2025-05-29 21:02:09 [INFO]: epoch 339: training loss 0.0429\n",
      "2025-05-29 21:02:09 [INFO]: epoch 340: training loss 0.0507\n",
      "2025-05-29 21:02:09 [INFO]: epoch 341: training loss 0.0599\n",
      "2025-05-29 21:02:09 [INFO]: epoch 342: training loss 0.0493\n",
      "2025-05-29 21:02:09 [INFO]: epoch 343: training loss 0.0632\n",
      "2025-05-29 21:02:09 [INFO]: epoch 344: training loss 0.0552\n",
      "2025-05-29 21:02:09 [INFO]: epoch 345: training loss 0.0468\n",
      "2025-05-29 21:02:09 [INFO]: epoch 346: training loss 0.0445\n",
      "2025-05-29 21:02:09 [INFO]: epoch 347: training loss 0.0504\n",
      "2025-05-29 21:02:09 [INFO]: epoch 348: training loss 0.0495\n",
      "2025-05-29 21:02:10 [INFO]: epoch 349: training loss 0.0486\n",
      "2025-05-29 21:02:10 [INFO]: epoch 350: training loss 0.0473\n",
      "2025-05-29 21:02:10 [INFO]: epoch 351: training loss 0.0495\n",
      "2025-05-29 21:02:10 [INFO]: epoch 352: training loss 0.0515\n",
      "2025-05-29 21:02:10 [INFO]: epoch 353: training loss 0.0502\n",
      "2025-05-29 21:02:10 [INFO]: epoch 354: training loss 0.0473\n",
      "2025-05-29 21:02:10 [INFO]: epoch 355: training loss 0.0449\n",
      "2025-05-29 21:02:10 [INFO]: epoch 356: training loss 0.0419\n",
      "2025-05-29 21:02:10 [INFO]: epoch 357: training loss 0.0575\n",
      "2025-05-29 21:02:10 [INFO]: epoch 358: training loss 0.0443\n",
      "2025-05-29 21:02:10 [INFO]: epoch 359: training loss 0.0405\n",
      "2025-05-29 21:02:10 [INFO]: epoch 360: training loss 0.0435\n",
      "2025-05-29 21:02:10 [INFO]: epoch 361: training loss 0.0480\n",
      "2025-05-29 21:02:10 [INFO]: epoch 362: training loss 0.0464\n",
      "2025-05-29 21:02:10 [INFO]: epoch 363: training loss 0.0344\n",
      "2025-05-29 21:02:10 [INFO]: epoch 364: training loss 0.0534\n",
      "2025-05-29 21:02:10 [INFO]: epoch 365: training loss 0.0457\n",
      "2025-05-29 21:02:10 [INFO]: epoch 366: training loss 0.0368\n",
      "2025-05-29 21:02:10 [INFO]: epoch 367: training loss 0.0424\n",
      "2025-05-29 21:02:10 [INFO]: epoch 368: training loss 0.0457\n",
      "2025-05-29 21:02:10 [INFO]: epoch 369: training loss 0.0451\n",
      "2025-05-29 21:02:10 [INFO]: epoch 370: training loss 0.0432\n",
      "2025-05-29 21:02:10 [INFO]: epoch 371: training loss 0.0404\n",
      "2025-05-29 21:02:10 [INFO]: epoch 372: training loss 0.0383\n",
      "2025-05-29 21:02:10 [INFO]: epoch 373: training loss 0.0385\n",
      "2025-05-29 21:02:10 [INFO]: epoch 374: training loss 0.0492\n",
      "2025-05-29 21:02:10 [INFO]: epoch 375: training loss 0.0409\n",
      "2025-05-29 21:02:10 [INFO]: epoch 376: training loss 0.0407\n",
      "2025-05-29 21:02:10 [INFO]: epoch 377: training loss 0.0434\n",
      "2025-05-29 21:02:10 [INFO]: epoch 378: training loss 0.0448\n",
      "2025-05-29 21:02:10 [INFO]: epoch 379: training loss 0.0408\n",
      "2025-05-29 21:02:10 [INFO]: epoch 380: training loss 0.0436\n",
      "2025-05-29 21:02:10 [INFO]: epoch 381: training loss 0.0381\n",
      "2025-05-29 21:02:10 [INFO]: epoch 382: training loss 0.0487\n",
      "2025-05-29 21:02:10 [INFO]: epoch 383: training loss 0.0405\n",
      "2025-05-29 21:02:10 [INFO]: epoch 384: training loss 0.0414\n",
      "2025-05-29 21:02:10 [INFO]: epoch 385: training loss 0.0441\n",
      "2025-05-29 21:02:10 [INFO]: epoch 386: training loss 0.0374\n",
      "2025-05-29 21:02:10 [INFO]: epoch 387: training loss 0.0399\n",
      "2025-05-29 21:02:10 [INFO]: epoch 388: training loss 0.0400\n",
      "2025-05-29 21:02:10 [INFO]: epoch 389: training loss 0.0411\n",
      "2025-05-29 21:02:10 [INFO]: epoch 390: training loss 0.0378\n",
      "2025-05-29 21:02:10 [INFO]: epoch 391: training loss 0.0414\n",
      "2025-05-29 21:02:10 [INFO]: epoch 392: training loss 0.0408\n",
      "2025-05-29 21:02:10 [INFO]: epoch 393: training loss 0.0333\n",
      "2025-05-29 21:02:10 [INFO]: epoch 394: training loss 0.0386\n",
      "2025-05-29 21:02:10 [INFO]: epoch 395: training loss 0.0383\n",
      "2025-05-29 21:02:10 [INFO]: epoch 396: training loss 0.0333\n",
      "2025-05-29 21:02:10 [INFO]: epoch 397: training loss 0.0368\n",
      "2025-05-29 21:02:10 [INFO]: epoch 398: training loss 0.0451\n",
      "2025-05-29 21:02:10 [INFO]: epoch 399: training loss 0.0339\n",
      "2025-05-29 21:02:10 [INFO]: epoch 400: training loss 0.0412\n",
      "2025-05-29 21:02:10 [INFO]: epoch 401: training loss 0.0399\n",
      "2025-05-29 21:02:10 [INFO]: epoch 402: training loss 0.0378\n",
      "2025-05-29 21:02:10 [INFO]: epoch 403: training loss 0.0382\n",
      "2025-05-29 21:02:10 [INFO]: epoch 404: training loss 0.0470\n",
      "2025-05-29 21:02:10 [INFO]: epoch 405: training loss 0.0342\n",
      "2025-05-29 21:02:10 [INFO]: epoch 406: training loss 0.0398\n",
      "2025-05-29 21:02:10 [INFO]: epoch 407: training loss 0.0429\n",
      "2025-05-29 21:02:10 [INFO]: epoch 408: training loss 0.0389\n",
      "2025-05-29 21:02:10 [INFO]: epoch 409: training loss 0.0407\n",
      "2025-05-29 21:02:10 [INFO]: epoch 410: training loss 0.0383\n",
      "2025-05-29 21:02:10 [INFO]: epoch 411: training loss 0.0379\n",
      "2025-05-29 21:02:10 [INFO]: epoch 412: training loss 0.0371\n",
      "2025-05-29 21:02:10 [INFO]: epoch 413: training loss 0.0348\n",
      "2025-05-29 21:02:10 [INFO]: epoch 414: training loss 0.0375\n",
      "2025-05-29 21:02:10 [INFO]: epoch 415: training loss 0.0341\n",
      "2025-05-29 21:02:10 [INFO]: epoch 416: training loss 0.0408\n",
      "2025-05-29 21:02:10 [INFO]: epoch 417: training loss 0.0356\n",
      "2025-05-29 21:02:10 [INFO]: epoch 418: training loss 0.0387\n",
      "2025-05-29 21:02:10 [INFO]: epoch 419: training loss 0.0355\n",
      "2025-05-29 21:02:10 [INFO]: epoch 420: training loss 0.0355\n",
      "2025-05-29 21:02:10 [INFO]: epoch 421: training loss 0.0396\n",
      "2025-05-29 21:02:10 [INFO]: epoch 422: training loss 0.0416\n",
      "2025-05-29 21:02:10 [INFO]: epoch 423: training loss 0.0354\n",
      "2025-05-29 21:02:10 [INFO]: epoch 424: training loss 0.0396\n",
      "2025-05-29 21:02:10 [INFO]: epoch 425: training loss 0.0344\n",
      "2025-05-29 21:02:10 [INFO]: epoch 426: training loss 0.0352\n",
      "2025-05-29 21:02:10 [INFO]: epoch 427: training loss 0.0392\n",
      "2025-05-29 21:02:10 [INFO]: epoch 428: training loss 0.0365\n",
      "2025-05-29 21:02:10 [INFO]: epoch 429: training loss 0.0377\n",
      "2025-05-29 21:02:11 [INFO]: epoch 430: training loss 0.0315\n",
      "2025-05-29 21:02:11 [INFO]: epoch 431: training loss 0.0343\n",
      "2025-05-29 21:02:11 [INFO]: epoch 432: training loss 0.0420\n",
      "2025-05-29 21:02:11 [INFO]: epoch 433: training loss 0.0353\n",
      "2025-05-29 21:02:11 [INFO]: epoch 434: training loss 0.0367\n",
      "2025-05-29 21:02:11 [INFO]: epoch 435: training loss 0.0418\n",
      "2025-05-29 21:02:11 [INFO]: epoch 436: training loss 0.0363\n",
      "2025-05-29 21:02:11 [INFO]: epoch 437: training loss 0.0368\n",
      "2025-05-29 21:02:11 [INFO]: epoch 438: training loss 0.0367\n",
      "2025-05-29 21:02:11 [INFO]: epoch 439: training loss 0.0337\n",
      "2025-05-29 21:02:11 [INFO]: epoch 440: training loss 0.0385\n",
      "2025-05-29 21:02:11 [INFO]: epoch 441: training loss 0.0463\n",
      "2025-05-29 21:02:11 [INFO]: epoch 442: training loss 0.0406\n",
      "2025-05-29 21:02:11 [INFO]: epoch 443: training loss 0.0339\n",
      "2025-05-29 21:02:11 [INFO]: epoch 444: training loss 0.0486\n",
      "2025-05-29 21:02:11 [INFO]: epoch 445: training loss 0.0438\n",
      "2025-05-29 21:02:11 [INFO]: epoch 446: training loss 0.0328\n",
      "2025-05-29 21:02:11 [INFO]: epoch 447: training loss 0.0357\n",
      "2025-05-29 21:02:11 [INFO]: epoch 448: training loss 0.0428\n",
      "2025-05-29 21:02:11 [INFO]: epoch 449: training loss 0.0438\n",
      "2025-05-29 21:02:11 [INFO]: epoch 450: training loss 0.0353\n",
      "2025-05-29 21:02:11 [INFO]: epoch 451: training loss 0.0378\n",
      "2025-05-29 21:02:11 [INFO]: epoch 452: training loss 0.0398\n",
      "2025-05-29 21:02:11 [INFO]: epoch 453: training loss 0.0382\n",
      "2025-05-29 21:02:11 [INFO]: epoch 454: training loss 0.0332\n",
      "2025-05-29 21:02:11 [INFO]: epoch 455: training loss 0.0445\n",
      "2025-05-29 21:02:11 [INFO]: epoch 456: training loss 0.0404\n",
      "2025-05-29 21:02:11 [INFO]: epoch 457: training loss 0.0332\n",
      "2025-05-29 21:02:11 [INFO]: epoch 458: training loss 0.0406\n",
      "2025-05-29 21:02:11 [INFO]: epoch 459: training loss 0.0406\n",
      "2025-05-29 21:02:11 [INFO]: epoch 460: training loss 0.0376\n",
      "2025-05-29 21:02:11 [INFO]: epoch 461: training loss 0.0365\n",
      "2025-05-29 21:02:11 [INFO]: epoch 462: training loss 0.0426\n",
      "2025-05-29 21:02:11 [INFO]: epoch 463: training loss 0.0399\n",
      "2025-05-29 21:02:11 [INFO]: epoch 464: training loss 0.0392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:02:11 [INFO]: epoch 465: training loss 0.0415\n",
      "2025-05-29 21:02:11 [INFO]: epoch 466: training loss 0.0478\n",
      "2025-05-29 21:02:11 [INFO]: epoch 467: training loss 0.0444\n",
      "2025-05-29 21:02:11 [INFO]: epoch 468: training loss 0.0351\n",
      "2025-05-29 21:02:11 [INFO]: epoch 469: training loss 0.0358\n",
      "2025-05-29 21:02:11 [INFO]: epoch 470: training loss 0.0408\n",
      "2025-05-29 21:02:11 [INFO]: epoch 471: training loss 0.0340\n",
      "2025-05-29 21:02:11 [INFO]: epoch 472: training loss 0.0379\n",
      "2025-05-29 21:02:11 [INFO]: epoch 473: training loss 0.0384\n",
      "2025-05-29 21:02:11 [INFO]: epoch 474: training loss 0.0345\n",
      "2025-05-29 21:02:11 [INFO]: epoch 475: training loss 0.0385\n",
      "2025-05-29 21:02:11 [INFO]: epoch 476: training loss 0.0471\n",
      "2025-05-29 21:02:11 [INFO]: epoch 477: training loss 0.0455\n",
      "2025-05-29 21:02:11 [INFO]: epoch 478: training loss 0.0394\n",
      "2025-05-29 21:02:11 [INFO]: epoch 479: training loss 0.0323\n",
      "2025-05-29 21:02:11 [INFO]: epoch 480: training loss 0.0439\n",
      "2025-05-29 21:02:11 [INFO]: epoch 481: training loss 0.0398\n",
      "2025-05-29 21:02:11 [INFO]: epoch 482: training loss 0.0412\n",
      "2025-05-29 21:02:11 [INFO]: epoch 483: training loss 0.0310\n",
      "2025-05-29 21:02:11 [INFO]: epoch 484: training loss 0.0330\n",
      "2025-05-29 21:02:11 [INFO]: epoch 485: training loss 0.0344\n",
      "2025-05-29 21:02:11 [INFO]: epoch 486: training loss 0.0376\n",
      "2025-05-29 21:02:11 [INFO]: epoch 487: training loss 0.0365\n",
      "2025-05-29 21:02:11 [INFO]: epoch 488: training loss 0.0360\n",
      "2025-05-29 21:02:11 [INFO]: epoch 489: training loss 0.0374\n",
      "2025-05-29 21:02:11 [INFO]: epoch 490: training loss 0.0412\n",
      "2025-05-29 21:02:11 [INFO]: epoch 491: training loss 0.0376\n",
      "2025-05-29 21:02:11 [INFO]: epoch 492: training loss 0.0375\n",
      "2025-05-29 21:02:11 [INFO]: epoch 493: training loss 0.0387\n",
      "2025-05-29 21:02:11 [INFO]: epoch 494: training loss 0.0414\n",
      "2025-05-29 21:02:11 [INFO]: epoch 495: training loss 0.0392\n",
      "2025-05-29 21:02:11 [INFO]: epoch 496: training loss 0.0393\n",
      "2025-05-29 21:02:11 [INFO]: epoch 497: training loss 0.0410\n",
      "2025-05-29 21:02:11 [INFO]: epoch 498: training loss 0.0393\n",
      "2025-05-29 21:02:11 [INFO]: epoch 499: training loss 0.0417\n",
      "2025-05-29 21:02:11 [INFO]: epoch 500: training loss 0.0355\n",
      "2025-05-29 21:02:11 [INFO]: epoch 501: training loss 0.0443\n",
      "2025-05-29 21:02:11 [INFO]: epoch 502: training loss 0.0346\n",
      "2025-05-29 21:02:11 [INFO]: epoch 503: training loss 0.0319\n",
      "2025-05-29 21:02:11 [INFO]: epoch 504: training loss 0.0412\n",
      "2025-05-29 21:02:11 [INFO]: epoch 505: training loss 0.0340\n",
      "2025-05-29 21:02:11 [INFO]: epoch 506: training loss 0.0365\n",
      "2025-05-29 21:02:11 [INFO]: epoch 507: training loss 0.0391\n",
      "2025-05-29 21:02:11 [INFO]: epoch 508: training loss 0.0434\n",
      "2025-05-29 21:02:11 [INFO]: epoch 509: training loss 0.0382\n",
      "2025-05-29 21:02:12 [INFO]: epoch 510: training loss 0.0344\n",
      "2025-05-29 21:02:12 [INFO]: epoch 511: training loss 0.0341\n",
      "2025-05-29 21:02:12 [INFO]: epoch 512: training loss 0.0296\n",
      "2025-05-29 21:02:12 [INFO]: epoch 513: training loss 0.0342\n",
      "2025-05-29 21:02:12 [INFO]: epoch 514: training loss 0.0315\n",
      "2025-05-29 21:02:12 [INFO]: epoch 515: training loss 0.0282\n",
      "2025-05-29 21:02:12 [INFO]: epoch 516: training loss 0.0337\n",
      "2025-05-29 21:02:12 [INFO]: epoch 517: training loss 0.0395\n",
      "2025-05-29 21:02:12 [INFO]: epoch 518: training loss 0.0348\n",
      "2025-05-29 21:02:12 [INFO]: epoch 519: training loss 0.0331\n",
      "2025-05-29 21:02:12 [INFO]: epoch 520: training loss 0.0345\n",
      "2025-05-29 21:02:12 [INFO]: epoch 521: training loss 0.0367\n",
      "2025-05-29 21:02:12 [INFO]: epoch 522: training loss 0.0369\n",
      "2025-05-29 21:02:12 [INFO]: epoch 523: training loss 0.0366\n",
      "2025-05-29 21:02:12 [INFO]: epoch 524: training loss 0.0367\n",
      "2025-05-29 21:02:12 [INFO]: epoch 525: training loss 0.0288\n",
      "2025-05-29 21:02:12 [INFO]: epoch 526: training loss 0.0334\n",
      "2025-05-29 21:02:12 [INFO]: epoch 527: training loss 0.0385\n",
      "2025-05-29 21:02:12 [INFO]: epoch 528: training loss 0.0466\n",
      "2025-05-29 21:02:12 [INFO]: epoch 529: training loss 0.0453\n",
      "2025-05-29 21:02:12 [INFO]: epoch 530: training loss 0.0317\n",
      "2025-05-29 21:02:12 [INFO]: epoch 531: training loss 0.0362\n",
      "2025-05-29 21:02:12 [INFO]: epoch 532: training loss 0.0400\n",
      "2025-05-29 21:02:12 [INFO]: epoch 533: training loss 0.0294\n",
      "2025-05-29 21:02:12 [INFO]: epoch 534: training loss 0.0328\n",
      "2025-05-29 21:02:12 [INFO]: epoch 535: training loss 0.0418\n",
      "2025-05-29 21:02:12 [INFO]: epoch 536: training loss 0.0382\n",
      "2025-05-29 21:02:12 [INFO]: epoch 537: training loss 0.0318\n",
      "2025-05-29 21:02:12 [INFO]: epoch 538: training loss 0.0350\n",
      "2025-05-29 21:02:12 [INFO]: epoch 539: training loss 0.0302\n",
      "2025-05-29 21:02:12 [INFO]: epoch 540: training loss 0.0350\n",
      "2025-05-29 21:02:12 [INFO]: epoch 541: training loss 0.0338\n",
      "2025-05-29 21:02:12 [INFO]: epoch 542: training loss 0.0289\n",
      "2025-05-29 21:02:12 [INFO]: epoch 543: training loss 0.0360\n",
      "2025-05-29 21:02:12 [INFO]: epoch 544: training loss 0.0289\n",
      "2025-05-29 21:02:12 [INFO]: epoch 545: training loss 0.0390\n",
      "2025-05-29 21:02:12 [INFO]: epoch 546: training loss 0.0300\n",
      "2025-05-29 21:02:12 [INFO]: epoch 547: training loss 0.0358\n",
      "2025-05-29 21:02:12 [INFO]: epoch 548: training loss 0.0363\n",
      "2025-05-29 21:02:12 [INFO]: epoch 549: training loss 0.0313\n",
      "2025-05-29 21:02:12 [INFO]: epoch 550: training loss 0.0373\n",
      "2025-05-29 21:02:12 [INFO]: epoch 551: training loss 0.0353\n",
      "2025-05-29 21:02:12 [INFO]: epoch 552: training loss 0.0256\n",
      "2025-05-29 21:02:12 [INFO]: epoch 553: training loss 0.0314\n",
      "2025-05-29 21:02:12 [INFO]: epoch 554: training loss 0.0366\n",
      "2025-05-29 21:02:12 [INFO]: epoch 555: training loss 0.0306\n",
      "2025-05-29 21:02:12 [INFO]: epoch 556: training loss 0.0322\n",
      "2025-05-29 21:02:12 [INFO]: epoch 557: training loss 0.0406\n",
      "2025-05-29 21:02:12 [INFO]: epoch 558: training loss 0.0312\n",
      "2025-05-29 21:02:12 [INFO]: epoch 559: training loss 0.0304\n",
      "2025-05-29 21:02:12 [INFO]: epoch 560: training loss 0.0298\n",
      "2025-05-29 21:02:12 [INFO]: epoch 561: training loss 0.0306\n",
      "2025-05-29 21:02:12 [INFO]: epoch 562: training loss 0.0319\n",
      "2025-05-29 21:02:12 [INFO]: epoch 563: training loss 0.0308\n",
      "2025-05-29 21:02:12 [INFO]: epoch 564: training loss 0.0291\n",
      "2025-05-29 21:02:12 [INFO]: epoch 565: training loss 0.0289\n",
      "2025-05-29 21:02:12 [INFO]: epoch 566: training loss 0.0287\n",
      "2025-05-29 21:02:12 [INFO]: epoch 567: training loss 0.0273\n",
      "2025-05-29 21:02:12 [INFO]: epoch 568: training loss 0.0338\n",
      "2025-05-29 21:02:12 [INFO]: epoch 569: training loss 0.0351\n",
      "2025-05-29 21:02:12 [INFO]: epoch 570: training loss 0.0313\n",
      "2025-05-29 21:02:12 [INFO]: epoch 571: training loss 0.0301\n",
      "2025-05-29 21:02:12 [INFO]: epoch 572: training loss 0.0352\n",
      "2025-05-29 21:02:12 [INFO]: epoch 573: training loss 0.0337\n",
      "2025-05-29 21:02:12 [INFO]: epoch 574: training loss 0.0291\n",
      "2025-05-29 21:02:12 [INFO]: epoch 575: training loss 0.0310\n",
      "2025-05-29 21:02:12 [INFO]: epoch 576: training loss 0.0279\n",
      "2025-05-29 21:02:12 [INFO]: epoch 577: training loss 0.0276\n",
      "2025-05-29 21:02:12 [INFO]: epoch 578: training loss 0.0261\n",
      "2025-05-29 21:02:12 [INFO]: epoch 579: training loss 0.0304\n",
      "2025-05-29 21:02:12 [INFO]: epoch 580: training loss 0.0314\n",
      "2025-05-29 21:02:12 [INFO]: epoch 581: training loss 0.0292\n",
      "2025-05-29 21:02:12 [INFO]: epoch 582: training loss 0.0331\n",
      "2025-05-29 21:02:12 [INFO]: epoch 583: training loss 0.0279\n",
      "2025-05-29 21:02:12 [INFO]: epoch 584: training loss 0.0375\n",
      "2025-05-29 21:02:12 [INFO]: epoch 585: training loss 0.0307\n",
      "2025-05-29 21:02:12 [INFO]: epoch 586: training loss 0.0287\n",
      "2025-05-29 21:02:12 [INFO]: epoch 587: training loss 0.0331\n",
      "2025-05-29 21:02:12 [INFO]: epoch 588: training loss 0.0304\n",
      "2025-05-29 21:02:12 [INFO]: epoch 589: training loss 0.0263\n",
      "2025-05-29 21:02:13 [INFO]: epoch 590: training loss 0.0324\n",
      "2025-05-29 21:02:13 [INFO]: epoch 591: training loss 0.0344\n",
      "2025-05-29 21:02:13 [INFO]: epoch 592: training loss 0.0262\n",
      "2025-05-29 21:02:13 [INFO]: epoch 593: training loss 0.0303\n",
      "2025-05-29 21:02:13 [INFO]: epoch 594: training loss 0.0309\n",
      "2025-05-29 21:02:13 [INFO]: epoch 595: training loss 0.0305\n",
      "2025-05-29 21:02:13 [INFO]: epoch 596: training loss 0.0284\n",
      "2025-05-29 21:02:13 [INFO]: epoch 597: training loss 0.0348\n",
      "2025-05-29 21:02:13 [INFO]: epoch 598: training loss 0.0329\n",
      "2025-05-29 21:02:13 [INFO]: epoch 599: training loss 0.0288\n",
      "2025-05-29 21:02:13 [INFO]: Finished training.\n",
      "2025-05-29 21:02:13 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:30<00:07,  7.54s/it]2025-05-29 21:02:13 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:02:13 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:02:13 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:02:13 [INFO]: epoch 0: training loss 1.3000\n",
      "2025-05-29 21:02:13 [INFO]: epoch 1: training loss 0.5707\n",
      "2025-05-29 21:02:13 [INFO]: epoch 2: training loss 0.6549\n",
      "2025-05-29 21:02:13 [INFO]: epoch 3: training loss 0.6913\n",
      "2025-05-29 21:02:13 [INFO]: epoch 4: training loss 0.7090\n",
      "2025-05-29 21:02:13 [INFO]: epoch 5: training loss 0.5618\n",
      "2025-05-29 21:02:13 [INFO]: epoch 6: training loss 0.4586\n",
      "2025-05-29 21:02:13 [INFO]: epoch 7: training loss 0.4873\n",
      "2025-05-29 21:02:13 [INFO]: epoch 8: training loss 0.4667\n",
      "2025-05-29 21:02:13 [INFO]: epoch 9: training loss 0.4729\n",
      "2025-05-29 21:02:13 [INFO]: epoch 10: training loss 0.4727\n",
      "2025-05-29 21:02:13 [INFO]: epoch 11: training loss 0.4429\n",
      "2025-05-29 21:02:13 [INFO]: epoch 12: training loss 0.4216\n",
      "2025-05-29 21:02:13 [INFO]: epoch 13: training loss 0.4085\n",
      "2025-05-29 21:02:13 [INFO]: epoch 14: training loss 0.4039\n",
      "2025-05-29 21:02:13 [INFO]: epoch 15: training loss 0.4168\n",
      "2025-05-29 21:02:13 [INFO]: epoch 16: training loss 0.4248\n",
      "2025-05-29 21:02:13 [INFO]: epoch 17: training loss 0.3974\n",
      "2025-05-29 21:02:13 [INFO]: epoch 18: training loss 0.4308\n",
      "2025-05-29 21:02:13 [INFO]: epoch 19: training loss 0.3890\n",
      "2025-05-29 21:02:13 [INFO]: epoch 20: training loss 0.3829\n",
      "2025-05-29 21:02:13 [INFO]: epoch 21: training loss 0.3649\n",
      "2025-05-29 21:02:13 [INFO]: epoch 22: training loss 0.4106\n",
      "2025-05-29 21:02:13 [INFO]: epoch 23: training loss 0.3967\n",
      "2025-05-29 21:02:13 [INFO]: epoch 24: training loss 0.3855\n",
      "2025-05-29 21:02:13 [INFO]: epoch 25: training loss 0.3579\n",
      "2025-05-29 21:02:13 [INFO]: epoch 26: training loss 0.3584\n",
      "2025-05-29 21:02:13 [INFO]: epoch 27: training loss 0.3595\n",
      "2025-05-29 21:02:13 [INFO]: epoch 28: training loss 0.3422\n",
      "2025-05-29 21:02:13 [INFO]: epoch 29: training loss 0.3352\n",
      "2025-05-29 21:02:13 [INFO]: epoch 30: training loss 0.3608\n",
      "2025-05-29 21:02:13 [INFO]: epoch 31: training loss 0.3560\n",
      "2025-05-29 21:02:13 [INFO]: epoch 32: training loss 0.3299\n",
      "2025-05-29 21:02:13 [INFO]: epoch 33: training loss 0.3228\n",
      "2025-05-29 21:02:13 [INFO]: epoch 34: training loss 0.3225\n",
      "2025-05-29 21:02:13 [INFO]: epoch 35: training loss 0.3237\n",
      "2025-05-29 21:02:13 [INFO]: epoch 36: training loss 0.3061\n",
      "2025-05-29 21:02:13 [INFO]: epoch 37: training loss 0.3346\n",
      "2025-05-29 21:02:13 [INFO]: epoch 38: training loss 0.3349\n",
      "2025-05-29 21:02:13 [INFO]: epoch 39: training loss 0.3289\n",
      "2025-05-29 21:02:13 [INFO]: epoch 40: training loss 0.3302\n",
      "2025-05-29 21:02:13 [INFO]: epoch 41: training loss 0.3255\n",
      "2025-05-29 21:02:13 [INFO]: epoch 42: training loss 0.3157\n",
      "2025-05-29 21:02:13 [INFO]: epoch 43: training loss 0.3409\n",
      "2025-05-29 21:02:13 [INFO]: epoch 44: training loss 0.3355\n",
      "2025-05-29 21:02:13 [INFO]: epoch 45: training loss 0.3169\n",
      "2025-05-29 21:02:13 [INFO]: epoch 46: training loss 0.3086\n",
      "2025-05-29 21:02:13 [INFO]: epoch 47: training loss 0.3231\n",
      "2025-05-29 21:02:13 [INFO]: epoch 48: training loss 0.3003\n",
      "2025-05-29 21:02:13 [INFO]: epoch 49: training loss 0.3266\n",
      "2025-05-29 21:02:13 [INFO]: epoch 50: training loss 0.3413\n",
      "2025-05-29 21:02:13 [INFO]: epoch 51: training loss 0.3087\n",
      "2025-05-29 21:02:13 [INFO]: epoch 52: training loss 0.2835\n",
      "2025-05-29 21:02:13 [INFO]: epoch 53: training loss 0.2965\n",
      "2025-05-29 21:02:13 [INFO]: epoch 54: training loss 0.3118\n",
      "2025-05-29 21:02:13 [INFO]: epoch 55: training loss 0.3305\n",
      "2025-05-29 21:02:13 [INFO]: epoch 56: training loss 0.3032\n",
      "2025-05-29 21:02:13 [INFO]: epoch 57: training loss 0.2896\n",
      "2025-05-29 21:02:13 [INFO]: epoch 58: training loss 0.2868\n",
      "2025-05-29 21:02:13 [INFO]: epoch 59: training loss 0.2740\n",
      "2025-05-29 21:02:13 [INFO]: epoch 60: training loss 0.2766\n",
      "2025-05-29 21:02:13 [INFO]: epoch 61: training loss 0.2763\n",
      "2025-05-29 21:02:13 [INFO]: epoch 62: training loss 0.2531\n",
      "2025-05-29 21:02:13 [INFO]: epoch 63: training loss 0.2800\n",
      "2025-05-29 21:02:14 [INFO]: epoch 64: training loss 0.2728\n",
      "2025-05-29 21:02:14 [INFO]: epoch 65: training loss 0.2527\n",
      "2025-05-29 21:02:14 [INFO]: epoch 66: training loss 0.2818\n",
      "2025-05-29 21:02:14 [INFO]: epoch 67: training loss 0.2584\n",
      "2025-05-29 21:02:14 [INFO]: epoch 68: training loss 0.2782\n",
      "2025-05-29 21:02:14 [INFO]: epoch 69: training loss 0.2611\n",
      "2025-05-29 21:02:14 [INFO]: epoch 70: training loss 0.2707\n",
      "2025-05-29 21:02:14 [INFO]: epoch 71: training loss 0.2622\n",
      "2025-05-29 21:02:14 [INFO]: epoch 72: training loss 0.2520\n",
      "2025-05-29 21:02:14 [INFO]: epoch 73: training loss 0.2725\n",
      "2025-05-29 21:02:14 [INFO]: epoch 74: training loss 0.2686\n",
      "2025-05-29 21:02:14 [INFO]: epoch 75: training loss 0.2450\n",
      "2025-05-29 21:02:14 [INFO]: epoch 76: training loss 0.2406\n",
      "2025-05-29 21:02:14 [INFO]: epoch 77: training loss 0.2573\n",
      "2025-05-29 21:02:14 [INFO]: epoch 78: training loss 0.2830\n",
      "2025-05-29 21:02:14 [INFO]: epoch 79: training loss 0.2598\n",
      "2025-05-29 21:02:14 [INFO]: epoch 80: training loss 0.2668\n",
      "2025-05-29 21:02:14 [INFO]: epoch 81: training loss 0.2437\n",
      "2025-05-29 21:02:14 [INFO]: epoch 82: training loss 0.2478\n",
      "2025-05-29 21:02:14 [INFO]: epoch 83: training loss 0.2607\n",
      "2025-05-29 21:02:14 [INFO]: epoch 84: training loss 0.2488\n",
      "2025-05-29 21:02:14 [INFO]: epoch 85: training loss 0.2280\n",
      "2025-05-29 21:02:14 [INFO]: epoch 86: training loss 0.2227\n",
      "2025-05-29 21:02:14 [INFO]: epoch 87: training loss 0.2340\n",
      "2025-05-29 21:02:14 [INFO]: epoch 88: training loss 0.2656\n",
      "2025-05-29 21:02:14 [INFO]: epoch 89: training loss 0.2300\n",
      "2025-05-29 21:02:14 [INFO]: epoch 90: training loss 0.2433\n",
      "2025-05-29 21:02:14 [INFO]: epoch 91: training loss 0.2258\n",
      "2025-05-29 21:02:14 [INFO]: epoch 92: training loss 0.2067\n",
      "2025-05-29 21:02:14 [INFO]: epoch 93: training loss 0.2477\n",
      "2025-05-29 21:02:14 [INFO]: epoch 94: training loss 0.2668\n",
      "2025-05-29 21:02:14 [INFO]: epoch 95: training loss 0.2335\n",
      "2025-05-29 21:02:14 [INFO]: epoch 96: training loss 0.2304\n",
      "2025-05-29 21:02:14 [INFO]: epoch 97: training loss 0.2258\n",
      "2025-05-29 21:02:14 [INFO]: epoch 98: training loss 0.2212\n",
      "2025-05-29 21:02:14 [INFO]: epoch 99: training loss 0.2144\n",
      "2025-05-29 21:02:14 [INFO]: epoch 100: training loss 0.2157\n",
      "2025-05-29 21:02:14 [INFO]: epoch 101: training loss 0.2276\n",
      "2025-05-29 21:02:14 [INFO]: epoch 102: training loss 0.2204\n",
      "2025-05-29 21:02:14 [INFO]: epoch 103: training loss 0.2143\n",
      "2025-05-29 21:02:14 [INFO]: epoch 104: training loss 0.2124\n",
      "2025-05-29 21:02:14 [INFO]: epoch 105: training loss 0.2291\n",
      "2025-05-29 21:02:14 [INFO]: epoch 106: training loss 0.2077\n",
      "2025-05-29 21:02:14 [INFO]: epoch 107: training loss 0.1995\n",
      "2025-05-29 21:02:14 [INFO]: epoch 108: training loss 0.2004\n",
      "2025-05-29 21:02:14 [INFO]: epoch 109: training loss 0.2016\n",
      "2025-05-29 21:02:14 [INFO]: epoch 110: training loss 0.2062\n",
      "2025-05-29 21:02:14 [INFO]: epoch 111: training loss 0.1931\n",
      "2025-05-29 21:02:14 [INFO]: epoch 112: training loss 0.2066\n",
      "2025-05-29 21:02:14 [INFO]: epoch 113: training loss 0.1910\n",
      "2025-05-29 21:02:14 [INFO]: epoch 114: training loss 0.1820\n",
      "2025-05-29 21:02:14 [INFO]: epoch 115: training loss 0.1968\n",
      "2025-05-29 21:02:14 [INFO]: epoch 116: training loss 0.1934\n",
      "2025-05-29 21:02:14 [INFO]: epoch 117: training loss 0.1812\n",
      "2025-05-29 21:02:14 [INFO]: epoch 118: training loss 0.1955\n",
      "2025-05-29 21:02:14 [INFO]: epoch 119: training loss 0.1809\n",
      "2025-05-29 21:02:14 [INFO]: epoch 120: training loss 0.1989\n",
      "2025-05-29 21:02:14 [INFO]: epoch 121: training loss 0.1756\n",
      "2025-05-29 21:02:14 [INFO]: epoch 122: training loss 0.1749\n",
      "2025-05-29 21:02:14 [INFO]: epoch 123: training loss 0.2087\n",
      "2025-05-29 21:02:14 [INFO]: epoch 124: training loss 0.1994\n",
      "2025-05-29 21:02:14 [INFO]: epoch 125: training loss 0.1860\n",
      "2025-05-29 21:02:14 [INFO]: epoch 126: training loss 0.1834\n",
      "2025-05-29 21:02:14 [INFO]: epoch 127: training loss 0.1997\n",
      "2025-05-29 21:02:14 [INFO]: epoch 128: training loss 0.1729\n",
      "2025-05-29 21:02:14 [INFO]: epoch 129: training loss 0.1928\n",
      "2025-05-29 21:02:14 [INFO]: epoch 130: training loss 0.1917\n",
      "2025-05-29 21:02:14 [INFO]: epoch 131: training loss 0.1636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:02:14 [INFO]: epoch 132: training loss 0.1557\n",
      "2025-05-29 21:02:14 [INFO]: epoch 133: training loss 0.1703\n",
      "2025-05-29 21:02:14 [INFO]: epoch 134: training loss 0.1796\n",
      "2025-05-29 21:02:14 [INFO]: epoch 135: training loss 0.1806\n",
      "2025-05-29 21:02:14 [INFO]: epoch 136: training loss 0.1859\n",
      "2025-05-29 21:02:14 [INFO]: epoch 137: training loss 0.1679\n",
      "2025-05-29 21:02:14 [INFO]: epoch 138: training loss 0.1644\n",
      "2025-05-29 21:02:14 [INFO]: epoch 139: training loss 0.1664\n",
      "2025-05-29 21:02:14 [INFO]: epoch 140: training loss 0.1717\n",
      "2025-05-29 21:02:14 [INFO]: epoch 141: training loss 0.1672\n",
      "2025-05-29 21:02:14 [INFO]: epoch 142: training loss 0.1870\n",
      "2025-05-29 21:02:14 [INFO]: epoch 143: training loss 0.1788\n",
      "2025-05-29 21:02:15 [INFO]: epoch 144: training loss 0.1876\n",
      "2025-05-29 21:02:15 [INFO]: epoch 145: training loss 0.1685\n",
      "2025-05-29 21:02:15 [INFO]: epoch 146: training loss 0.1901\n",
      "2025-05-29 21:02:15 [INFO]: epoch 147: training loss 0.1589\n",
      "2025-05-29 21:02:15 [INFO]: epoch 148: training loss 0.1720\n",
      "2025-05-29 21:02:15 [INFO]: epoch 149: training loss 0.1845\n",
      "2025-05-29 21:02:15 [INFO]: epoch 150: training loss 0.1767\n",
      "2025-05-29 21:02:15 [INFO]: epoch 151: training loss 0.1661\n",
      "2025-05-29 21:02:15 [INFO]: epoch 152: training loss 0.1614\n",
      "2025-05-29 21:02:15 [INFO]: epoch 153: training loss 0.1583\n",
      "2025-05-29 21:02:15 [INFO]: epoch 154: training loss 0.1671\n",
      "2025-05-29 21:02:15 [INFO]: epoch 155: training loss 0.1566\n",
      "2025-05-29 21:02:15 [INFO]: epoch 156: training loss 0.1545\n",
      "2025-05-29 21:02:15 [INFO]: epoch 157: training loss 0.1597\n",
      "2025-05-29 21:02:15 [INFO]: epoch 158: training loss 0.1574\n",
      "2025-05-29 21:02:15 [INFO]: epoch 159: training loss 0.1522\n",
      "2025-05-29 21:02:15 [INFO]: epoch 160: training loss 0.1610\n",
      "2025-05-29 21:02:15 [INFO]: epoch 161: training loss 0.1526\n",
      "2025-05-29 21:02:15 [INFO]: epoch 162: training loss 0.1474\n",
      "2025-05-29 21:02:15 [INFO]: epoch 163: training loss 0.1513\n",
      "2025-05-29 21:02:15 [INFO]: epoch 164: training loss 0.1641\n",
      "2025-05-29 21:02:15 [INFO]: epoch 165: training loss 0.1556\n",
      "2025-05-29 21:02:15 [INFO]: epoch 166: training loss 0.1539\n",
      "2025-05-29 21:02:15 [INFO]: epoch 167: training loss 0.1463\n",
      "2025-05-29 21:02:15 [INFO]: epoch 168: training loss 0.1436\n",
      "2025-05-29 21:02:15 [INFO]: epoch 169: training loss 0.1433\n",
      "2025-05-29 21:02:15 [INFO]: epoch 170: training loss 0.1596\n",
      "2025-05-29 21:02:15 [INFO]: epoch 171: training loss 0.1396\n",
      "2025-05-29 21:02:15 [INFO]: epoch 172: training loss 0.1376\n",
      "2025-05-29 21:02:15 [INFO]: epoch 173: training loss 0.1446\n",
      "2025-05-29 21:02:15 [INFO]: epoch 174: training loss 0.1429\n",
      "2025-05-29 21:02:15 [INFO]: epoch 175: training loss 0.1355\n",
      "2025-05-29 21:02:15 [INFO]: epoch 176: training loss 0.1355\n",
      "2025-05-29 21:02:15 [INFO]: epoch 177: training loss 0.1321\n",
      "2025-05-29 21:02:15 [INFO]: epoch 178: training loss 0.1271\n",
      "2025-05-29 21:02:15 [INFO]: epoch 179: training loss 0.1428\n",
      "2025-05-29 21:02:15 [INFO]: epoch 180: training loss 0.1380\n",
      "2025-05-29 21:02:15 [INFO]: epoch 181: training loss 0.1375\n",
      "2025-05-29 21:02:15 [INFO]: epoch 182: training loss 0.1413\n",
      "2025-05-29 21:02:15 [INFO]: epoch 183: training loss 0.1150\n",
      "2025-05-29 21:02:15 [INFO]: epoch 184: training loss 0.1411\n",
      "2025-05-29 21:02:15 [INFO]: epoch 185: training loss 0.1350\n",
      "2025-05-29 21:02:15 [INFO]: epoch 186: training loss 0.1258\n",
      "2025-05-29 21:02:15 [INFO]: epoch 187: training loss 0.1259\n",
      "2025-05-29 21:02:15 [INFO]: epoch 188: training loss 0.1334\n",
      "2025-05-29 21:02:15 [INFO]: epoch 189: training loss 0.1229\n",
      "2025-05-29 21:02:15 [INFO]: epoch 190: training loss 0.1330\n",
      "2025-05-29 21:02:15 [INFO]: epoch 191: training loss 0.1419\n",
      "2025-05-29 21:02:15 [INFO]: epoch 192: training loss 0.1374\n",
      "2025-05-29 21:02:15 [INFO]: epoch 193: training loss 0.1197\n",
      "2025-05-29 21:02:15 [INFO]: epoch 194: training loss 0.1484\n",
      "2025-05-29 21:02:15 [INFO]: epoch 195: training loss 0.1346\n",
      "2025-05-29 21:02:15 [INFO]: epoch 196: training loss 0.1441\n",
      "2025-05-29 21:02:15 [INFO]: epoch 197: training loss 0.1372\n",
      "2025-05-29 21:02:15 [INFO]: epoch 198: training loss 0.1272\n",
      "2025-05-29 21:02:15 [INFO]: epoch 199: training loss 0.1460\n",
      "2025-05-29 21:02:15 [INFO]: epoch 200: training loss 0.1438\n",
      "2025-05-29 21:02:15 [INFO]: epoch 201: training loss 0.1317\n",
      "2025-05-29 21:02:15 [INFO]: epoch 202: training loss 0.1268\n",
      "2025-05-29 21:02:15 [INFO]: epoch 203: training loss 0.1328\n",
      "2025-05-29 21:02:15 [INFO]: epoch 204: training loss 0.1295\n",
      "2025-05-29 21:02:15 [INFO]: epoch 205: training loss 0.1366\n",
      "2025-05-29 21:02:15 [INFO]: epoch 206: training loss 0.1207\n",
      "2025-05-29 21:02:15 [INFO]: epoch 207: training loss 0.1188\n",
      "2025-05-29 21:02:15 [INFO]: epoch 208: training loss 0.1280\n",
      "2025-05-29 21:02:15 [INFO]: epoch 209: training loss 0.1277\n",
      "2025-05-29 21:02:15 [INFO]: epoch 210: training loss 0.1332\n",
      "2025-05-29 21:02:15 [INFO]: epoch 211: training loss 0.1145\n",
      "2025-05-29 21:02:15 [INFO]: epoch 212: training loss 0.1108\n",
      "2025-05-29 21:02:15 [INFO]: epoch 213: training loss 0.1286\n",
      "2025-05-29 21:02:15 [INFO]: epoch 214: training loss 0.1283\n",
      "2025-05-29 21:02:15 [INFO]: epoch 215: training loss 0.1182\n",
      "2025-05-29 21:02:15 [INFO]: epoch 216: training loss 0.1113\n",
      "2025-05-29 21:02:15 [INFO]: epoch 217: training loss 0.1248\n",
      "2025-05-29 21:02:15 [INFO]: epoch 218: training loss 0.1275\n",
      "2025-05-29 21:02:15 [INFO]: epoch 219: training loss 0.1115\n",
      "2025-05-29 21:02:15 [INFO]: epoch 220: training loss 0.1144\n",
      "2025-05-29 21:02:15 [INFO]: epoch 221: training loss 0.1086\n",
      "2025-05-29 21:02:15 [INFO]: epoch 222: training loss 0.1119\n",
      "2025-05-29 21:02:15 [INFO]: epoch 223: training loss 0.0933\n",
      "2025-05-29 21:02:15 [INFO]: epoch 224: training loss 0.1017\n",
      "2025-05-29 21:02:16 [INFO]: epoch 225: training loss 0.1040\n",
      "2025-05-29 21:02:16 [INFO]: epoch 226: training loss 0.1048\n",
      "2025-05-29 21:02:16 [INFO]: epoch 227: training loss 0.1064\n",
      "2025-05-29 21:02:16 [INFO]: epoch 228: training loss 0.1036\n",
      "2025-05-29 21:02:16 [INFO]: epoch 229: training loss 0.1233\n",
      "2025-05-29 21:02:16 [INFO]: epoch 230: training loss 0.0951\n",
      "2025-05-29 21:02:16 [INFO]: epoch 231: training loss 0.1096\n",
      "2025-05-29 21:02:16 [INFO]: epoch 232: training loss 0.1038\n",
      "2025-05-29 21:02:16 [INFO]: epoch 233: training loss 0.0931\n",
      "2025-05-29 21:02:16 [INFO]: epoch 234: training loss 0.1151\n",
      "2025-05-29 21:02:16 [INFO]: epoch 235: training loss 0.0993\n",
      "2025-05-29 21:02:16 [INFO]: epoch 236: training loss 0.1027\n",
      "2025-05-29 21:02:16 [INFO]: epoch 237: training loss 0.1337\n",
      "2025-05-29 21:02:16 [INFO]: epoch 238: training loss 0.0993\n",
      "2025-05-29 21:02:16 [INFO]: epoch 239: training loss 0.0930\n",
      "2025-05-29 21:02:16 [INFO]: epoch 240: training loss 0.1088\n",
      "2025-05-29 21:02:16 [INFO]: epoch 241: training loss 0.1020\n",
      "2025-05-29 21:02:16 [INFO]: epoch 242: training loss 0.0973\n",
      "2025-05-29 21:02:16 [INFO]: epoch 243: training loss 0.1046\n",
      "2025-05-29 21:02:16 [INFO]: epoch 244: training loss 0.0937\n",
      "2025-05-29 21:02:16 [INFO]: epoch 245: training loss 0.0912\n",
      "2025-05-29 21:02:16 [INFO]: epoch 246: training loss 0.1092\n",
      "2025-05-29 21:02:16 [INFO]: epoch 247: training loss 0.1024\n",
      "2025-05-29 21:02:16 [INFO]: epoch 248: training loss 0.0909\n",
      "2025-05-29 21:02:16 [INFO]: epoch 249: training loss 0.0898\n",
      "2025-05-29 21:02:16 [INFO]: epoch 250: training loss 0.0825\n",
      "2025-05-29 21:02:16 [INFO]: epoch 251: training loss 0.0835\n",
      "2025-05-29 21:02:16 [INFO]: epoch 252: training loss 0.0994\n",
      "2025-05-29 21:02:16 [INFO]: epoch 253: training loss 0.0798\n",
      "2025-05-29 21:02:16 [INFO]: epoch 254: training loss 0.0981\n",
      "2025-05-29 21:02:16 [INFO]: epoch 255: training loss 0.0819\n",
      "2025-05-29 21:02:16 [INFO]: epoch 256: training loss 0.0863\n",
      "2025-05-29 21:02:16 [INFO]: epoch 257: training loss 0.0950\n",
      "2025-05-29 21:02:16 [INFO]: epoch 258: training loss 0.0834\n",
      "2025-05-29 21:02:16 [INFO]: epoch 259: training loss 0.1011\n",
      "2025-05-29 21:02:16 [INFO]: epoch 260: training loss 0.0831\n",
      "2025-05-29 21:02:16 [INFO]: epoch 261: training loss 0.0893\n",
      "2025-05-29 21:02:16 [INFO]: epoch 262: training loss 0.0833\n",
      "2025-05-29 21:02:16 [INFO]: epoch 263: training loss 0.0883\n",
      "2025-05-29 21:02:16 [INFO]: epoch 264: training loss 0.0930\n",
      "2025-05-29 21:02:16 [INFO]: epoch 265: training loss 0.0840\n",
      "2025-05-29 21:02:16 [INFO]: epoch 266: training loss 0.0766\n",
      "2025-05-29 21:02:16 [INFO]: epoch 267: training loss 0.0797\n",
      "2025-05-29 21:02:16 [INFO]: epoch 268: training loss 0.0839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:02:16 [INFO]: epoch 269: training loss 0.0719\n",
      "2025-05-29 21:02:16 [INFO]: epoch 270: training loss 0.0958\n",
      "2025-05-29 21:02:16 [INFO]: epoch 271: training loss 0.0791\n",
      "2025-05-29 21:02:16 [INFO]: epoch 272: training loss 0.0832\n",
      "2025-05-29 21:02:16 [INFO]: epoch 273: training loss 0.0737\n",
      "2025-05-29 21:02:16 [INFO]: epoch 274: training loss 0.0746\n",
      "2025-05-29 21:02:16 [INFO]: epoch 275: training loss 0.0821\n",
      "2025-05-29 21:02:16 [INFO]: epoch 276: training loss 0.0841\n",
      "2025-05-29 21:02:16 [INFO]: epoch 277: training loss 0.0668\n",
      "2025-05-29 21:02:16 [INFO]: epoch 278: training loss 0.0805\n",
      "2025-05-29 21:02:16 [INFO]: epoch 279: training loss 0.0923\n",
      "2025-05-29 21:02:16 [INFO]: epoch 280: training loss 0.0875\n",
      "2025-05-29 21:02:16 [INFO]: epoch 281: training loss 0.0865\n",
      "2025-05-29 21:02:16 [INFO]: epoch 282: training loss 0.0771\n",
      "2025-05-29 21:02:16 [INFO]: epoch 283: training loss 0.0728\n",
      "2025-05-29 21:02:16 [INFO]: epoch 284: training loss 0.0818\n",
      "2025-05-29 21:02:16 [INFO]: epoch 285: training loss 0.0733\n",
      "2025-05-29 21:02:16 [INFO]: epoch 286: training loss 0.0600\n",
      "2025-05-29 21:02:16 [INFO]: epoch 287: training loss 0.0901\n",
      "2025-05-29 21:02:16 [INFO]: epoch 288: training loss 0.0776\n",
      "2025-05-29 21:02:16 [INFO]: epoch 289: training loss 0.0702\n",
      "2025-05-29 21:02:16 [INFO]: epoch 290: training loss 0.0727\n",
      "2025-05-29 21:02:16 [INFO]: epoch 291: training loss 0.0846\n",
      "2025-05-29 21:02:16 [INFO]: epoch 292: training loss 0.0753\n",
      "2025-05-29 21:02:16 [INFO]: epoch 293: training loss 0.0650\n",
      "2025-05-29 21:02:16 [INFO]: epoch 294: training loss 0.0657\n",
      "2025-05-29 21:02:16 [INFO]: epoch 295: training loss 0.0661\n",
      "2025-05-29 21:02:16 [INFO]: epoch 296: training loss 0.0824\n",
      "2025-05-29 21:02:16 [INFO]: epoch 297: training loss 0.0699\n",
      "2025-05-29 21:02:16 [INFO]: epoch 298: training loss 0.0639\n",
      "2025-05-29 21:02:16 [INFO]: epoch 299: training loss 0.0710\n",
      "2025-05-29 21:02:16 [INFO]: epoch 300: training loss 0.0753\n",
      "2025-05-29 21:02:16 [INFO]: epoch 301: training loss 0.0596\n",
      "2025-05-29 21:02:16 [INFO]: epoch 302: training loss 0.0597\n",
      "2025-05-29 21:02:16 [INFO]: epoch 303: training loss 0.0626\n",
      "2025-05-29 21:02:16 [INFO]: epoch 304: training loss 0.0638\n",
      "2025-05-29 21:02:17 [INFO]: epoch 305: training loss 0.0664\n",
      "2025-05-29 21:02:17 [INFO]: epoch 306: training loss 0.0566\n",
      "2025-05-29 21:02:17 [INFO]: epoch 307: training loss 0.0648\n",
      "2025-05-29 21:02:17 [INFO]: epoch 308: training loss 0.0632\n",
      "2025-05-29 21:02:17 [INFO]: epoch 309: training loss 0.0665\n",
      "2025-05-29 21:02:17 [INFO]: epoch 310: training loss 0.0611\n",
      "2025-05-29 21:02:17 [INFO]: epoch 311: training loss 0.0629\n",
      "2025-05-29 21:02:17 [INFO]: epoch 312: training loss 0.0553\n",
      "2025-05-29 21:02:17 [INFO]: epoch 313: training loss 0.0610\n",
      "2025-05-29 21:02:17 [INFO]: epoch 314: training loss 0.0536\n",
      "2025-05-29 21:02:17 [INFO]: epoch 315: training loss 0.0540\n",
      "2025-05-29 21:02:17 [INFO]: epoch 316: training loss 0.0545\n",
      "2025-05-29 21:02:17 [INFO]: epoch 317: training loss 0.0623\n",
      "2025-05-29 21:02:17 [INFO]: epoch 318: training loss 0.0529\n",
      "2025-05-29 21:02:17 [INFO]: epoch 319: training loss 0.0492\n",
      "2025-05-29 21:02:17 [INFO]: epoch 320: training loss 0.0514\n",
      "2025-05-29 21:02:17 [INFO]: epoch 321: training loss 0.0516\n",
      "2025-05-29 21:02:17 [INFO]: epoch 322: training loss 0.0566\n",
      "2025-05-29 21:02:17 [INFO]: epoch 323: training loss 0.0576\n",
      "2025-05-29 21:02:17 [INFO]: epoch 324: training loss 0.0538\n",
      "2025-05-29 21:02:17 [INFO]: epoch 325: training loss 0.0546\n",
      "2025-05-29 21:02:17 [INFO]: epoch 326: training loss 0.0616\n",
      "2025-05-29 21:02:17 [INFO]: epoch 327: training loss 0.0615\n",
      "2025-05-29 21:02:17 [INFO]: epoch 328: training loss 0.0511\n",
      "2025-05-29 21:02:17 [INFO]: epoch 329: training loss 0.0514\n",
      "2025-05-29 21:02:17 [INFO]: epoch 330: training loss 0.0599\n",
      "2025-05-29 21:02:17 [INFO]: epoch 331: training loss 0.0452\n",
      "2025-05-29 21:02:17 [INFO]: epoch 332: training loss 0.0503\n",
      "2025-05-29 21:02:17 [INFO]: epoch 333: training loss 0.0605\n",
      "2025-05-29 21:02:17 [INFO]: epoch 334: training loss 0.0537\n",
      "2025-05-29 21:02:17 [INFO]: epoch 335: training loss 0.0544\n",
      "2025-05-29 21:02:17 [INFO]: epoch 336: training loss 0.0424\n",
      "2025-05-29 21:02:17 [INFO]: epoch 337: training loss 0.0443\n",
      "2025-05-29 21:02:17 [INFO]: epoch 338: training loss 0.0516\n",
      "2025-05-29 21:02:17 [INFO]: epoch 339: training loss 0.0500\n",
      "2025-05-29 21:02:17 [INFO]: epoch 340: training loss 0.0556\n",
      "2025-05-29 21:02:17 [INFO]: epoch 341: training loss 0.0484\n",
      "2025-05-29 21:02:17 [INFO]: epoch 342: training loss 0.0482\n",
      "2025-05-29 21:02:17 [INFO]: epoch 343: training loss 0.0457\n",
      "2025-05-29 21:02:17 [INFO]: epoch 344: training loss 0.0480\n",
      "2025-05-29 21:02:17 [INFO]: epoch 345: training loss 0.0456\n",
      "2025-05-29 21:02:17 [INFO]: epoch 346: training loss 0.0522\n",
      "2025-05-29 21:02:17 [INFO]: epoch 347: training loss 0.0456\n",
      "2025-05-29 21:02:17 [INFO]: epoch 348: training loss 0.0460\n",
      "2025-05-29 21:02:17 [INFO]: epoch 349: training loss 0.0434\n",
      "2025-05-29 21:02:17 [INFO]: epoch 350: training loss 0.0481\n",
      "2025-05-29 21:02:17 [INFO]: epoch 351: training loss 0.0405\n",
      "2025-05-29 21:02:17 [INFO]: epoch 352: training loss 0.0475\n",
      "2025-05-29 21:02:17 [INFO]: epoch 353: training loss 0.0445\n",
      "2025-05-29 21:02:17 [INFO]: epoch 354: training loss 0.0462\n",
      "2025-05-29 21:02:17 [INFO]: epoch 355: training loss 0.0482\n",
      "2025-05-29 21:02:17 [INFO]: epoch 356: training loss 0.0544\n",
      "2025-05-29 21:02:17 [INFO]: epoch 357: training loss 0.0413\n",
      "2025-05-29 21:02:17 [INFO]: epoch 358: training loss 0.0533\n",
      "2025-05-29 21:02:17 [INFO]: epoch 359: training loss 0.0541\n",
      "2025-05-29 21:02:17 [INFO]: epoch 360: training loss 0.0457\n",
      "2025-05-29 21:02:17 [INFO]: epoch 361: training loss 0.0533\n",
      "2025-05-29 21:02:17 [INFO]: epoch 362: training loss 0.0527\n",
      "2025-05-29 21:02:17 [INFO]: epoch 363: training loss 0.0489\n",
      "2025-05-29 21:02:17 [INFO]: epoch 364: training loss 0.0549\n",
      "2025-05-29 21:02:17 [INFO]: epoch 365: training loss 0.0544\n",
      "2025-05-29 21:02:17 [INFO]: epoch 366: training loss 0.0575\n",
      "2025-05-29 21:02:17 [INFO]: epoch 367: training loss 0.0482\n",
      "2025-05-29 21:02:17 [INFO]: epoch 368: training loss 0.0534\n",
      "2025-05-29 21:02:17 [INFO]: epoch 369: training loss 0.0539\n",
      "2025-05-29 21:02:17 [INFO]: epoch 370: training loss 0.0373\n",
      "2025-05-29 21:02:17 [INFO]: epoch 371: training loss 0.0569\n",
      "2025-05-29 21:02:17 [INFO]: epoch 372: training loss 0.0647\n",
      "2025-05-29 21:02:17 [INFO]: epoch 373: training loss 0.0514\n",
      "2025-05-29 21:02:17 [INFO]: epoch 374: training loss 0.0449\n",
      "2025-05-29 21:02:17 [INFO]: epoch 375: training loss 0.0534\n",
      "2025-05-29 21:02:17 [INFO]: epoch 376: training loss 0.0578\n",
      "2025-05-29 21:02:17 [INFO]: epoch 377: training loss 0.0512\n",
      "2025-05-29 21:02:17 [INFO]: epoch 378: training loss 0.0484\n",
      "2025-05-29 21:02:17 [INFO]: epoch 379: training loss 0.0510\n",
      "2025-05-29 21:02:17 [INFO]: epoch 380: training loss 0.0441\n",
      "2025-05-29 21:02:17 [INFO]: epoch 381: training loss 0.0540\n",
      "2025-05-29 21:02:17 [INFO]: epoch 382: training loss 0.0506\n",
      "2025-05-29 21:02:17 [INFO]: epoch 383: training loss 0.0548\n",
      "2025-05-29 21:02:17 [INFO]: epoch 384: training loss 0.0408\n",
      "2025-05-29 21:02:17 [INFO]: epoch 385: training loss 0.0486\n",
      "2025-05-29 21:02:17 [INFO]: epoch 386: training loss 0.0415\n",
      "2025-05-29 21:02:18 [INFO]: epoch 387: training loss 0.0532\n",
      "2025-05-29 21:02:18 [INFO]: epoch 388: training loss 0.0512\n",
      "2025-05-29 21:02:18 [INFO]: epoch 389: training loss 0.0434\n",
      "2025-05-29 21:02:18 [INFO]: epoch 390: training loss 0.0443\n",
      "2025-05-29 21:02:18 [INFO]: epoch 391: training loss 0.0459\n",
      "2025-05-29 21:02:18 [INFO]: epoch 392: training loss 0.0373\n",
      "2025-05-29 21:02:18 [INFO]: epoch 393: training loss 0.0423\n",
      "2025-05-29 21:02:18 [INFO]: epoch 394: training loss 0.0591\n",
      "2025-05-29 21:02:18 [INFO]: epoch 395: training loss 0.0510\n",
      "2025-05-29 21:02:18 [INFO]: epoch 396: training loss 0.0426\n",
      "2025-05-29 21:02:18 [INFO]: epoch 397: training loss 0.0510\n",
      "2025-05-29 21:02:18 [INFO]: epoch 398: training loss 0.0512\n",
      "2025-05-29 21:02:18 [INFO]: epoch 399: training loss 0.0458\n",
      "2025-05-29 21:02:18 [INFO]: epoch 400: training loss 0.0444\n",
      "2025-05-29 21:02:18 [INFO]: epoch 401: training loss 0.0440\n",
      "2025-05-29 21:02:18 [INFO]: epoch 402: training loss 0.0408\n",
      "2025-05-29 21:02:18 [INFO]: epoch 403: training loss 0.0413\n",
      "2025-05-29 21:02:18 [INFO]: epoch 404: training loss 0.0416\n",
      "2025-05-29 21:02:18 [INFO]: epoch 405: training loss 0.0351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:02:18 [INFO]: epoch 406: training loss 0.0357\n",
      "2025-05-29 21:02:18 [INFO]: epoch 407: training loss 0.0334\n",
      "2025-05-29 21:02:18 [INFO]: epoch 408: training loss 0.0364\n",
      "2025-05-29 21:02:18 [INFO]: epoch 409: training loss 0.0308\n",
      "2025-05-29 21:02:18 [INFO]: epoch 410: training loss 0.0380\n",
      "2025-05-29 21:02:18 [INFO]: epoch 411: training loss 0.0387\n",
      "2025-05-29 21:02:18 [INFO]: epoch 412: training loss 0.0400\n",
      "2025-05-29 21:02:18 [INFO]: epoch 413: training loss 0.0417\n",
      "2025-05-29 21:02:18 [INFO]: epoch 414: training loss 0.0291\n",
      "2025-05-29 21:02:18 [INFO]: epoch 415: training loss 0.0361\n",
      "2025-05-29 21:02:18 [INFO]: epoch 416: training loss 0.0352\n",
      "2025-05-29 21:02:18 [INFO]: epoch 417: training loss 0.0332\n",
      "2025-05-29 21:02:18 [INFO]: epoch 418: training loss 0.0391\n",
      "2025-05-29 21:02:18 [INFO]: epoch 419: training loss 0.0332\n",
      "2025-05-29 21:02:18 [INFO]: epoch 420: training loss 0.0379\n",
      "2025-05-29 21:02:18 [INFO]: epoch 421: training loss 0.0430\n",
      "2025-05-29 21:02:18 [INFO]: epoch 422: training loss 0.0323\n",
      "2025-05-29 21:02:18 [INFO]: epoch 423: training loss 0.0396\n",
      "2025-05-29 21:02:18 [INFO]: epoch 424: training loss 0.0396\n",
      "2025-05-29 21:02:18 [INFO]: epoch 425: training loss 0.0327\n",
      "2025-05-29 21:02:18 [INFO]: epoch 426: training loss 0.0347\n",
      "2025-05-29 21:02:18 [INFO]: epoch 427: training loss 0.0425\n",
      "2025-05-29 21:02:18 [INFO]: epoch 428: training loss 0.0347\n",
      "2025-05-29 21:02:18 [INFO]: epoch 429: training loss 0.0325\n",
      "2025-05-29 21:02:18 [INFO]: epoch 430: training loss 0.0309\n",
      "2025-05-29 21:02:18 [INFO]: epoch 431: training loss 0.0386\n",
      "2025-05-29 21:02:18 [INFO]: epoch 432: training loss 0.0347\n",
      "2025-05-29 21:02:18 [INFO]: epoch 433: training loss 0.0366\n",
      "2025-05-29 21:02:18 [INFO]: epoch 434: training loss 0.0342\n",
      "2025-05-29 21:02:18 [INFO]: epoch 435: training loss 0.0403\n",
      "2025-05-29 21:02:18 [INFO]: epoch 436: training loss 0.0306\n",
      "2025-05-29 21:02:18 [INFO]: epoch 437: training loss 0.0367\n",
      "2025-05-29 21:02:18 [INFO]: epoch 438: training loss 0.0299\n",
      "2025-05-29 21:02:18 [INFO]: epoch 439: training loss 0.0334\n",
      "2025-05-29 21:02:18 [INFO]: epoch 440: training loss 0.0352\n",
      "2025-05-29 21:02:18 [INFO]: epoch 441: training loss 0.0327\n",
      "2025-05-29 21:02:18 [INFO]: epoch 442: training loss 0.0355\n",
      "2025-05-29 21:02:18 [INFO]: epoch 443: training loss 0.0429\n",
      "2025-05-29 21:02:18 [INFO]: epoch 444: training loss 0.0281\n",
      "2025-05-29 21:02:18 [INFO]: epoch 445: training loss 0.0399\n",
      "2025-05-29 21:02:18 [INFO]: epoch 446: training loss 0.0274\n",
      "2025-05-29 21:02:18 [INFO]: epoch 447: training loss 0.0316\n",
      "2025-05-29 21:02:18 [INFO]: epoch 448: training loss 0.0322\n",
      "2025-05-29 21:02:18 [INFO]: epoch 449: training loss 0.0369\n",
      "2025-05-29 21:02:18 [INFO]: epoch 450: training loss 0.0265\n",
      "2025-05-29 21:02:18 [INFO]: epoch 451: training loss 0.0300\n",
      "2025-05-29 21:02:18 [INFO]: epoch 452: training loss 0.0287\n",
      "2025-05-29 21:02:18 [INFO]: epoch 453: training loss 0.0351\n",
      "2025-05-29 21:02:18 [INFO]: epoch 454: training loss 0.0305\n",
      "2025-05-29 21:02:18 [INFO]: epoch 455: training loss 0.0342\n",
      "2025-05-29 21:02:18 [INFO]: epoch 456: training loss 0.0348\n",
      "2025-05-29 21:02:18 [INFO]: epoch 457: training loss 0.0336\n",
      "2025-05-29 21:02:18 [INFO]: epoch 458: training loss 0.0370\n",
      "2025-05-29 21:02:18 [INFO]: epoch 459: training loss 0.0367\n",
      "2025-05-29 21:02:18 [INFO]: epoch 460: training loss 0.0302\n",
      "2025-05-29 21:02:18 [INFO]: epoch 461: training loss 0.0299\n",
      "2025-05-29 21:02:18 [INFO]: epoch 462: training loss 0.0329\n",
      "2025-05-29 21:02:18 [INFO]: epoch 463: training loss 0.0360\n",
      "2025-05-29 21:02:18 [INFO]: epoch 464: training loss 0.0335\n",
      "2025-05-29 21:02:18 [INFO]: epoch 465: training loss 0.0354\n",
      "2025-05-29 21:02:18 [INFO]: epoch 466: training loss 0.0338\n",
      "2025-05-29 21:02:19 [INFO]: epoch 467: training loss 0.0319\n",
      "2025-05-29 21:02:19 [INFO]: epoch 468: training loss 0.0318\n",
      "2025-05-29 21:02:19 [INFO]: epoch 469: training loss 0.0376\n",
      "2025-05-29 21:02:19 [INFO]: epoch 470: training loss 0.0351\n",
      "2025-05-29 21:02:19 [INFO]: epoch 471: training loss 0.0328\n",
      "2025-05-29 21:02:19 [INFO]: epoch 472: training loss 0.0462\n",
      "2025-05-29 21:02:19 [INFO]: epoch 473: training loss 0.0419\n",
      "2025-05-29 21:02:19 [INFO]: epoch 474: training loss 0.0345\n",
      "2025-05-29 21:02:19 [INFO]: epoch 475: training loss 0.0451\n",
      "2025-05-29 21:02:19 [INFO]: epoch 476: training loss 0.0408\n",
      "2025-05-29 21:02:19 [INFO]: epoch 477: training loss 0.0333\n",
      "2025-05-29 21:02:19 [INFO]: epoch 478: training loss 0.0453\n",
      "2025-05-29 21:02:19 [INFO]: epoch 479: training loss 0.0428\n",
      "2025-05-29 21:02:19 [INFO]: epoch 480: training loss 0.0339\n",
      "2025-05-29 21:02:19 [INFO]: epoch 481: training loss 0.0361\n",
      "2025-05-29 21:02:19 [INFO]: epoch 482: training loss 0.0501\n",
      "2025-05-29 21:02:19 [INFO]: epoch 483: training loss 0.0355\n",
      "2025-05-29 21:02:19 [INFO]: epoch 484: training loss 0.0412\n",
      "2025-05-29 21:02:19 [INFO]: epoch 485: training loss 0.0395\n",
      "2025-05-29 21:02:19 [INFO]: epoch 486: training loss 0.0316\n",
      "2025-05-29 21:02:19 [INFO]: epoch 487: training loss 0.0329\n",
      "2025-05-29 21:02:19 [INFO]: epoch 488: training loss 0.0406\n",
      "2025-05-29 21:02:19 [INFO]: epoch 489: training loss 0.0427\n",
      "2025-05-29 21:02:19 [INFO]: epoch 490: training loss 0.0429\n",
      "2025-05-29 21:02:19 [INFO]: epoch 491: training loss 0.0497\n",
      "2025-05-29 21:02:19 [INFO]: epoch 492: training loss 0.0377\n",
      "2025-05-29 21:02:19 [INFO]: epoch 493: training loss 0.0333\n",
      "2025-05-29 21:02:19 [INFO]: epoch 494: training loss 0.0365\n",
      "2025-05-29 21:02:19 [INFO]: epoch 495: training loss 0.0371\n",
      "2025-05-29 21:02:19 [INFO]: epoch 496: training loss 0.0318\n",
      "2025-05-29 21:02:19 [INFO]: epoch 497: training loss 0.0295\n",
      "2025-05-29 21:02:19 [INFO]: epoch 498: training loss 0.0369\n",
      "2025-05-29 21:02:19 [INFO]: epoch 499: training loss 0.0354\n",
      "2025-05-29 21:02:19 [INFO]: epoch 500: training loss 0.0302\n",
      "2025-05-29 21:02:19 [INFO]: epoch 501: training loss 0.0325\n",
      "2025-05-29 21:02:19 [INFO]: epoch 502: training loss 0.0315\n",
      "2025-05-29 21:02:19 [INFO]: epoch 503: training loss 0.0347\n",
      "2025-05-29 21:02:19 [INFO]: epoch 504: training loss 0.0365\n",
      "2025-05-29 21:02:19 [INFO]: epoch 505: training loss 0.0330\n",
      "2025-05-29 21:02:19 [INFO]: epoch 506: training loss 0.0318\n",
      "2025-05-29 21:02:19 [INFO]: epoch 507: training loss 0.0308\n",
      "2025-05-29 21:02:19 [INFO]: epoch 508: training loss 0.0345\n",
      "2025-05-29 21:02:19 [INFO]: epoch 509: training loss 0.0312\n",
      "2025-05-29 21:02:19 [INFO]: epoch 510: training loss 0.0319\n",
      "2025-05-29 21:02:19 [INFO]: epoch 511: training loss 0.0305\n",
      "2025-05-29 21:02:19 [INFO]: epoch 512: training loss 0.0365\n",
      "2025-05-29 21:02:19 [INFO]: epoch 513: training loss 0.0329\n",
      "2025-05-29 21:02:19 [INFO]: epoch 514: training loss 0.0328\n",
      "2025-05-29 21:02:19 [INFO]: epoch 515: training loss 0.0296\n",
      "2025-05-29 21:02:19 [INFO]: epoch 516: training loss 0.0312\n",
      "2025-05-29 21:02:19 [INFO]: epoch 517: training loss 0.0358\n",
      "2025-05-29 21:02:19 [INFO]: epoch 518: training loss 0.0374\n",
      "2025-05-29 21:02:19 [INFO]: epoch 519: training loss 0.0284\n",
      "2025-05-29 21:02:19 [INFO]: epoch 520: training loss 0.0334\n",
      "2025-05-29 21:02:19 [INFO]: epoch 521: training loss 0.0284\n",
      "2025-05-29 21:02:19 [INFO]: epoch 522: training loss 0.0235\n",
      "2025-05-29 21:02:19 [INFO]: epoch 523: training loss 0.0274\n",
      "2025-05-29 21:02:19 [INFO]: epoch 524: training loss 0.0323\n",
      "2025-05-29 21:02:19 [INFO]: epoch 525: training loss 0.0337\n",
      "2025-05-29 21:02:19 [INFO]: epoch 526: training loss 0.0321\n",
      "2025-05-29 21:02:19 [INFO]: epoch 527: training loss 0.0281\n",
      "2025-05-29 21:02:19 [INFO]: epoch 528: training loss 0.0337\n",
      "2025-05-29 21:02:19 [INFO]: epoch 529: training loss 0.0344\n",
      "2025-05-29 21:02:19 [INFO]: epoch 530: training loss 0.0309\n",
      "2025-05-29 21:02:19 [INFO]: epoch 531: training loss 0.0300\n",
      "2025-05-29 21:02:19 [INFO]: epoch 532: training loss 0.0285\n",
      "2025-05-29 21:02:19 [INFO]: epoch 533: training loss 0.0300\n",
      "2025-05-29 21:02:19 [INFO]: epoch 534: training loss 0.0276\n",
      "2025-05-29 21:02:19 [INFO]: epoch 535: training loss 0.0319\n",
      "2025-05-29 21:02:19 [INFO]: epoch 536: training loss 0.0241\n",
      "2025-05-29 21:02:19 [INFO]: epoch 537: training loss 0.0263\n",
      "2025-05-29 21:02:19 [INFO]: epoch 538: training loss 0.0271\n",
      "2025-05-29 21:02:19 [INFO]: epoch 539: training loss 0.0331\n",
      "2025-05-29 21:02:19 [INFO]: epoch 540: training loss 0.0316\n",
      "2025-05-29 21:02:19 [INFO]: epoch 541: training loss 0.0283\n",
      "2025-05-29 21:02:19 [INFO]: epoch 542: training loss 0.0333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:02:19 [INFO]: epoch 543: training loss 0.0293\n",
      "2025-05-29 21:02:19 [INFO]: epoch 544: training loss 0.0315\n",
      "2025-05-29 21:02:19 [INFO]: epoch 545: training loss 0.0292\n",
      "2025-05-29 21:02:19 [INFO]: epoch 546: training loss 0.0270\n",
      "2025-05-29 21:02:19 [INFO]: epoch 547: training loss 0.0304\n",
      "2025-05-29 21:02:20 [INFO]: epoch 548: training loss 0.0275\n",
      "2025-05-29 21:02:20 [INFO]: epoch 549: training loss 0.0270\n",
      "2025-05-29 21:02:20 [INFO]: epoch 550: training loss 0.0284\n",
      "2025-05-29 21:02:20 [INFO]: epoch 551: training loss 0.0304\n",
      "2025-05-29 21:02:20 [INFO]: epoch 552: training loss 0.0311\n",
      "2025-05-29 21:02:20 [INFO]: epoch 553: training loss 0.0260\n",
      "2025-05-29 21:02:20 [INFO]: epoch 554: training loss 0.0259\n",
      "2025-05-29 21:02:20 [INFO]: epoch 555: training loss 0.0287\n",
      "2025-05-29 21:02:20 [INFO]: epoch 556: training loss 0.0264\n",
      "2025-05-29 21:02:20 [INFO]: epoch 557: training loss 0.0256\n",
      "2025-05-29 21:02:20 [INFO]: epoch 558: training loss 0.0263\n",
      "2025-05-29 21:02:20 [INFO]: epoch 559: training loss 0.0288\n",
      "2025-05-29 21:02:20 [INFO]: epoch 560: training loss 0.0316\n",
      "2025-05-29 21:02:20 [INFO]: epoch 561: training loss 0.0250\n",
      "2025-05-29 21:02:20 [INFO]: epoch 562: training loss 0.0313\n",
      "2025-05-29 21:02:20 [INFO]: epoch 563: training loss 0.0291\n",
      "2025-05-29 21:02:20 [INFO]: epoch 564: training loss 0.0314\n",
      "2025-05-29 21:02:20 [INFO]: epoch 565: training loss 0.0285\n",
      "2025-05-29 21:02:20 [INFO]: epoch 566: training loss 0.0291\n",
      "2025-05-29 21:02:20 [INFO]: epoch 567: training loss 0.0304\n",
      "2025-05-29 21:02:20 [INFO]: epoch 568: training loss 0.0281\n",
      "2025-05-29 21:02:20 [INFO]: epoch 569: training loss 0.0274\n",
      "2025-05-29 21:02:20 [INFO]: epoch 570: training loss 0.0320\n",
      "2025-05-29 21:02:20 [INFO]: epoch 571: training loss 0.0277\n",
      "2025-05-29 21:02:20 [INFO]: epoch 572: training loss 0.0270\n",
      "2025-05-29 21:02:20 [INFO]: epoch 573: training loss 0.0340\n",
      "2025-05-29 21:02:20 [INFO]: epoch 574: training loss 0.0319\n",
      "2025-05-29 21:02:20 [INFO]: epoch 575: training loss 0.0351\n",
      "2025-05-29 21:02:20 [INFO]: epoch 576: training loss 0.0298\n",
      "2025-05-29 21:02:20 [INFO]: epoch 577: training loss 0.0248\n",
      "2025-05-29 21:02:20 [INFO]: epoch 578: training loss 0.0300\n",
      "2025-05-29 21:02:20 [INFO]: epoch 579: training loss 0.0260\n",
      "2025-05-29 21:02:20 [INFO]: epoch 580: training loss 0.0268\n",
      "2025-05-29 21:02:20 [INFO]: epoch 581: training loss 0.0277\n",
      "2025-05-29 21:02:20 [INFO]: epoch 582: training loss 0.0299\n",
      "2025-05-29 21:02:20 [INFO]: epoch 583: training loss 0.0259\n",
      "2025-05-29 21:02:20 [INFO]: epoch 584: training loss 0.0321\n",
      "2025-05-29 21:02:20 [INFO]: epoch 585: training loss 0.0293\n",
      "2025-05-29 21:02:20 [INFO]: epoch 586: training loss 0.0269\n",
      "2025-05-29 21:02:20 [INFO]: epoch 587: training loss 0.0323\n",
      "2025-05-29 21:02:20 [INFO]: epoch 588: training loss 0.0296\n",
      "2025-05-29 21:02:20 [INFO]: epoch 589: training loss 0.0270\n",
      "2025-05-29 21:02:20 [INFO]: epoch 590: training loss 0.0255\n",
      "2025-05-29 21:02:20 [INFO]: epoch 591: training loss 0.0247\n",
      "2025-05-29 21:02:20 [INFO]: epoch 592: training loss 0.0347\n",
      "2025-05-29 21:02:20 [INFO]: epoch 593: training loss 0.0239\n",
      "2025-05-29 21:02:20 [INFO]: epoch 594: training loss 0.0255\n",
      "2025-05-29 21:02:20 [INFO]: epoch 595: training loss 0.0290\n",
      "2025-05-29 21:02:20 [INFO]: epoch 596: training loss 0.0311\n",
      "2025-05-29 21:02:20 [INFO]: epoch 597: training loss 0.0266\n",
      "2025-05-29 21:02:20 [INFO]: epoch 598: training loss 0.0262\n",
      "2025-05-29 21:02:20 [INFO]: epoch 599: training loss 0.0303\n",
      "2025-05-29 21:02:20 [INFO]: Finished training.\n",
      "2025-05-29 21:02:20 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:37<00:00,  7.53s/it]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "imputation_list_1 = []\n",
    "imputation_list_2 = []\n",
    "\n",
    "for model in range(1,3):\n",
    "    \n",
    "    if model ==1:\n",
    "        epoch = 300\n",
    "    else:\n",
    "        epoch = 600\n",
    "        \n",
    "    for i in tqdm( range(5) ):             # 运行5次计算标准差\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        saits = SAITS(n_steps=30, n_features=6, n_layers=2, d_model=128, d_inner=64, n_heads=4, d_k=64, d_v=64, dropout=0.1, batch_size=10, epochs = epoch)\n",
    "\n",
    "        dataset = {\"X\": X_missed_1}\n",
    "\n",
    "        saits.fit(dataset)               \n",
    "        imputation = saits.impute(dataset)  \n",
    "        \n",
    "        if model ==1:\n",
    "            imputation_list_1.append(imputation)\n",
    "        else:\n",
    "            imputation_list_2.append(imputation)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1fdf364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "(5, 30, 6)\n",
      "(5, 30, 6)\n"
     ]
    }
   ],
   "source": [
    "print(len(imputation_list_1))\n",
    "print(len(imputation_list_2))\n",
    "\n",
    "imp_1 = np.array(imputation_list_1).reshape((5,30,6)) \n",
    "print(imp_1.shape)\n",
    "\n",
    "imp_2 = np.array(imputation_list_2).reshape((5,30,6)) \n",
    "print(imp_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "18465988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 6)\n",
      "(30, 6)\n"
     ]
    }
   ],
   "source": [
    "res_1 = torch.from_numpy(imp_1)\n",
    "\n",
    "res1_005 = torch.quantile( res_1, 0.05, dim=0 ).cpu().numpy()\n",
    "res1_05 = torch.quantile( res_1, 0.5, dim=0 ).cpu().numpy()\n",
    "res1_095 = torch.quantile( res_1, 0.95, dim=0 ).cpu().numpy()\n",
    "res1_x = torch.quantile( res_1, 0.8, dim=0 ).cpu().numpy()\n",
    "print(res1_005.shape)\n",
    "\n",
    "\n",
    "\n",
    "res_2 = torch.from_numpy(imp_2)\n",
    "\n",
    "res2_005 = torch.quantile( res_2, 0.05, dim=0 ).cpu().numpy()\n",
    "res2_05 = torch.quantile( res_2, 0.5, dim=0 ).cpu().numpy()\n",
    "res2_095 = torch.quantile( res_2, 0.95, dim=0 ).cpu().numpy()\n",
    "res2_x = torch.quantile( res_2, 0.7, dim=0 ).cpu().numpy()\n",
    "print(res2_005.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "114541b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18ac7d12fd0>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(7,4))\n",
    "\n",
    "X_missed = X_missed_1\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(res2_x[:,3], c='k', label='imp', alpha=0.8) \n",
    "plt.fill_between( range(0,30), res1_005[:,3], res1_095[:,3], color='#ff7f0e', alpha=0.4)  # alpha是透明程度\n",
    "plt.fill_between( range(0,30), res2_005[:,3], res2_095[:,3], color='#ff7f0e', alpha=0.8)  \n",
    "plt.plot(X_missed[0][:,3].cpu().numpy(), c='#1f77b4', marker='o',alpha=1, markersize='4') # 观测值的蓝色把之前的值覆盖了 生成区域的值还是之前的0.5分位数的\n",
    "\n",
    "\n",
    "# plt.savefig('分位数',bbox_inches='tight',dpi=500)\n",
    "# plt.savefig('分位数.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cb728b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 30, 6])\n",
      "tensor([0.5595,    nan,    nan, 0.5710, 0.5679,    nan], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "'第二个样本'\n",
    "zero_shot = np.array(norm_data[80:110])    # 40-70     seed1 100-130\n",
    "zero_shot = torch.from_numpy(zero_shot).float().cuda()\n",
    "zero_shot = zero_shot.unsqueeze(dim=0)\n",
    "print(zero_shot.shape)\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(5)\n",
    "# torch.manual_seed(4)\n",
    "# torch.cuda.manual_seed_all(4)\n",
    "\n",
    "X_ori, X_missed_2, missing_mask, indicating_mask = mcar(zero_shot, 0.3, np.nan)\n",
    "print(X_missed[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4910faff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]2025-05-29 21:03:40 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:03:40 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:03:40 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:03:40 [INFO]: epoch 0: training loss 0.7945\n",
      "2025-05-29 21:03:40 [INFO]: epoch 1: training loss 0.7691\n",
      "2025-05-29 21:03:40 [INFO]: epoch 2: training loss 0.7315\n",
      "2025-05-29 21:03:40 [INFO]: epoch 3: training loss 0.5424\n",
      "2025-05-29 21:03:40 [INFO]: epoch 4: training loss 0.4961\n",
      "2025-05-29 21:03:40 [INFO]: epoch 5: training loss 0.4744\n",
      "2025-05-29 21:03:40 [INFO]: epoch 6: training loss 0.4140\n",
      "2025-05-29 21:03:40 [INFO]: epoch 7: training loss 0.4753\n",
      "2025-05-29 21:03:40 [INFO]: epoch 8: training loss 0.4475\n",
      "2025-05-29 21:03:40 [INFO]: epoch 9: training loss 0.4104\n",
      "2025-05-29 21:03:40 [INFO]: epoch 10: training loss 0.3370\n",
      "2025-05-29 21:03:40 [INFO]: epoch 11: training loss 0.3445\n",
      "2025-05-29 21:03:40 [INFO]: epoch 12: training loss 0.3446\n",
      "2025-05-29 21:03:40 [INFO]: epoch 13: training loss 0.3661\n",
      "2025-05-29 21:03:40 [INFO]: epoch 14: training loss 0.3505\n",
      "2025-05-29 21:03:40 [INFO]: epoch 15: training loss 0.3660\n",
      "2025-05-29 21:03:40 [INFO]: epoch 16: training loss 0.2949\n",
      "2025-05-29 21:03:40 [INFO]: epoch 17: training loss 0.2955\n",
      "2025-05-29 21:03:40 [INFO]: epoch 18: training loss 0.2854\n",
      "2025-05-29 21:03:40 [INFO]: epoch 19: training loss 0.2964\n",
      "2025-05-29 21:03:40 [INFO]: epoch 20: training loss 0.3102\n",
      "2025-05-29 21:03:40 [INFO]: epoch 21: training loss 0.2835\n",
      "2025-05-29 21:03:41 [INFO]: epoch 22: training loss 0.2699\n",
      "2025-05-29 21:03:41 [INFO]: epoch 23: training loss 0.2689\n",
      "2025-05-29 21:03:41 [INFO]: epoch 24: training loss 0.2600\n",
      "2025-05-29 21:03:41 [INFO]: epoch 25: training loss 0.2372\n",
      "2025-05-29 21:03:41 [INFO]: epoch 26: training loss 0.2684\n",
      "2025-05-29 21:03:41 [INFO]: epoch 27: training loss 0.2773\n",
      "2025-05-29 21:03:41 [INFO]: epoch 28: training loss 0.2453\n",
      "2025-05-29 21:03:41 [INFO]: epoch 29: training loss 0.2661\n",
      "2025-05-29 21:03:41 [INFO]: epoch 30: training loss 0.3299\n",
      "2025-05-29 21:03:41 [INFO]: epoch 31: training loss 0.2779\n",
      "2025-05-29 21:03:41 [INFO]: epoch 32: training loss 0.2084\n",
      "2025-05-29 21:03:41 [INFO]: epoch 33: training loss 0.2342\n",
      "2025-05-29 21:03:41 [INFO]: epoch 34: training loss 0.2529\n",
      "2025-05-29 21:03:41 [INFO]: epoch 35: training loss 0.2395\n",
      "2025-05-29 21:03:41 [INFO]: epoch 36: training loss 0.2478\n",
      "2025-05-29 21:03:41 [INFO]: epoch 37: training loss 0.2329\n",
      "2025-05-29 21:03:41 [INFO]: epoch 38: training loss 0.2275\n",
      "2025-05-29 21:03:41 [INFO]: epoch 39: training loss 0.2349\n",
      "2025-05-29 21:03:41 [INFO]: epoch 40: training loss 0.2413\n",
      "2025-05-29 21:03:41 [INFO]: epoch 41: training loss 0.2464\n",
      "2025-05-29 21:03:41 [INFO]: epoch 42: training loss 0.2330\n",
      "2025-05-29 21:03:41 [INFO]: epoch 43: training loss 0.2226\n",
      "2025-05-29 21:03:41 [INFO]: epoch 44: training loss 0.2197\n",
      "2025-05-29 21:03:41 [INFO]: epoch 45: training loss 0.2077\n",
      "2025-05-29 21:03:41 [INFO]: epoch 46: training loss 0.2322\n",
      "2025-05-29 21:03:41 [INFO]: epoch 47: training loss 0.2002\n",
      "2025-05-29 21:03:41 [INFO]: epoch 48: training loss 0.2033\n",
      "2025-05-29 21:03:41 [INFO]: epoch 49: training loss 0.1844\n",
      "2025-05-29 21:03:41 [INFO]: epoch 50: training loss 0.2021\n",
      "2025-05-29 21:03:41 [INFO]: epoch 51: training loss 0.1970\n",
      "2025-05-29 21:03:41 [INFO]: epoch 52: training loss 0.1914\n",
      "2025-05-29 21:03:41 [INFO]: epoch 53: training loss 0.1954\n",
      "2025-05-29 21:03:41 [INFO]: epoch 54: training loss 0.1786\n",
      "2025-05-29 21:03:41 [INFO]: epoch 55: training loss 0.1880\n",
      "2025-05-29 21:03:41 [INFO]: epoch 56: training loss 0.1910\n",
      "2025-05-29 21:03:41 [INFO]: epoch 57: training loss 0.1833\n",
      "2025-05-29 21:03:41 [INFO]: epoch 58: training loss 0.1806\n",
      "2025-05-29 21:03:41 [INFO]: epoch 59: training loss 0.1743\n",
      "2025-05-29 21:03:41 [INFO]: epoch 60: training loss 0.1866\n",
      "2025-05-29 21:03:41 [INFO]: epoch 61: training loss 0.1790\n",
      "2025-05-29 21:03:41 [INFO]: epoch 62: training loss 0.1793\n",
      "2025-05-29 21:03:41 [INFO]: epoch 63: training loss 0.1614\n",
      "2025-05-29 21:03:41 [INFO]: epoch 64: training loss 0.2234\n",
      "2025-05-29 21:03:41 [INFO]: epoch 65: training loss 0.2213\n",
      "2025-05-29 21:03:41 [INFO]: epoch 66: training loss 0.1629\n",
      "2025-05-29 21:03:41 [INFO]: epoch 67: training loss 0.1657\n",
      "2025-05-29 21:03:41 [INFO]: epoch 68: training loss 0.1662\n",
      "2025-05-29 21:03:41 [INFO]: epoch 69: training loss 0.1934\n",
      "2025-05-29 21:03:41 [INFO]: epoch 70: training loss 0.1822\n",
      "2025-05-29 21:03:41 [INFO]: epoch 71: training loss 0.1793\n",
      "2025-05-29 21:03:41 [INFO]: epoch 72: training loss 0.1607\n",
      "2025-05-29 21:03:41 [INFO]: epoch 73: training loss 0.1744\n",
      "2025-05-29 21:03:41 [INFO]: epoch 74: training loss 0.1619\n",
      "2025-05-29 21:03:41 [INFO]: epoch 75: training loss 0.1673\n",
      "2025-05-29 21:03:41 [INFO]: epoch 76: training loss 0.1886\n",
      "2025-05-29 21:03:41 [INFO]: epoch 77: training loss 0.1730\n",
      "2025-05-29 21:03:41 [INFO]: epoch 78: training loss 0.1454\n",
      "2025-05-29 21:03:41 [INFO]: epoch 79: training loss 0.1687\n",
      "2025-05-29 21:03:41 [INFO]: epoch 80: training loss 0.1867\n",
      "2025-05-29 21:03:41 [INFO]: epoch 81: training loss 0.1865\n",
      "2025-05-29 21:03:41 [INFO]: epoch 82: training loss 0.1402\n",
      "2025-05-29 21:03:41 [INFO]: epoch 83: training loss 0.1634\n",
      "2025-05-29 21:03:41 [INFO]: epoch 84: training loss 0.1955\n",
      "2025-05-29 21:03:41 [INFO]: epoch 85: training loss 0.1637\n",
      "2025-05-29 21:03:41 [INFO]: epoch 86: training loss 0.1652\n",
      "2025-05-29 21:03:41 [INFO]: epoch 87: training loss 0.1506\n",
      "2025-05-29 21:03:41 [INFO]: epoch 88: training loss 0.1722\n",
      "2025-05-29 21:03:41 [INFO]: epoch 89: training loss 0.1520\n",
      "2025-05-29 21:03:41 [INFO]: epoch 90: training loss 0.1635\n",
      "2025-05-29 21:03:41 [INFO]: epoch 91: training loss 0.1574\n",
      "2025-05-29 21:03:41 [INFO]: epoch 92: training loss 0.1599\n",
      "2025-05-29 21:03:41 [INFO]: epoch 93: training loss 0.1687\n",
      "2025-05-29 21:03:41 [INFO]: epoch 94: training loss 0.1552\n",
      "2025-05-29 21:03:41 [INFO]: epoch 95: training loss 0.1385\n",
      "2025-05-29 21:03:41 [INFO]: epoch 96: training loss 0.1420\n",
      "2025-05-29 21:03:41 [INFO]: epoch 97: training loss 0.1307\n",
      "2025-05-29 21:03:41 [INFO]: epoch 98: training loss 0.1265\n",
      "2025-05-29 21:03:41 [INFO]: epoch 99: training loss 0.1200\n",
      "2025-05-29 21:03:41 [INFO]: epoch 100: training loss 0.1299\n",
      "2025-05-29 21:03:41 [INFO]: epoch 101: training loss 0.1424\n",
      "2025-05-29 21:03:41 [INFO]: epoch 102: training loss 0.1295\n",
      "2025-05-29 21:03:42 [INFO]: epoch 103: training loss 0.1481\n",
      "2025-05-29 21:03:42 [INFO]: epoch 104: training loss 0.1526\n",
      "2025-05-29 21:03:42 [INFO]: epoch 105: training loss 0.1578\n",
      "2025-05-29 21:03:42 [INFO]: epoch 106: training loss 0.1342\n",
      "2025-05-29 21:03:42 [INFO]: epoch 107: training loss 0.1461\n",
      "2025-05-29 21:03:42 [INFO]: epoch 108: training loss 0.1139\n",
      "2025-05-29 21:03:42 [INFO]: epoch 109: training loss 0.1174\n",
      "2025-05-29 21:03:42 [INFO]: epoch 110: training loss 0.1245\n",
      "2025-05-29 21:03:42 [INFO]: epoch 111: training loss 0.1305\n",
      "2025-05-29 21:03:42 [INFO]: epoch 112: training loss 0.1153\n",
      "2025-05-29 21:03:42 [INFO]: epoch 113: training loss 0.1162\n",
      "2025-05-29 21:03:42 [INFO]: epoch 114: training loss 0.1217\n",
      "2025-05-29 21:03:42 [INFO]: epoch 115: training loss 0.1252\n",
      "2025-05-29 21:03:42 [INFO]: epoch 116: training loss 0.1175\n",
      "2025-05-29 21:03:42 [INFO]: epoch 117: training loss 0.1202\n",
      "2025-05-29 21:03:42 [INFO]: epoch 118: training loss 0.1102\n",
      "2025-05-29 21:03:42 [INFO]: epoch 119: training loss 0.1150\n",
      "2025-05-29 21:03:42 [INFO]: epoch 120: training loss 0.1094\n",
      "2025-05-29 21:03:42 [INFO]: epoch 121: training loss 0.1147\n",
      "2025-05-29 21:03:42 [INFO]: epoch 122: training loss 0.1094\n",
      "2025-05-29 21:03:42 [INFO]: epoch 123: training loss 0.1034\n",
      "2025-05-29 21:03:42 [INFO]: epoch 124: training loss 0.1057\n",
      "2025-05-29 21:03:42 [INFO]: epoch 125: training loss 0.1142\n",
      "2025-05-29 21:03:42 [INFO]: epoch 126: training loss 0.1089\n",
      "2025-05-29 21:03:42 [INFO]: epoch 127: training loss 0.1190\n",
      "2025-05-29 21:03:42 [INFO]: epoch 128: training loss 0.1270\n",
      "2025-05-29 21:03:42 [INFO]: epoch 129: training loss 0.0922\n",
      "2025-05-29 21:03:42 [INFO]: epoch 130: training loss 0.1178\n",
      "2025-05-29 21:03:42 [INFO]: epoch 131: training loss 0.1144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:03:42 [INFO]: epoch 132: training loss 0.1095\n",
      "2025-05-29 21:03:42 [INFO]: epoch 133: training loss 0.0949\n",
      "2025-05-29 21:03:42 [INFO]: epoch 134: training loss 0.1140\n",
      "2025-05-29 21:03:42 [INFO]: epoch 135: training loss 0.1198\n",
      "2025-05-29 21:03:42 [INFO]: epoch 136: training loss 0.1047\n",
      "2025-05-29 21:03:42 [INFO]: epoch 137: training loss 0.1015\n",
      "2025-05-29 21:03:42 [INFO]: epoch 138: training loss 0.1088\n",
      "2025-05-29 21:03:42 [INFO]: epoch 139: training loss 0.1046\n",
      "2025-05-29 21:03:42 [INFO]: epoch 140: training loss 0.1055\n",
      "2025-05-29 21:03:42 [INFO]: epoch 141: training loss 0.1070\n",
      "2025-05-29 21:03:42 [INFO]: epoch 142: training loss 0.0914\n",
      "2025-05-29 21:03:42 [INFO]: epoch 143: training loss 0.1029\n",
      "2025-05-29 21:03:42 [INFO]: epoch 144: training loss 0.0959\n",
      "2025-05-29 21:03:42 [INFO]: epoch 145: training loss 0.1022\n",
      "2025-05-29 21:03:42 [INFO]: epoch 146: training loss 0.0956\n",
      "2025-05-29 21:03:42 [INFO]: epoch 147: training loss 0.0965\n",
      "2025-05-29 21:03:42 [INFO]: epoch 148: training loss 0.0796\n",
      "2025-05-29 21:03:42 [INFO]: epoch 149: training loss 0.0941\n",
      "2025-05-29 21:03:42 [INFO]: epoch 150: training loss 0.1058\n",
      "2025-05-29 21:03:42 [INFO]: epoch 151: training loss 0.1095\n",
      "2025-05-29 21:03:42 [INFO]: epoch 152: training loss 0.1002\n",
      "2025-05-29 21:03:42 [INFO]: epoch 153: training loss 0.0843\n",
      "2025-05-29 21:03:42 [INFO]: epoch 154: training loss 0.0916\n",
      "2025-05-29 21:03:42 [INFO]: epoch 155: training loss 0.0917\n",
      "2025-05-29 21:03:42 [INFO]: epoch 156: training loss 0.0908\n",
      "2025-05-29 21:03:42 [INFO]: epoch 157: training loss 0.1208\n",
      "2025-05-29 21:03:42 [INFO]: epoch 158: training loss 0.0915\n",
      "2025-05-29 21:03:42 [INFO]: epoch 159: training loss 0.0883\n",
      "2025-05-29 21:03:42 [INFO]: epoch 160: training loss 0.0818\n",
      "2025-05-29 21:03:42 [INFO]: epoch 161: training loss 0.1095\n",
      "2025-05-29 21:03:42 [INFO]: epoch 162: training loss 0.0960\n",
      "2025-05-29 21:03:42 [INFO]: epoch 163: training loss 0.0908\n",
      "2025-05-29 21:03:42 [INFO]: epoch 164: training loss 0.0860\n",
      "2025-05-29 21:03:42 [INFO]: epoch 165: training loss 0.0910\n",
      "2025-05-29 21:03:42 [INFO]: epoch 166: training loss 0.0843\n",
      "2025-05-29 21:03:42 [INFO]: epoch 167: training loss 0.0907\n",
      "2025-05-29 21:03:42 [INFO]: epoch 168: training loss 0.0855\n",
      "2025-05-29 21:03:42 [INFO]: epoch 169: training loss 0.0785\n",
      "2025-05-29 21:03:42 [INFO]: epoch 170: training loss 0.0823\n",
      "2025-05-29 21:03:42 [INFO]: epoch 171: training loss 0.0893\n",
      "2025-05-29 21:03:42 [INFO]: epoch 172: training loss 0.0921\n",
      "2025-05-29 21:03:42 [INFO]: epoch 173: training loss 0.0817\n",
      "2025-05-29 21:03:42 [INFO]: epoch 174: training loss 0.0888\n",
      "2025-05-29 21:03:42 [INFO]: epoch 175: training loss 0.0825\n",
      "2025-05-29 21:03:42 [INFO]: epoch 176: training loss 0.0886\n",
      "2025-05-29 21:03:42 [INFO]: epoch 177: training loss 0.0725\n",
      "2025-05-29 21:03:42 [INFO]: epoch 178: training loss 0.0792\n",
      "2025-05-29 21:03:42 [INFO]: epoch 179: training loss 0.0776\n",
      "2025-05-29 21:03:42 [INFO]: epoch 180: training loss 0.0771\n",
      "2025-05-29 21:03:42 [INFO]: epoch 181: training loss 0.0801\n",
      "2025-05-29 21:03:42 [INFO]: epoch 182: training loss 0.0897\n",
      "2025-05-29 21:03:43 [INFO]: epoch 183: training loss 0.0827\n",
      "2025-05-29 21:03:43 [INFO]: epoch 184: training loss 0.0758\n",
      "2025-05-29 21:03:43 [INFO]: epoch 185: training loss 0.0829\n",
      "2025-05-29 21:03:43 [INFO]: epoch 186: training loss 0.0722\n",
      "2025-05-29 21:03:43 [INFO]: epoch 187: training loss 0.0884\n",
      "2025-05-29 21:03:43 [INFO]: epoch 188: training loss 0.0834\n",
      "2025-05-29 21:03:43 [INFO]: epoch 189: training loss 0.0694\n",
      "2025-05-29 21:03:43 [INFO]: epoch 190: training loss 0.0777\n",
      "2025-05-29 21:03:43 [INFO]: epoch 191: training loss 0.0931\n",
      "2025-05-29 21:03:43 [INFO]: epoch 192: training loss 0.0768\n",
      "2025-05-29 21:03:43 [INFO]: epoch 193: training loss 0.0994\n",
      "2025-05-29 21:03:43 [INFO]: epoch 194: training loss 0.1014\n",
      "2025-05-29 21:03:43 [INFO]: epoch 195: training loss 0.0745\n",
      "2025-05-29 21:03:43 [INFO]: epoch 196: training loss 0.0814\n",
      "2025-05-29 21:03:43 [INFO]: epoch 197: training loss 0.0918\n",
      "2025-05-29 21:03:43 [INFO]: epoch 198: training loss 0.0771\n",
      "2025-05-29 21:03:43 [INFO]: epoch 199: training loss 0.0652\n",
      "2025-05-29 21:03:43 [INFO]: epoch 200: training loss 0.0805\n",
      "2025-05-29 21:03:43 [INFO]: epoch 201: training loss 0.0750\n",
      "2025-05-29 21:03:43 [INFO]: epoch 202: training loss 0.0799\n",
      "2025-05-29 21:03:43 [INFO]: epoch 203: training loss 0.0659\n",
      "2025-05-29 21:03:43 [INFO]: epoch 204: training loss 0.0918\n",
      "2025-05-29 21:03:43 [INFO]: epoch 205: training loss 0.0826\n",
      "2025-05-29 21:03:43 [INFO]: epoch 206: training loss 0.0771\n",
      "2025-05-29 21:03:43 [INFO]: epoch 207: training loss 0.0854\n",
      "2025-05-29 21:03:43 [INFO]: epoch 208: training loss 0.0827\n",
      "2025-05-29 21:03:43 [INFO]: epoch 209: training loss 0.0740\n",
      "2025-05-29 21:03:43 [INFO]: epoch 210: training loss 0.0667\n",
      "2025-05-29 21:03:43 [INFO]: epoch 211: training loss 0.0747\n",
      "2025-05-29 21:03:43 [INFO]: epoch 212: training loss 0.0839\n",
      "2025-05-29 21:03:43 [INFO]: epoch 213: training loss 0.0736\n",
      "2025-05-29 21:03:43 [INFO]: epoch 214: training loss 0.0688\n",
      "2025-05-29 21:03:43 [INFO]: epoch 215: training loss 0.0757\n",
      "2025-05-29 21:03:43 [INFO]: epoch 216: training loss 0.0754\n",
      "2025-05-29 21:03:43 [INFO]: epoch 217: training loss 0.0705\n",
      "2025-05-29 21:03:43 [INFO]: epoch 218: training loss 0.0608\n",
      "2025-05-29 21:03:43 [INFO]: epoch 219: training loss 0.0653\n",
      "2025-05-29 21:03:43 [INFO]: epoch 220: training loss 0.0706\n",
      "2025-05-29 21:03:43 [INFO]: epoch 221: training loss 0.0759\n",
      "2025-05-29 21:03:43 [INFO]: epoch 222: training loss 0.0592\n",
      "2025-05-29 21:03:43 [INFO]: epoch 223: training loss 0.0678\n",
      "2025-05-29 21:03:43 [INFO]: epoch 224: training loss 0.0656\n",
      "2025-05-29 21:03:43 [INFO]: epoch 225: training loss 0.0752\n",
      "2025-05-29 21:03:43 [INFO]: epoch 226: training loss 0.0881\n",
      "2025-05-29 21:03:43 [INFO]: epoch 227: training loss 0.0651\n",
      "2025-05-29 21:03:43 [INFO]: epoch 228: training loss 0.0652\n",
      "2025-05-29 21:03:43 [INFO]: epoch 229: training loss 0.0862\n",
      "2025-05-29 21:03:43 [INFO]: epoch 230: training loss 0.0764\n",
      "2025-05-29 21:03:43 [INFO]: epoch 231: training loss 0.0647\n",
      "2025-05-29 21:03:43 [INFO]: epoch 232: training loss 0.0762\n",
      "2025-05-29 21:03:43 [INFO]: epoch 233: training loss 0.0791\n",
      "2025-05-29 21:03:43 [INFO]: epoch 234: training loss 0.0632\n",
      "2025-05-29 21:03:43 [INFO]: epoch 235: training loss 0.0765\n",
      "2025-05-29 21:03:43 [INFO]: epoch 236: training loss 0.0677\n",
      "2025-05-29 21:03:43 [INFO]: epoch 237: training loss 0.0631\n",
      "2025-05-29 21:03:43 [INFO]: epoch 238: training loss 0.0732\n",
      "2025-05-29 21:03:43 [INFO]: epoch 239: training loss 0.0664\n",
      "2025-05-29 21:03:43 [INFO]: epoch 240: training loss 0.0599\n",
      "2025-05-29 21:03:43 [INFO]: epoch 241: training loss 0.0596\n",
      "2025-05-29 21:03:43 [INFO]: epoch 242: training loss 0.0750\n",
      "2025-05-29 21:03:43 [INFO]: epoch 243: training loss 0.0623\n",
      "2025-05-29 21:03:43 [INFO]: epoch 244: training loss 0.0644\n",
      "2025-05-29 21:03:43 [INFO]: epoch 245: training loss 0.0698\n",
      "2025-05-29 21:03:43 [INFO]: epoch 246: training loss 0.0657\n",
      "2025-05-29 21:03:43 [INFO]: epoch 247: training loss 0.0598\n",
      "2025-05-29 21:03:43 [INFO]: epoch 248: training loss 0.0588\n",
      "2025-05-29 21:03:43 [INFO]: epoch 249: training loss 0.0537\n",
      "2025-05-29 21:03:43 [INFO]: epoch 250: training loss 0.0501\n",
      "2025-05-29 21:03:43 [INFO]: epoch 251: training loss 0.0618\n",
      "2025-05-29 21:03:43 [INFO]: epoch 252: training loss 0.0550\n",
      "2025-05-29 21:03:43 [INFO]: epoch 253: training loss 0.0675\n",
      "2025-05-29 21:03:43 [INFO]: epoch 254: training loss 0.0588\n",
      "2025-05-29 21:03:43 [INFO]: epoch 255: training loss 0.0546\n",
      "2025-05-29 21:03:43 [INFO]: epoch 256: training loss 0.0580\n",
      "2025-05-29 21:03:43 [INFO]: epoch 257: training loss 0.0591\n",
      "2025-05-29 21:03:43 [INFO]: epoch 258: training loss 0.0646\n",
      "2025-05-29 21:03:43 [INFO]: epoch 259: training loss 0.0622\n",
      "2025-05-29 21:03:43 [INFO]: epoch 260: training loss 0.0584\n",
      "2025-05-29 21:03:43 [INFO]: epoch 261: training loss 0.0662\n",
      "2025-05-29 21:03:43 [INFO]: epoch 262: training loss 0.0567\n",
      "2025-05-29 21:03:44 [INFO]: epoch 263: training loss 0.0511\n",
      "2025-05-29 21:03:44 [INFO]: epoch 264: training loss 0.0567\n",
      "2025-05-29 21:03:44 [INFO]: epoch 265: training loss 0.0639\n",
      "2025-05-29 21:03:44 [INFO]: epoch 266: training loss 0.0589\n",
      "2025-05-29 21:03:44 [INFO]: epoch 267: training loss 0.0646\n",
      "2025-05-29 21:03:44 [INFO]: epoch 268: training loss 0.0559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:03:44 [INFO]: epoch 269: training loss 0.0499\n",
      "2025-05-29 21:03:44 [INFO]: epoch 270: training loss 0.0512\n",
      "2025-05-29 21:03:44 [INFO]: epoch 271: training loss 0.0567\n",
      "2025-05-29 21:03:44 [INFO]: epoch 272: training loss 0.0570\n",
      "2025-05-29 21:03:44 [INFO]: epoch 273: training loss 0.0611\n",
      "2025-05-29 21:03:44 [INFO]: epoch 274: training loss 0.0555\n",
      "2025-05-29 21:03:44 [INFO]: epoch 275: training loss 0.0558\n",
      "2025-05-29 21:03:44 [INFO]: epoch 276: training loss 0.0453\n",
      "2025-05-29 21:03:44 [INFO]: epoch 277: training loss 0.0697\n",
      "2025-05-29 21:03:44 [INFO]: epoch 278: training loss 0.0505\n",
      "2025-05-29 21:03:44 [INFO]: epoch 279: training loss 0.0630\n",
      "2025-05-29 21:03:44 [INFO]: epoch 280: training loss 0.0674\n",
      "2025-05-29 21:03:44 [INFO]: epoch 281: training loss 0.0544\n",
      "2025-05-29 21:03:44 [INFO]: epoch 282: training loss 0.0557\n",
      "2025-05-29 21:03:44 [INFO]: epoch 283: training loss 0.0692\n",
      "2025-05-29 21:03:44 [INFO]: epoch 284: training loss 0.0719\n",
      "2025-05-29 21:03:44 [INFO]: epoch 285: training loss 0.0593\n",
      "2025-05-29 21:03:44 [INFO]: epoch 286: training loss 0.0461\n",
      "2025-05-29 21:03:44 [INFO]: epoch 287: training loss 0.0716\n",
      "2025-05-29 21:03:44 [INFO]: epoch 288: training loss 0.0584\n",
      "2025-05-29 21:03:44 [INFO]: epoch 289: training loss 0.0455\n",
      "2025-05-29 21:03:44 [INFO]: epoch 290: training loss 0.0531\n",
      "2025-05-29 21:03:44 [INFO]: epoch 291: training loss 0.0686\n",
      "2025-05-29 21:03:44 [INFO]: epoch 292: training loss 0.0630\n",
      "2025-05-29 21:03:44 [INFO]: epoch 293: training loss 0.0601\n",
      "2025-05-29 21:03:44 [INFO]: epoch 294: training loss 0.0651\n",
      "2025-05-29 21:03:44 [INFO]: epoch 295: training loss 0.0528\n",
      "2025-05-29 21:03:44 [INFO]: epoch 296: training loss 0.0480\n",
      "2025-05-29 21:03:44 [INFO]: epoch 297: training loss 0.0628\n",
      "2025-05-29 21:03:44 [INFO]: epoch 298: training loss 0.0598\n",
      "2025-05-29 21:03:44 [INFO]: epoch 299: training loss 0.0504\n",
      "2025-05-29 21:03:44 [INFO]: Finished training.\n",
      "2025-05-29 21:03:44 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 20%|████████████████▊                                                                   | 1/5 [00:03<00:15,  3.93s/it]2025-05-29 21:03:44 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:03:44 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:03:44 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:03:44 [INFO]: epoch 0: training loss 0.6915\n",
      "2025-05-29 21:03:44 [INFO]: epoch 1: training loss 0.5533\n",
      "2025-05-29 21:03:44 [INFO]: epoch 2: training loss 0.5932\n",
      "2025-05-29 21:03:44 [INFO]: epoch 3: training loss 0.4084\n",
      "2025-05-29 21:03:44 [INFO]: epoch 4: training loss 0.3215\n",
      "2025-05-29 21:03:44 [INFO]: epoch 5: training loss 0.3471\n",
      "2025-05-29 21:03:44 [INFO]: epoch 6: training loss 0.4033\n",
      "2025-05-29 21:03:44 [INFO]: epoch 7: training loss 0.3569\n",
      "2025-05-29 21:03:44 [INFO]: epoch 8: training loss 0.3461\n",
      "2025-05-29 21:03:44 [INFO]: epoch 9: training loss 0.3336\n",
      "2025-05-29 21:03:44 [INFO]: epoch 10: training loss 0.3003\n",
      "2025-05-29 21:03:44 [INFO]: epoch 11: training loss 0.2507\n",
      "2025-05-29 21:03:44 [INFO]: epoch 12: training loss 0.2453\n",
      "2025-05-29 21:03:44 [INFO]: epoch 13: training loss 0.2749\n",
      "2025-05-29 21:03:44 [INFO]: epoch 14: training loss 0.3239\n",
      "2025-05-29 21:03:44 [INFO]: epoch 15: training loss 0.2727\n",
      "2025-05-29 21:03:44 [INFO]: epoch 16: training loss 0.2456\n",
      "2025-05-29 21:03:44 [INFO]: epoch 17: training loss 0.2055\n",
      "2025-05-29 21:03:44 [INFO]: epoch 18: training loss 0.2394\n",
      "2025-05-29 21:03:44 [INFO]: epoch 19: training loss 0.2179\n",
      "2025-05-29 21:03:44 [INFO]: epoch 20: training loss 0.1991\n",
      "2025-05-29 21:03:44 [INFO]: epoch 21: training loss 0.2057\n",
      "2025-05-29 21:03:44 [INFO]: epoch 22: training loss 0.2099\n",
      "2025-05-29 21:03:44 [INFO]: epoch 23: training loss 0.2122\n",
      "2025-05-29 21:03:44 [INFO]: epoch 24: training loss 0.2390\n",
      "2025-05-29 21:03:44 [INFO]: epoch 25: training loss 0.1823\n",
      "2025-05-29 21:03:44 [INFO]: epoch 26: training loss 0.2262\n",
      "2025-05-29 21:03:44 [INFO]: epoch 27: training loss 0.1967\n",
      "2025-05-29 21:03:44 [INFO]: epoch 28: training loss 0.2088\n",
      "2025-05-29 21:03:44 [INFO]: epoch 29: training loss 0.1613\n",
      "2025-05-29 21:03:44 [INFO]: epoch 30: training loss 0.1754\n",
      "2025-05-29 21:03:44 [INFO]: epoch 31: training loss 0.1768\n",
      "2025-05-29 21:03:44 [INFO]: epoch 32: training loss 0.1723\n",
      "2025-05-29 21:03:44 [INFO]: epoch 33: training loss 0.1762\n",
      "2025-05-29 21:03:44 [INFO]: epoch 34: training loss 0.1710\n",
      "2025-05-29 21:03:44 [INFO]: epoch 35: training loss 0.1519\n",
      "2025-05-29 21:03:44 [INFO]: epoch 36: training loss 0.1752\n",
      "2025-05-29 21:03:45 [INFO]: epoch 37: training loss 0.1753\n",
      "2025-05-29 21:03:45 [INFO]: epoch 38: training loss 0.1572\n",
      "2025-05-29 21:03:45 [INFO]: epoch 39: training loss 0.1554\n",
      "2025-05-29 21:03:45 [INFO]: epoch 40: training loss 0.1274\n",
      "2025-05-29 21:03:45 [INFO]: epoch 41: training loss 0.1605\n",
      "2025-05-29 21:03:45 [INFO]: epoch 42: training loss 0.1475\n",
      "2025-05-29 21:03:45 [INFO]: epoch 43: training loss 0.1595\n",
      "2025-05-29 21:03:45 [INFO]: epoch 44: training loss 0.1408\n",
      "2025-05-29 21:03:45 [INFO]: epoch 45: training loss 0.1423\n",
      "2025-05-29 21:03:45 [INFO]: epoch 46: training loss 0.1604\n",
      "2025-05-29 21:03:45 [INFO]: epoch 47: training loss 0.1525\n",
      "2025-05-29 21:03:45 [INFO]: epoch 48: training loss 0.1473\n",
      "2025-05-29 21:03:45 [INFO]: epoch 49: training loss 0.1728\n",
      "2025-05-29 21:03:45 [INFO]: epoch 50: training loss 0.1475\n",
      "2025-05-29 21:03:45 [INFO]: epoch 51: training loss 0.1427\n",
      "2025-05-29 21:03:45 [INFO]: epoch 52: training loss 0.1510\n",
      "2025-05-29 21:03:45 [INFO]: epoch 53: training loss 0.1351\n",
      "2025-05-29 21:03:45 [INFO]: epoch 54: training loss 0.1258\n",
      "2025-05-29 21:03:45 [INFO]: epoch 55: training loss 0.1429\n",
      "2025-05-29 21:03:45 [INFO]: epoch 56: training loss 0.1590\n",
      "2025-05-29 21:03:45 [INFO]: epoch 57: training loss 0.1590\n",
      "2025-05-29 21:03:45 [INFO]: epoch 58: training loss 0.1285\n",
      "2025-05-29 21:03:45 [INFO]: epoch 59: training loss 0.1333\n",
      "2025-05-29 21:03:45 [INFO]: epoch 60: training loss 0.1418\n",
      "2025-05-29 21:03:45 [INFO]: epoch 61: training loss 0.1538\n",
      "2025-05-29 21:03:45 [INFO]: epoch 62: training loss 0.1446\n",
      "2025-05-29 21:03:45 [INFO]: epoch 63: training loss 0.1245\n",
      "2025-05-29 21:03:45 [INFO]: epoch 64: training loss 0.1352\n",
      "2025-05-29 21:03:45 [INFO]: epoch 65: training loss 0.1449\n",
      "2025-05-29 21:03:45 [INFO]: epoch 66: training loss 0.1395\n",
      "2025-05-29 21:03:45 [INFO]: epoch 67: training loss 0.1351\n",
      "2025-05-29 21:03:45 [INFO]: epoch 68: training loss 0.1272\n",
      "2025-05-29 21:03:45 [INFO]: epoch 69: training loss 0.1262\n",
      "2025-05-29 21:03:45 [INFO]: epoch 70: training loss 0.1507\n",
      "2025-05-29 21:03:45 [INFO]: epoch 71: training loss 0.1319\n",
      "2025-05-29 21:03:45 [INFO]: epoch 72: training loss 0.1168\n",
      "2025-05-29 21:03:45 [INFO]: epoch 73: training loss 0.1122\n",
      "2025-05-29 21:03:45 [INFO]: epoch 74: training loss 0.1127\n",
      "2025-05-29 21:03:45 [INFO]: epoch 75: training loss 0.1095\n",
      "2025-05-29 21:03:45 [INFO]: epoch 76: training loss 0.1152\n",
      "2025-05-29 21:03:45 [INFO]: epoch 77: training loss 0.1154\n",
      "2025-05-29 21:03:45 [INFO]: epoch 78: training loss 0.1207\n",
      "2025-05-29 21:03:45 [INFO]: epoch 79: training loss 0.1068\n",
      "2025-05-29 21:03:45 [INFO]: epoch 80: training loss 0.1076\n",
      "2025-05-29 21:03:45 [INFO]: epoch 81: training loss 0.1030\n",
      "2025-05-29 21:03:45 [INFO]: epoch 82: training loss 0.1325\n",
      "2025-05-29 21:03:45 [INFO]: epoch 83: training loss 0.1071\n",
      "2025-05-29 21:03:45 [INFO]: epoch 84: training loss 0.1075\n",
      "2025-05-29 21:03:45 [INFO]: epoch 85: training loss 0.1057\n",
      "2025-05-29 21:03:45 [INFO]: epoch 86: training loss 0.1206\n",
      "2025-05-29 21:03:45 [INFO]: epoch 87: training loss 0.1066\n",
      "2025-05-29 21:03:45 [INFO]: epoch 88: training loss 0.0985\n",
      "2025-05-29 21:03:45 [INFO]: epoch 89: training loss 0.1084\n",
      "2025-05-29 21:03:45 [INFO]: epoch 90: training loss 0.1139\n",
      "2025-05-29 21:03:45 [INFO]: epoch 91: training loss 0.0903\n",
      "2025-05-29 21:03:45 [INFO]: epoch 92: training loss 0.0998\n",
      "2025-05-29 21:03:45 [INFO]: epoch 93: training loss 0.1028\n",
      "2025-05-29 21:03:45 [INFO]: epoch 94: training loss 0.1056\n",
      "2025-05-29 21:03:45 [INFO]: epoch 95: training loss 0.1081\n",
      "2025-05-29 21:03:45 [INFO]: epoch 96: training loss 0.0944\n",
      "2025-05-29 21:03:45 [INFO]: epoch 97: training loss 0.0973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:03:45 [INFO]: epoch 98: training loss 0.1040\n",
      "2025-05-29 21:03:45 [INFO]: epoch 99: training loss 0.1037\n",
      "2025-05-29 21:03:45 [INFO]: epoch 100: training loss 0.1053\n",
      "2025-05-29 21:03:45 [INFO]: epoch 101: training loss 0.0994\n",
      "2025-05-29 21:03:45 [INFO]: epoch 102: training loss 0.0917\n",
      "2025-05-29 21:03:45 [INFO]: epoch 103: training loss 0.0977\n",
      "2025-05-29 21:03:45 [INFO]: epoch 104: training loss 0.1099\n",
      "2025-05-29 21:03:45 [INFO]: epoch 105: training loss 0.1032\n",
      "2025-05-29 21:03:45 [INFO]: epoch 106: training loss 0.0832\n",
      "2025-05-29 21:03:45 [INFO]: epoch 107: training loss 0.0954\n",
      "2025-05-29 21:03:45 [INFO]: epoch 108: training loss 0.0949\n",
      "2025-05-29 21:03:45 [INFO]: epoch 109: training loss 0.0963\n",
      "2025-05-29 21:03:45 [INFO]: epoch 110: training loss 0.1004\n",
      "2025-05-29 21:03:45 [INFO]: epoch 111: training loss 0.0958\n",
      "2025-05-29 21:03:45 [INFO]: epoch 112: training loss 0.1040\n",
      "2025-05-29 21:03:45 [INFO]: epoch 113: training loss 0.0946\n",
      "2025-05-29 21:03:45 [INFO]: epoch 114: training loss 0.0833\n",
      "2025-05-29 21:03:45 [INFO]: epoch 115: training loss 0.0914\n",
      "2025-05-29 21:03:46 [INFO]: epoch 116: training loss 0.1006\n",
      "2025-05-29 21:03:46 [INFO]: epoch 117: training loss 0.0889\n",
      "2025-05-29 21:03:46 [INFO]: epoch 118: training loss 0.0807\n",
      "2025-05-29 21:03:46 [INFO]: epoch 119: training loss 0.0928\n",
      "2025-05-29 21:03:46 [INFO]: epoch 120: training loss 0.0932\n",
      "2025-05-29 21:03:46 [INFO]: epoch 121: training loss 0.0881\n",
      "2025-05-29 21:03:46 [INFO]: epoch 122: training loss 0.0921\n",
      "2025-05-29 21:03:46 [INFO]: epoch 123: training loss 0.0803\n",
      "2025-05-29 21:03:46 [INFO]: epoch 124: training loss 0.0838\n",
      "2025-05-29 21:03:46 [INFO]: epoch 125: training loss 0.0835\n",
      "2025-05-29 21:03:46 [INFO]: epoch 126: training loss 0.0792\n",
      "2025-05-29 21:03:46 [INFO]: epoch 127: training loss 0.0804\n",
      "2025-05-29 21:03:46 [INFO]: epoch 128: training loss 0.0804\n",
      "2025-05-29 21:03:46 [INFO]: epoch 129: training loss 0.0798\n",
      "2025-05-29 21:03:46 [INFO]: epoch 130: training loss 0.0790\n",
      "2025-05-29 21:03:46 [INFO]: epoch 131: training loss 0.0783\n",
      "2025-05-29 21:03:46 [INFO]: epoch 132: training loss 0.0877\n",
      "2025-05-29 21:03:46 [INFO]: epoch 133: training loss 0.0765\n",
      "2025-05-29 21:03:46 [INFO]: epoch 134: training loss 0.0793\n",
      "2025-05-29 21:03:46 [INFO]: epoch 135: training loss 0.0778\n",
      "2025-05-29 21:03:46 [INFO]: epoch 136: training loss 0.0777\n",
      "2025-05-29 21:03:46 [INFO]: epoch 137: training loss 0.0716\n",
      "2025-05-29 21:03:46 [INFO]: epoch 138: training loss 0.0786\n",
      "2025-05-29 21:03:46 [INFO]: epoch 139: training loss 0.0759\n",
      "2025-05-29 21:03:46 [INFO]: epoch 140: training loss 0.0697\n",
      "2025-05-29 21:03:46 [INFO]: epoch 141: training loss 0.0761\n",
      "2025-05-29 21:03:46 [INFO]: epoch 142: training loss 0.0809\n",
      "2025-05-29 21:03:46 [INFO]: epoch 143: training loss 0.0794\n",
      "2025-05-29 21:03:46 [INFO]: epoch 144: training loss 0.0919\n",
      "2025-05-29 21:03:46 [INFO]: epoch 145: training loss 0.0851\n",
      "2025-05-29 21:03:46 [INFO]: epoch 146: training loss 0.0789\n",
      "2025-05-29 21:03:46 [INFO]: epoch 147: training loss 0.0794\n",
      "2025-05-29 21:03:46 [INFO]: epoch 148: training loss 0.0754\n",
      "2025-05-29 21:03:46 [INFO]: epoch 149: training loss 0.0701\n",
      "2025-05-29 21:03:46 [INFO]: epoch 150: training loss 0.0757\n",
      "2025-05-29 21:03:46 [INFO]: epoch 151: training loss 0.0830\n",
      "2025-05-29 21:03:46 [INFO]: epoch 152: training loss 0.0801\n",
      "2025-05-29 21:03:46 [INFO]: epoch 153: training loss 0.0707\n",
      "2025-05-29 21:03:46 [INFO]: epoch 154: training loss 0.0754\n",
      "2025-05-29 21:03:46 [INFO]: epoch 155: training loss 0.0605\n",
      "2025-05-29 21:03:46 [INFO]: epoch 156: training loss 0.0782\n",
      "2025-05-29 21:03:46 [INFO]: epoch 157: training loss 0.0731\n",
      "2025-05-29 21:03:46 [INFO]: epoch 158: training loss 0.0634\n",
      "2025-05-29 21:03:46 [INFO]: epoch 159: training loss 0.0712\n",
      "2025-05-29 21:03:46 [INFO]: epoch 160: training loss 0.0619\n",
      "2025-05-29 21:03:46 [INFO]: epoch 161: training loss 0.0733\n",
      "2025-05-29 21:03:46 [INFO]: epoch 162: training loss 0.0898\n",
      "2025-05-29 21:03:46 [INFO]: epoch 163: training loss 0.0937\n",
      "2025-05-29 21:03:46 [INFO]: epoch 164: training loss 0.0687\n",
      "2025-05-29 21:03:46 [INFO]: epoch 165: training loss 0.0628\n",
      "2025-05-29 21:03:46 [INFO]: epoch 166: training loss 0.0826\n",
      "2025-05-29 21:03:46 [INFO]: epoch 167: training loss 0.0710\n",
      "2025-05-29 21:03:46 [INFO]: epoch 168: training loss 0.0605\n",
      "2025-05-29 21:03:46 [INFO]: epoch 169: training loss 0.0735\n",
      "2025-05-29 21:03:46 [INFO]: epoch 170: training loss 0.0839\n",
      "2025-05-29 21:03:46 [INFO]: epoch 171: training loss 0.0696\n",
      "2025-05-29 21:03:46 [INFO]: epoch 172: training loss 0.0751\n",
      "2025-05-29 21:03:46 [INFO]: epoch 173: training loss 0.0735\n",
      "2025-05-29 21:03:46 [INFO]: epoch 174: training loss 0.0643\n",
      "2025-05-29 21:03:46 [INFO]: epoch 175: training loss 0.0794\n",
      "2025-05-29 21:03:46 [INFO]: epoch 176: training loss 0.0712\n",
      "2025-05-29 21:03:46 [INFO]: epoch 177: training loss 0.0639\n",
      "2025-05-29 21:03:46 [INFO]: epoch 178: training loss 0.0627\n",
      "2025-05-29 21:03:46 [INFO]: epoch 179: training loss 0.0796\n",
      "2025-05-29 21:03:46 [INFO]: epoch 180: training loss 0.0803\n",
      "2025-05-29 21:03:46 [INFO]: epoch 181: training loss 0.0655\n",
      "2025-05-29 21:03:46 [INFO]: epoch 182: training loss 0.0686\n",
      "2025-05-29 21:03:46 [INFO]: epoch 183: training loss 0.0747\n",
      "2025-05-29 21:03:46 [INFO]: epoch 184: training loss 0.0745\n",
      "2025-05-29 21:03:46 [INFO]: epoch 185: training loss 0.0690\n",
      "2025-05-29 21:03:46 [INFO]: epoch 186: training loss 0.0701\n",
      "2025-05-29 21:03:46 [INFO]: epoch 187: training loss 0.0520\n",
      "2025-05-29 21:03:46 [INFO]: epoch 188: training loss 0.0588\n",
      "2025-05-29 21:03:46 [INFO]: epoch 189: training loss 0.0650\n",
      "2025-05-29 21:03:46 [INFO]: epoch 190: training loss 0.0604\n",
      "2025-05-29 21:03:46 [INFO]: epoch 191: training loss 0.0521\n",
      "2025-05-29 21:03:46 [INFO]: epoch 192: training loss 0.0546\n",
      "2025-05-29 21:03:46 [INFO]: epoch 193: training loss 0.0695\n",
      "2025-05-29 21:03:46 [INFO]: epoch 194: training loss 0.0650\n",
      "2025-05-29 21:03:46 [INFO]: epoch 195: training loss 0.0576\n",
      "2025-05-29 21:03:46 [INFO]: epoch 196: training loss 0.0611\n",
      "2025-05-29 21:03:47 [INFO]: epoch 197: training loss 0.0712\n",
      "2025-05-29 21:03:47 [INFO]: epoch 198: training loss 0.0675\n",
      "2025-05-29 21:03:47 [INFO]: epoch 199: training loss 0.0571\n",
      "2025-05-29 21:03:47 [INFO]: epoch 200: training loss 0.0569\n",
      "2025-05-29 21:03:47 [INFO]: epoch 201: training loss 0.0589\n",
      "2025-05-29 21:03:47 [INFO]: epoch 202: training loss 0.0532\n",
      "2025-05-29 21:03:47 [INFO]: epoch 203: training loss 0.0625\n",
      "2025-05-29 21:03:47 [INFO]: epoch 204: training loss 0.0543\n",
      "2025-05-29 21:03:47 [INFO]: epoch 205: training loss 0.0503\n",
      "2025-05-29 21:03:47 [INFO]: epoch 206: training loss 0.0620\n",
      "2025-05-29 21:03:47 [INFO]: epoch 207: training loss 0.0604\n",
      "2025-05-29 21:03:47 [INFO]: epoch 208: training loss 0.0501\n",
      "2025-05-29 21:03:47 [INFO]: epoch 209: training loss 0.0456\n",
      "2025-05-29 21:03:47 [INFO]: epoch 210: training loss 0.0658\n",
      "2025-05-29 21:03:47 [INFO]: epoch 211: training loss 0.0524\n",
      "2025-05-29 21:03:47 [INFO]: epoch 212: training loss 0.0524\n",
      "2025-05-29 21:03:47 [INFO]: epoch 213: training loss 0.0568\n",
      "2025-05-29 21:03:47 [INFO]: epoch 214: training loss 0.0596\n",
      "2025-05-29 21:03:47 [INFO]: epoch 215: training loss 0.0589\n",
      "2025-05-29 21:03:47 [INFO]: epoch 216: training loss 0.0516\n",
      "2025-05-29 21:03:47 [INFO]: epoch 217: training loss 0.0568\n",
      "2025-05-29 21:03:47 [INFO]: epoch 218: training loss 0.0526\n",
      "2025-05-29 21:03:47 [INFO]: epoch 219: training loss 0.0523\n",
      "2025-05-29 21:03:47 [INFO]: epoch 220: training loss 0.0496\n",
      "2025-05-29 21:03:47 [INFO]: epoch 221: training loss 0.0495\n",
      "2025-05-29 21:03:47 [INFO]: epoch 222: training loss 0.0453\n",
      "2025-05-29 21:03:47 [INFO]: epoch 223: training loss 0.0494\n",
      "2025-05-29 21:03:47 [INFO]: epoch 224: training loss 0.0531\n",
      "2025-05-29 21:03:47 [INFO]: epoch 225: training loss 0.0504\n",
      "2025-05-29 21:03:47 [INFO]: epoch 226: training loss 0.0553\n",
      "2025-05-29 21:03:47 [INFO]: epoch 227: training loss 0.0495\n",
      "2025-05-29 21:03:47 [INFO]: epoch 228: training loss 0.0437\n",
      "2025-05-29 21:03:47 [INFO]: epoch 229: training loss 0.0498\n",
      "2025-05-29 21:03:47 [INFO]: epoch 230: training loss 0.0581\n",
      "2025-05-29 21:03:47 [INFO]: epoch 231: training loss 0.0511\n",
      "2025-05-29 21:03:47 [INFO]: epoch 232: training loss 0.0516\n",
      "2025-05-29 21:03:47 [INFO]: epoch 233: training loss 0.0449\n",
      "2025-05-29 21:03:47 [INFO]: epoch 234: training loss 0.0557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:03:47 [INFO]: epoch 235: training loss 0.0538\n",
      "2025-05-29 21:03:47 [INFO]: epoch 236: training loss 0.0470\n",
      "2025-05-29 21:03:47 [INFO]: epoch 237: training loss 0.0440\n",
      "2025-05-29 21:03:47 [INFO]: epoch 238: training loss 0.0467\n",
      "2025-05-29 21:03:47 [INFO]: epoch 239: training loss 0.0398\n",
      "2025-05-29 21:03:47 [INFO]: epoch 240: training loss 0.0459\n",
      "2025-05-29 21:03:47 [INFO]: epoch 241: training loss 0.0341\n",
      "2025-05-29 21:03:47 [INFO]: epoch 242: training loss 0.0451\n",
      "2025-05-29 21:03:47 [INFO]: epoch 243: training loss 0.0464\n",
      "2025-05-29 21:03:47 [INFO]: epoch 244: training loss 0.0487\n",
      "2025-05-29 21:03:47 [INFO]: epoch 245: training loss 0.0486\n",
      "2025-05-29 21:03:47 [INFO]: epoch 246: training loss 0.0471\n",
      "2025-05-29 21:03:47 [INFO]: epoch 247: training loss 0.0412\n",
      "2025-05-29 21:03:47 [INFO]: epoch 248: training loss 0.0421\n",
      "2025-05-29 21:03:47 [INFO]: epoch 249: training loss 0.0444\n",
      "2025-05-29 21:03:47 [INFO]: epoch 250: training loss 0.0455\n",
      "2025-05-29 21:03:47 [INFO]: epoch 251: training loss 0.0536\n",
      "2025-05-29 21:03:47 [INFO]: epoch 252: training loss 0.0625\n",
      "2025-05-29 21:03:47 [INFO]: epoch 253: training loss 0.0535\n",
      "2025-05-29 21:03:47 [INFO]: epoch 254: training loss 0.0413\n",
      "2025-05-29 21:03:47 [INFO]: epoch 255: training loss 0.0402\n",
      "2025-05-29 21:03:47 [INFO]: epoch 256: training loss 0.0518\n",
      "2025-05-29 21:03:47 [INFO]: epoch 257: training loss 0.0456\n",
      "2025-05-29 21:03:47 [INFO]: epoch 258: training loss 0.0429\n",
      "2025-05-29 21:03:47 [INFO]: epoch 259: training loss 0.0488\n",
      "2025-05-29 21:03:47 [INFO]: epoch 260: training loss 0.0459\n",
      "2025-05-29 21:03:47 [INFO]: epoch 261: training loss 0.0477\n",
      "2025-05-29 21:03:47 [INFO]: epoch 262: training loss 0.0416\n",
      "2025-05-29 21:03:47 [INFO]: epoch 263: training loss 0.0383\n",
      "2025-05-29 21:03:47 [INFO]: epoch 264: training loss 0.0467\n",
      "2025-05-29 21:03:47 [INFO]: epoch 265: training loss 0.0391\n",
      "2025-05-29 21:03:47 [INFO]: epoch 266: training loss 0.0404\n",
      "2025-05-29 21:03:47 [INFO]: epoch 267: training loss 0.0435\n",
      "2025-05-29 21:03:47 [INFO]: epoch 268: training loss 0.0427\n",
      "2025-05-29 21:03:47 [INFO]: epoch 269: training loss 0.0460\n",
      "2025-05-29 21:03:47 [INFO]: epoch 270: training loss 0.0448\n",
      "2025-05-29 21:03:47 [INFO]: epoch 271: training loss 0.0460\n",
      "2025-05-29 21:03:47 [INFO]: epoch 272: training loss 0.0424\n",
      "2025-05-29 21:03:47 [INFO]: epoch 273: training loss 0.0404\n",
      "2025-05-29 21:03:47 [INFO]: epoch 274: training loss 0.0546\n",
      "2025-05-29 21:03:47 [INFO]: epoch 275: training loss 0.0462\n",
      "2025-05-29 21:03:48 [INFO]: epoch 276: training loss 0.0505\n",
      "2025-05-29 21:03:48 [INFO]: epoch 277: training loss 0.0413\n",
      "2025-05-29 21:03:48 [INFO]: epoch 278: training loss 0.0375\n",
      "2025-05-29 21:03:48 [INFO]: epoch 279: training loss 0.0349\n",
      "2025-05-29 21:03:48 [INFO]: epoch 280: training loss 0.0430\n",
      "2025-05-29 21:03:48 [INFO]: epoch 281: training loss 0.0417\n",
      "2025-05-29 21:03:48 [INFO]: epoch 282: training loss 0.0369\n",
      "2025-05-29 21:03:48 [INFO]: epoch 283: training loss 0.0449\n",
      "2025-05-29 21:03:48 [INFO]: epoch 284: training loss 0.0466\n",
      "2025-05-29 21:03:48 [INFO]: epoch 285: training loss 0.0401\n",
      "2025-05-29 21:03:48 [INFO]: epoch 286: training loss 0.0404\n",
      "2025-05-29 21:03:48 [INFO]: epoch 287: training loss 0.0436\n",
      "2025-05-29 21:03:48 [INFO]: epoch 288: training loss 0.0368\n",
      "2025-05-29 21:03:48 [INFO]: epoch 289: training loss 0.0358\n",
      "2025-05-29 21:03:48 [INFO]: epoch 290: training loss 0.0417\n",
      "2025-05-29 21:03:48 [INFO]: epoch 291: training loss 0.0385\n",
      "2025-05-29 21:03:48 [INFO]: epoch 292: training loss 0.0337\n",
      "2025-05-29 21:03:48 [INFO]: epoch 293: training loss 0.0475\n",
      "2025-05-29 21:03:48 [INFO]: epoch 294: training loss 0.0408\n",
      "2025-05-29 21:03:48 [INFO]: epoch 295: training loss 0.0359\n",
      "2025-05-29 21:03:48 [INFO]: epoch 296: training loss 0.0354\n",
      "2025-05-29 21:03:48 [INFO]: epoch 297: training loss 0.0414\n",
      "2025-05-29 21:03:48 [INFO]: epoch 298: training loss 0.0305\n",
      "2025-05-29 21:03:48 [INFO]: epoch 299: training loss 0.0451\n",
      "2025-05-29 21:03:48 [INFO]: Finished training.\n",
      "2025-05-29 21:03:48 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:07<00:11,  3.87s/it]2025-05-29 21:03:48 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:03:48 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:03:48 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:03:48 [INFO]: epoch 0: training loss 1.2556\n",
      "2025-05-29 21:03:48 [INFO]: epoch 1: training loss 0.6642\n",
      "2025-05-29 21:03:48 [INFO]: epoch 2: training loss 0.5865\n",
      "2025-05-29 21:03:48 [INFO]: epoch 3: training loss 0.6097\n",
      "2025-05-29 21:03:48 [INFO]: epoch 4: training loss 0.6244\n",
      "2025-05-29 21:03:48 [INFO]: epoch 5: training loss 0.6058\n",
      "2025-05-29 21:03:48 [INFO]: epoch 6: training loss 0.5723\n",
      "2025-05-29 21:03:48 [INFO]: epoch 7: training loss 0.4511\n",
      "2025-05-29 21:03:48 [INFO]: epoch 8: training loss 0.4381\n",
      "2025-05-29 21:03:48 [INFO]: epoch 9: training loss 0.4455\n",
      "2025-05-29 21:03:48 [INFO]: epoch 10: training loss 0.4433\n",
      "2025-05-29 21:03:48 [INFO]: epoch 11: training loss 0.4928\n",
      "2025-05-29 21:03:48 [INFO]: epoch 12: training loss 0.4756\n",
      "2025-05-29 21:03:48 [INFO]: epoch 13: training loss 0.4558\n",
      "2025-05-29 21:03:48 [INFO]: epoch 14: training loss 0.3805\n",
      "2025-05-29 21:03:48 [INFO]: epoch 15: training loss 0.4226\n",
      "2025-05-29 21:03:48 [INFO]: epoch 16: training loss 0.3871\n",
      "2025-05-29 21:03:48 [INFO]: epoch 17: training loss 0.3769\n",
      "2025-05-29 21:03:48 [INFO]: epoch 18: training loss 0.3735\n",
      "2025-05-29 21:03:48 [INFO]: epoch 19: training loss 0.3808\n",
      "2025-05-29 21:03:48 [INFO]: epoch 20: training loss 0.3612\n",
      "2025-05-29 21:03:48 [INFO]: epoch 21: training loss 0.3346\n",
      "2025-05-29 21:03:48 [INFO]: epoch 22: training loss 0.3443\n",
      "2025-05-29 21:03:48 [INFO]: epoch 23: training loss 0.3235\n",
      "2025-05-29 21:03:48 [INFO]: epoch 24: training loss 0.3400\n",
      "2025-05-29 21:03:48 [INFO]: epoch 25: training loss 0.3167\n",
      "2025-05-29 21:03:48 [INFO]: epoch 26: training loss 0.3063\n",
      "2025-05-29 21:03:48 [INFO]: epoch 27: training loss 0.3133\n",
      "2025-05-29 21:03:48 [INFO]: epoch 28: training loss 0.3026\n",
      "2025-05-29 21:03:48 [INFO]: epoch 29: training loss 0.2827\n",
      "2025-05-29 21:03:48 [INFO]: epoch 30: training loss 0.3119\n",
      "2025-05-29 21:03:48 [INFO]: epoch 31: training loss 0.2685\n",
      "2025-05-29 21:03:48 [INFO]: epoch 32: training loss 0.2592\n",
      "2025-05-29 21:03:48 [INFO]: epoch 33: training loss 0.2750\n",
      "2025-05-29 21:03:48 [INFO]: epoch 34: training loss 0.2893\n",
      "2025-05-29 21:03:48 [INFO]: epoch 35: training loss 0.2805\n",
      "2025-05-29 21:03:48 [INFO]: epoch 36: training loss 0.2643\n",
      "2025-05-29 21:03:48 [INFO]: epoch 37: training loss 0.2452\n",
      "2025-05-29 21:03:48 [INFO]: epoch 38: training loss 0.2559\n",
      "2025-05-29 21:03:48 [INFO]: epoch 39: training loss 0.2724\n",
      "2025-05-29 21:03:48 [INFO]: epoch 40: training loss 0.2512\n",
      "2025-05-29 21:03:48 [INFO]: epoch 41: training loss 0.2432\n",
      "2025-05-29 21:03:48 [INFO]: epoch 42: training loss 0.2356\n",
      "2025-05-29 21:03:48 [INFO]: epoch 43: training loss 0.2338\n",
      "2025-05-29 21:03:48 [INFO]: epoch 44: training loss 0.2306\n",
      "2025-05-29 21:03:48 [INFO]: epoch 45: training loss 0.2226\n",
      "2025-05-29 21:03:48 [INFO]: epoch 46: training loss 0.2238\n",
      "2025-05-29 21:03:48 [INFO]: epoch 47: training loss 0.2237\n",
      "2025-05-29 21:03:49 [INFO]: epoch 48: training loss 0.2320\n",
      "2025-05-29 21:03:49 [INFO]: epoch 49: training loss 0.2203\n",
      "2025-05-29 21:03:49 [INFO]: epoch 50: training loss 0.2045\n",
      "2025-05-29 21:03:49 [INFO]: epoch 51: training loss 0.2168\n",
      "2025-05-29 21:03:49 [INFO]: epoch 52: training loss 0.2190\n",
      "2025-05-29 21:03:49 [INFO]: epoch 53: training loss 0.1876\n",
      "2025-05-29 21:03:49 [INFO]: epoch 54: training loss 0.2156\n",
      "2025-05-29 21:03:49 [INFO]: epoch 55: training loss 0.1862\n",
      "2025-05-29 21:03:49 [INFO]: epoch 56: training loss 0.1986\n",
      "2025-05-29 21:03:49 [INFO]: epoch 57: training loss 0.2105\n",
      "2025-05-29 21:03:49 [INFO]: epoch 58: training loss 0.1997\n",
      "2025-05-29 21:03:49 [INFO]: epoch 59: training loss 0.1597\n",
      "2025-05-29 21:03:49 [INFO]: epoch 60: training loss 0.1807\n",
      "2025-05-29 21:03:49 [INFO]: epoch 61: training loss 0.1739\n",
      "2025-05-29 21:03:49 [INFO]: epoch 62: training loss 0.1753\n",
      "2025-05-29 21:03:49 [INFO]: epoch 63: training loss 0.1798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:03:49 [INFO]: epoch 64: training loss 0.1631\n",
      "2025-05-29 21:03:49 [INFO]: epoch 65: training loss 0.1684\n",
      "2025-05-29 21:03:49 [INFO]: epoch 66: training loss 0.1495\n",
      "2025-05-29 21:03:49 [INFO]: epoch 67: training loss 0.1624\n",
      "2025-05-29 21:03:49 [INFO]: epoch 68: training loss 0.1554\n",
      "2025-05-29 21:03:49 [INFO]: epoch 69: training loss 0.1706\n",
      "2025-05-29 21:03:49 [INFO]: epoch 70: training loss 0.1577\n",
      "2025-05-29 21:03:49 [INFO]: epoch 71: training loss 0.1530\n",
      "2025-05-29 21:03:49 [INFO]: epoch 72: training loss 0.1548\n",
      "2025-05-29 21:03:49 [INFO]: epoch 73: training loss 0.1384\n",
      "2025-05-29 21:03:49 [INFO]: epoch 74: training loss 0.1556\n",
      "2025-05-29 21:03:49 [INFO]: epoch 75: training loss 0.1363\n",
      "2025-05-29 21:03:49 [INFO]: epoch 76: training loss 0.1484\n",
      "2025-05-29 21:03:49 [INFO]: epoch 77: training loss 0.1423\n",
      "2025-05-29 21:03:49 [INFO]: epoch 78: training loss 0.1230\n",
      "2025-05-29 21:03:49 [INFO]: epoch 79: training loss 0.1297\n",
      "2025-05-29 21:03:49 [INFO]: epoch 80: training loss 0.1214\n",
      "2025-05-29 21:03:49 [INFO]: epoch 81: training loss 0.1351\n",
      "2025-05-29 21:03:49 [INFO]: epoch 82: training loss 0.1183\n",
      "2025-05-29 21:03:49 [INFO]: epoch 83: training loss 0.1158\n",
      "2025-05-29 21:03:49 [INFO]: epoch 84: training loss 0.1344\n",
      "2025-05-29 21:03:49 [INFO]: epoch 85: training loss 0.1344\n",
      "2025-05-29 21:03:49 [INFO]: epoch 86: training loss 0.1178\n",
      "2025-05-29 21:03:49 [INFO]: epoch 87: training loss 0.1150\n",
      "2025-05-29 21:03:49 [INFO]: epoch 88: training loss 0.1236\n",
      "2025-05-29 21:03:49 [INFO]: epoch 89: training loss 0.1138\n",
      "2025-05-29 21:03:49 [INFO]: epoch 90: training loss 0.1122\n",
      "2025-05-29 21:03:49 [INFO]: epoch 91: training loss 0.1197\n",
      "2025-05-29 21:03:49 [INFO]: epoch 92: training loss 0.1052\n",
      "2025-05-29 21:03:49 [INFO]: epoch 93: training loss 0.1193\n",
      "2025-05-29 21:03:49 [INFO]: epoch 94: training loss 0.1086\n",
      "2025-05-29 21:03:49 [INFO]: epoch 95: training loss 0.1217\n",
      "2025-05-29 21:03:49 [INFO]: epoch 96: training loss 0.1130\n",
      "2025-05-29 21:03:49 [INFO]: epoch 97: training loss 0.1030\n",
      "2025-05-29 21:03:49 [INFO]: epoch 98: training loss 0.1103\n",
      "2025-05-29 21:03:49 [INFO]: epoch 99: training loss 0.1108\n",
      "2025-05-29 21:03:49 [INFO]: epoch 100: training loss 0.1096\n",
      "2025-05-29 21:03:49 [INFO]: epoch 101: training loss 0.1124\n",
      "2025-05-29 21:03:49 [INFO]: epoch 102: training loss 0.0986\n",
      "2025-05-29 21:03:49 [INFO]: epoch 103: training loss 0.0977\n",
      "2025-05-29 21:03:49 [INFO]: epoch 104: training loss 0.1006\n",
      "2025-05-29 21:03:49 [INFO]: epoch 105: training loss 0.0978\n",
      "2025-05-29 21:03:49 [INFO]: epoch 106: training loss 0.0842\n",
      "2025-05-29 21:03:49 [INFO]: epoch 107: training loss 0.0837\n",
      "2025-05-29 21:03:49 [INFO]: epoch 108: training loss 0.0944\n",
      "2025-05-29 21:03:49 [INFO]: epoch 109: training loss 0.0852\n",
      "2025-05-29 21:03:49 [INFO]: epoch 110: training loss 0.0977\n",
      "2025-05-29 21:03:49 [INFO]: epoch 111: training loss 0.0912\n",
      "2025-05-29 21:03:49 [INFO]: epoch 112: training loss 0.0765\n",
      "2025-05-29 21:03:49 [INFO]: epoch 113: training loss 0.0818\n",
      "2025-05-29 21:03:49 [INFO]: epoch 114: training loss 0.0772\n",
      "2025-05-29 21:03:49 [INFO]: epoch 115: training loss 0.0886\n",
      "2025-05-29 21:03:49 [INFO]: epoch 116: training loss 0.0896\n",
      "2025-05-29 21:03:49 [INFO]: epoch 117: training loss 0.0758\n",
      "2025-05-29 21:03:49 [INFO]: epoch 118: training loss 0.0820\n",
      "2025-05-29 21:03:49 [INFO]: epoch 119: training loss 0.0891\n",
      "2025-05-29 21:03:49 [INFO]: epoch 120: training loss 0.0800\n",
      "2025-05-29 21:03:49 [INFO]: epoch 121: training loss 0.0869\n",
      "2025-05-29 21:03:49 [INFO]: epoch 122: training loss 0.0819\n",
      "2025-05-29 21:03:49 [INFO]: epoch 123: training loss 0.0784\n",
      "2025-05-29 21:03:49 [INFO]: epoch 124: training loss 0.0946\n",
      "2025-05-29 21:03:49 [INFO]: epoch 125: training loss 0.0802\n",
      "2025-05-29 21:03:50 [INFO]: epoch 126: training loss 0.0857\n",
      "2025-05-29 21:03:50 [INFO]: epoch 127: training loss 0.0872\n",
      "2025-05-29 21:03:50 [INFO]: epoch 128: training loss 0.0592\n",
      "2025-05-29 21:03:50 [INFO]: epoch 129: training loss 0.0735\n",
      "2025-05-29 21:03:50 [INFO]: epoch 130: training loss 0.0808\n",
      "2025-05-29 21:03:50 [INFO]: epoch 131: training loss 0.0809\n",
      "2025-05-29 21:03:50 [INFO]: epoch 132: training loss 0.0762\n",
      "2025-05-29 21:03:50 [INFO]: epoch 133: training loss 0.0737\n",
      "2025-05-29 21:03:50 [INFO]: epoch 134: training loss 0.0707\n",
      "2025-05-29 21:03:50 [INFO]: epoch 135: training loss 0.0802\n",
      "2025-05-29 21:03:50 [INFO]: epoch 136: training loss 0.0946\n",
      "2025-05-29 21:03:50 [INFO]: epoch 137: training loss 0.0880\n",
      "2025-05-29 21:03:50 [INFO]: epoch 138: training loss 0.0799\n",
      "2025-05-29 21:03:50 [INFO]: epoch 139: training loss 0.0974\n",
      "2025-05-29 21:03:50 [INFO]: epoch 140: training loss 0.0797\n",
      "2025-05-29 21:03:50 [INFO]: epoch 141: training loss 0.0934\n",
      "2025-05-29 21:03:50 [INFO]: epoch 142: training loss 0.0795\n",
      "2025-05-29 21:03:50 [INFO]: epoch 143: training loss 0.0980\n",
      "2025-05-29 21:03:50 [INFO]: epoch 144: training loss 0.0792\n",
      "2025-05-29 21:03:50 [INFO]: epoch 145: training loss 0.0658\n",
      "2025-05-29 21:03:50 [INFO]: epoch 146: training loss 0.0922\n",
      "2025-05-29 21:03:50 [INFO]: epoch 147: training loss 0.0841\n",
      "2025-05-29 21:03:50 [INFO]: epoch 148: training loss 0.0655\n",
      "2025-05-29 21:03:50 [INFO]: epoch 149: training loss 0.0699\n",
      "2025-05-29 21:03:50 [INFO]: epoch 150: training loss 0.0734\n",
      "2025-05-29 21:03:50 [INFO]: epoch 151: training loss 0.0851\n",
      "2025-05-29 21:03:50 [INFO]: epoch 152: training loss 0.0849\n",
      "2025-05-29 21:03:50 [INFO]: epoch 153: training loss 0.0826\n",
      "2025-05-29 21:03:50 [INFO]: epoch 154: training loss 0.0802\n",
      "2025-05-29 21:03:50 [INFO]: epoch 155: training loss 0.0793\n",
      "2025-05-29 21:03:50 [INFO]: epoch 156: training loss 0.0669\n",
      "2025-05-29 21:03:50 [INFO]: epoch 157: training loss 0.0693\n",
      "2025-05-29 21:03:50 [INFO]: epoch 158: training loss 0.0828\n",
      "2025-05-29 21:03:50 [INFO]: epoch 159: training loss 0.0736\n",
      "2025-05-29 21:03:50 [INFO]: epoch 160: training loss 0.0643\n",
      "2025-05-29 21:03:50 [INFO]: epoch 161: training loss 0.0745\n",
      "2025-05-29 21:03:50 [INFO]: epoch 162: training loss 0.0666\n",
      "2025-05-29 21:03:50 [INFO]: epoch 163: training loss 0.0669\n",
      "2025-05-29 21:03:50 [INFO]: epoch 164: training loss 0.0636\n",
      "2025-05-29 21:03:50 [INFO]: epoch 165: training loss 0.0750\n",
      "2025-05-29 21:03:50 [INFO]: epoch 166: training loss 0.0784\n",
      "2025-05-29 21:03:50 [INFO]: epoch 167: training loss 0.0796\n",
      "2025-05-29 21:03:50 [INFO]: epoch 168: training loss 0.0695\n",
      "2025-05-29 21:03:50 [INFO]: epoch 169: training loss 0.0853\n",
      "2025-05-29 21:03:50 [INFO]: epoch 170: training loss 0.0726\n",
      "2025-05-29 21:03:50 [INFO]: epoch 171: training loss 0.0810\n",
      "2025-05-29 21:03:50 [INFO]: epoch 172: training loss 0.0835\n",
      "2025-05-29 21:03:50 [INFO]: epoch 173: training loss 0.0757\n",
      "2025-05-29 21:03:50 [INFO]: epoch 174: training loss 0.0530\n",
      "2025-05-29 21:03:50 [INFO]: epoch 175: training loss 0.0734\n",
      "2025-05-29 21:03:50 [INFO]: epoch 176: training loss 0.1038\n",
      "2025-05-29 21:03:50 [INFO]: epoch 177: training loss 0.0740\n",
      "2025-05-29 21:03:50 [INFO]: epoch 178: training loss 0.0757\n",
      "2025-05-29 21:03:50 [INFO]: epoch 179: training loss 0.0847\n",
      "2025-05-29 21:03:50 [INFO]: epoch 180: training loss 0.0760\n",
      "2025-05-29 21:03:50 [INFO]: epoch 181: training loss 0.0652\n",
      "2025-05-29 21:03:50 [INFO]: epoch 182: training loss 0.0762\n",
      "2025-05-29 21:03:50 [INFO]: epoch 183: training loss 0.0730\n",
      "2025-05-29 21:03:50 [INFO]: epoch 184: training loss 0.0634\n",
      "2025-05-29 21:03:50 [INFO]: epoch 185: training loss 0.0604\n",
      "2025-05-29 21:03:50 [INFO]: epoch 186: training loss 0.0821\n",
      "2025-05-29 21:03:50 [INFO]: epoch 187: training loss 0.0725\n",
      "2025-05-29 21:03:50 [INFO]: epoch 188: training loss 0.0612\n",
      "2025-05-29 21:03:50 [INFO]: epoch 189: training loss 0.0866\n",
      "2025-05-29 21:03:50 [INFO]: epoch 190: training loss 0.0733\n",
      "2025-05-29 21:03:50 [INFO]: epoch 191: training loss 0.0660\n",
      "2025-05-29 21:03:50 [INFO]: epoch 192: training loss 0.0681\n",
      "2025-05-29 21:03:50 [INFO]: epoch 193: training loss 0.0673\n",
      "2025-05-29 21:03:50 [INFO]: epoch 194: training loss 0.0602\n",
      "2025-05-29 21:03:50 [INFO]: epoch 195: training loss 0.0583\n",
      "2025-05-29 21:03:50 [INFO]: epoch 196: training loss 0.0720\n",
      "2025-05-29 21:03:50 [INFO]: epoch 197: training loss 0.0748\n",
      "2025-05-29 21:03:50 [INFO]: epoch 198: training loss 0.0591\n",
      "2025-05-29 21:03:50 [INFO]: epoch 199: training loss 0.0691\n",
      "2025-05-29 21:03:50 [INFO]: epoch 200: training loss 0.0733\n",
      "2025-05-29 21:03:50 [INFO]: epoch 201: training loss 0.0676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:03:50 [INFO]: epoch 202: training loss 0.0572\n",
      "2025-05-29 21:03:50 [INFO]: epoch 203: training loss 0.0603\n",
      "2025-05-29 21:03:50 [INFO]: epoch 204: training loss 0.0649\n",
      "2025-05-29 21:03:51 [INFO]: epoch 205: training loss 0.0670\n",
      "2025-05-29 21:03:51 [INFO]: epoch 206: training loss 0.0663\n",
      "2025-05-29 21:03:51 [INFO]: epoch 207: training loss 0.0704\n",
      "2025-05-29 21:03:51 [INFO]: epoch 208: training loss 0.0673\n",
      "2025-05-29 21:03:51 [INFO]: epoch 209: training loss 0.0651\n",
      "2025-05-29 21:03:51 [INFO]: epoch 210: training loss 0.0670\n",
      "2025-05-29 21:03:51 [INFO]: epoch 211: training loss 0.0654\n",
      "2025-05-29 21:03:51 [INFO]: epoch 212: training loss 0.0635\n",
      "2025-05-29 21:03:51 [INFO]: epoch 213: training loss 0.0587\n",
      "2025-05-29 21:03:51 [INFO]: epoch 214: training loss 0.0620\n",
      "2025-05-29 21:03:51 [INFO]: epoch 215: training loss 0.0713\n",
      "2025-05-29 21:03:51 [INFO]: epoch 216: training loss 0.0593\n",
      "2025-05-29 21:03:51 [INFO]: epoch 217: training loss 0.0585\n",
      "2025-05-29 21:03:51 [INFO]: epoch 218: training loss 0.0584\n",
      "2025-05-29 21:03:51 [INFO]: epoch 219: training loss 0.0680\n",
      "2025-05-29 21:03:51 [INFO]: epoch 220: training loss 0.0611\n",
      "2025-05-29 21:03:51 [INFO]: epoch 221: training loss 0.0544\n",
      "2025-05-29 21:03:51 [INFO]: epoch 222: training loss 0.0589\n",
      "2025-05-29 21:03:51 [INFO]: epoch 223: training loss 0.0702\n",
      "2025-05-29 21:03:51 [INFO]: epoch 224: training loss 0.0740\n",
      "2025-05-29 21:03:51 [INFO]: epoch 225: training loss 0.0637\n",
      "2025-05-29 21:03:51 [INFO]: epoch 226: training loss 0.0558\n",
      "2025-05-29 21:03:51 [INFO]: epoch 227: training loss 0.0583\n",
      "2025-05-29 21:03:51 [INFO]: epoch 228: training loss 0.0637\n",
      "2025-05-29 21:03:51 [INFO]: epoch 229: training loss 0.0566\n",
      "2025-05-29 21:03:51 [INFO]: epoch 230: training loss 0.0545\n",
      "2025-05-29 21:03:51 [INFO]: epoch 231: training loss 0.0540\n",
      "2025-05-29 21:03:51 [INFO]: epoch 232: training loss 0.0526\n",
      "2025-05-29 21:03:51 [INFO]: epoch 233: training loss 0.0570\n",
      "2025-05-29 21:03:51 [INFO]: epoch 234: training loss 0.0505\n",
      "2025-05-29 21:03:51 [INFO]: epoch 235: training loss 0.0482\n",
      "2025-05-29 21:03:51 [INFO]: epoch 236: training loss 0.0607\n",
      "2025-05-29 21:03:51 [INFO]: epoch 237: training loss 0.0476\n",
      "2025-05-29 21:03:51 [INFO]: epoch 238: training loss 0.0513\n",
      "2025-05-29 21:03:51 [INFO]: epoch 239: training loss 0.0526\n",
      "2025-05-29 21:03:51 [INFO]: epoch 240: training loss 0.0611\n",
      "2025-05-29 21:03:51 [INFO]: epoch 241: training loss 0.0593\n",
      "2025-05-29 21:03:51 [INFO]: epoch 242: training loss 0.0492\n",
      "2025-05-29 21:03:51 [INFO]: epoch 243: training loss 0.0520\n",
      "2025-05-29 21:03:51 [INFO]: epoch 244: training loss 0.0506\n",
      "2025-05-29 21:03:51 [INFO]: epoch 245: training loss 0.0565\n",
      "2025-05-29 21:03:51 [INFO]: epoch 246: training loss 0.0577\n",
      "2025-05-29 21:03:51 [INFO]: epoch 247: training loss 0.0520\n",
      "2025-05-29 21:03:51 [INFO]: epoch 248: training loss 0.0503\n",
      "2025-05-29 21:03:51 [INFO]: epoch 249: training loss 0.0493\n",
      "2025-05-29 21:03:51 [INFO]: epoch 250: training loss 0.0509\n",
      "2025-05-29 21:03:51 [INFO]: epoch 251: training loss 0.0392\n",
      "2025-05-29 21:03:51 [INFO]: epoch 252: training loss 0.0453\n",
      "2025-05-29 21:03:51 [INFO]: epoch 253: training loss 0.0549\n",
      "2025-05-29 21:03:51 [INFO]: epoch 254: training loss 0.0463\n",
      "2025-05-29 21:03:51 [INFO]: epoch 255: training loss 0.0456\n",
      "2025-05-29 21:03:51 [INFO]: epoch 256: training loss 0.0533\n",
      "2025-05-29 21:03:51 [INFO]: epoch 257: training loss 0.0579\n",
      "2025-05-29 21:03:51 [INFO]: epoch 258: training loss 0.0475\n",
      "2025-05-29 21:03:51 [INFO]: epoch 259: training loss 0.0493\n",
      "2025-05-29 21:03:51 [INFO]: epoch 260: training loss 0.0569\n",
      "2025-05-29 21:03:51 [INFO]: epoch 261: training loss 0.0479\n",
      "2025-05-29 21:03:51 [INFO]: epoch 262: training loss 0.0472\n",
      "2025-05-29 21:03:51 [INFO]: epoch 263: training loss 0.0467\n",
      "2025-05-29 21:03:51 [INFO]: epoch 264: training loss 0.0414\n",
      "2025-05-29 21:03:51 [INFO]: epoch 265: training loss 0.0423\n",
      "2025-05-29 21:03:51 [INFO]: epoch 266: training loss 0.0468\n",
      "2025-05-29 21:03:51 [INFO]: epoch 267: training loss 0.0439\n",
      "2025-05-29 21:03:51 [INFO]: epoch 268: training loss 0.0468\n",
      "2025-05-29 21:03:51 [INFO]: epoch 269: training loss 0.0463\n",
      "2025-05-29 21:03:51 [INFO]: epoch 270: training loss 0.0459\n",
      "2025-05-29 21:03:51 [INFO]: epoch 271: training loss 0.0492\n",
      "2025-05-29 21:03:51 [INFO]: epoch 272: training loss 0.0432\n",
      "2025-05-29 21:03:51 [INFO]: epoch 273: training loss 0.0428\n",
      "2025-05-29 21:03:51 [INFO]: epoch 274: training loss 0.0547\n",
      "2025-05-29 21:03:51 [INFO]: epoch 275: training loss 0.0515\n",
      "2025-05-29 21:03:51 [INFO]: epoch 276: training loss 0.0493\n",
      "2025-05-29 21:03:51 [INFO]: epoch 277: training loss 0.0478\n",
      "2025-05-29 21:03:51 [INFO]: epoch 278: training loss 0.0416\n",
      "2025-05-29 21:03:51 [INFO]: epoch 279: training loss 0.0469\n",
      "2025-05-29 21:03:51 [INFO]: epoch 280: training loss 0.0443\n",
      "2025-05-29 21:03:51 [INFO]: epoch 281: training loss 0.0484\n",
      "2025-05-29 21:03:51 [INFO]: epoch 282: training loss 0.0514\n",
      "2025-05-29 21:03:51 [INFO]: epoch 283: training loss 0.0426\n",
      "2025-05-29 21:03:51 [INFO]: epoch 284: training loss 0.0458\n",
      "2025-05-29 21:03:52 [INFO]: epoch 285: training loss 0.0530\n",
      "2025-05-29 21:03:52 [INFO]: epoch 286: training loss 0.0532\n",
      "2025-05-29 21:03:52 [INFO]: epoch 287: training loss 0.0442\n",
      "2025-05-29 21:03:52 [INFO]: epoch 288: training loss 0.0452\n",
      "2025-05-29 21:03:52 [INFO]: epoch 289: training loss 0.0479\n",
      "2025-05-29 21:03:52 [INFO]: epoch 290: training loss 0.0432\n",
      "2025-05-29 21:03:52 [INFO]: epoch 291: training loss 0.0381\n",
      "2025-05-29 21:03:52 [INFO]: epoch 292: training loss 0.0428\n",
      "2025-05-29 21:03:52 [INFO]: epoch 293: training loss 0.0394\n",
      "2025-05-29 21:03:52 [INFO]: epoch 294: training loss 0.0443\n",
      "2025-05-29 21:03:52 [INFO]: epoch 295: training loss 0.0383\n",
      "2025-05-29 21:03:52 [INFO]: epoch 296: training loss 0.0374\n",
      "2025-05-29 21:03:52 [INFO]: epoch 297: training loss 0.0368\n",
      "2025-05-29 21:03:52 [INFO]: epoch 298: training loss 0.0403\n",
      "2025-05-29 21:03:52 [INFO]: epoch 299: training loss 0.0360\n",
      "2025-05-29 21:03:52 [INFO]: Finished training.\n",
      "2025-05-29 21:03:52 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:11<00:07,  3.89s/it]2025-05-29 21:03:52 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:03:52 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:03:52 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:03:52 [INFO]: epoch 0: training loss 0.9274\n",
      "2025-05-29 21:03:52 [INFO]: epoch 1: training loss 0.7388\n",
      "2025-05-29 21:03:52 [INFO]: epoch 2: training loss 0.6340\n",
      "2025-05-29 21:03:52 [INFO]: epoch 3: training loss 0.5590\n",
      "2025-05-29 21:03:52 [INFO]: epoch 4: training loss 0.5757\n",
      "2025-05-29 21:03:52 [INFO]: epoch 5: training loss 0.4953\n",
      "2025-05-29 21:03:52 [INFO]: epoch 6: training loss 0.3636\n",
      "2025-05-29 21:03:52 [INFO]: epoch 7: training loss 0.3408\n",
      "2025-05-29 21:03:52 [INFO]: epoch 8: training loss 0.3792\n",
      "2025-05-29 21:03:52 [INFO]: epoch 9: training loss 0.3870\n",
      "2025-05-29 21:03:52 [INFO]: epoch 10: training loss 0.3911\n",
      "2025-05-29 21:03:52 [INFO]: epoch 11: training loss 0.3668\n",
      "2025-05-29 21:03:52 [INFO]: epoch 12: training loss 0.3457\n",
      "2025-05-29 21:03:52 [INFO]: epoch 13: training loss 0.3532\n",
      "2025-05-29 21:03:52 [INFO]: epoch 14: training loss 0.3357\n",
      "2025-05-29 21:03:52 [INFO]: epoch 15: training loss 0.2864\n",
      "2025-05-29 21:03:52 [INFO]: epoch 16: training loss 0.2842\n",
      "2025-05-29 21:03:52 [INFO]: epoch 17: training loss 0.3155\n",
      "2025-05-29 21:03:52 [INFO]: epoch 18: training loss 0.3439\n",
      "2025-05-29 21:03:52 [INFO]: epoch 19: training loss 0.2804\n",
      "2025-05-29 21:03:52 [INFO]: epoch 20: training loss 0.2444\n",
      "2025-05-29 21:03:52 [INFO]: epoch 21: training loss 0.2789\n",
      "2025-05-29 21:03:52 [INFO]: epoch 22: training loss 0.2583\n",
      "2025-05-29 21:03:52 [INFO]: epoch 23: training loss 0.2588\n",
      "2025-05-29 21:03:52 [INFO]: epoch 24: training loss 0.2801\n",
      "2025-05-29 21:03:52 [INFO]: epoch 25: training loss 0.2513\n",
      "2025-05-29 21:03:52 [INFO]: epoch 26: training loss 0.2341\n",
      "2025-05-29 21:03:52 [INFO]: epoch 27: training loss 0.2398\n",
      "2025-05-29 21:03:52 [INFO]: epoch 28: training loss 0.2462\n",
      "2025-05-29 21:03:52 [INFO]: epoch 29: training loss 0.2379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:03:52 [INFO]: epoch 30: training loss 0.2183\n",
      "2025-05-29 21:03:52 [INFO]: epoch 31: training loss 0.2117\n",
      "2025-05-29 21:03:52 [INFO]: epoch 32: training loss 0.2246\n",
      "2025-05-29 21:03:52 [INFO]: epoch 33: training loss 0.1958\n",
      "2025-05-29 21:03:52 [INFO]: epoch 34: training loss 0.2166\n",
      "2025-05-29 21:03:52 [INFO]: epoch 35: training loss 0.2033\n",
      "2025-05-29 21:03:52 [INFO]: epoch 36: training loss 0.2096\n",
      "2025-05-29 21:03:52 [INFO]: epoch 37: training loss 0.2051\n",
      "2025-05-29 21:03:52 [INFO]: epoch 38: training loss 0.2162\n",
      "2025-05-29 21:03:52 [INFO]: epoch 39: training loss 0.2115\n",
      "2025-05-29 21:03:52 [INFO]: epoch 40: training loss 0.2028\n",
      "2025-05-29 21:03:52 [INFO]: epoch 41: training loss 0.2032\n",
      "2025-05-29 21:03:52 [INFO]: epoch 42: training loss 0.1989\n",
      "2025-05-29 21:03:52 [INFO]: epoch 43: training loss 0.2209\n",
      "2025-05-29 21:03:52 [INFO]: epoch 44: training loss 0.1960\n",
      "2025-05-29 21:03:52 [INFO]: epoch 45: training loss 0.2157\n",
      "2025-05-29 21:03:52 [INFO]: epoch 46: training loss 0.2002\n",
      "2025-05-29 21:03:52 [INFO]: epoch 47: training loss 0.2096\n",
      "2025-05-29 21:03:52 [INFO]: epoch 48: training loss 0.1998\n",
      "2025-05-29 21:03:52 [INFO]: epoch 49: training loss 0.1907\n",
      "2025-05-29 21:03:52 [INFO]: epoch 50: training loss 0.1878\n",
      "2025-05-29 21:03:52 [INFO]: epoch 51: training loss 0.1968\n",
      "2025-05-29 21:03:52 [INFO]: epoch 52: training loss 0.1957\n",
      "2025-05-29 21:03:52 [INFO]: epoch 53: training loss 0.1944\n",
      "2025-05-29 21:03:52 [INFO]: epoch 54: training loss 0.1779\n",
      "2025-05-29 21:03:52 [INFO]: epoch 55: training loss 0.1976\n",
      "2025-05-29 21:03:52 [INFO]: epoch 56: training loss 0.1781\n",
      "2025-05-29 21:03:52 [INFO]: epoch 57: training loss 0.1827\n",
      "2025-05-29 21:03:52 [INFO]: epoch 58: training loss 0.2049\n",
      "2025-05-29 21:03:53 [INFO]: epoch 59: training loss 0.2010\n",
      "2025-05-29 21:03:53 [INFO]: epoch 60: training loss 0.1636\n",
      "2025-05-29 21:03:53 [INFO]: epoch 61: training loss 0.1709\n",
      "2025-05-29 21:03:53 [INFO]: epoch 62: training loss 0.1670\n",
      "2025-05-29 21:03:53 [INFO]: epoch 63: training loss 0.1680\n",
      "2025-05-29 21:03:53 [INFO]: epoch 64: training loss 0.1699\n",
      "2025-05-29 21:03:53 [INFO]: epoch 65: training loss 0.1509\n",
      "2025-05-29 21:03:53 [INFO]: epoch 66: training loss 0.1570\n",
      "2025-05-29 21:03:53 [INFO]: epoch 67: training loss 0.1588\n",
      "2025-05-29 21:03:53 [INFO]: epoch 68: training loss 0.1563\n",
      "2025-05-29 21:03:53 [INFO]: epoch 69: training loss 0.1576\n",
      "2025-05-29 21:03:53 [INFO]: epoch 70: training loss 0.1528\n",
      "2025-05-29 21:03:53 [INFO]: epoch 71: training loss 0.1937\n",
      "2025-05-29 21:03:53 [INFO]: epoch 72: training loss 0.1539\n",
      "2025-05-29 21:03:53 [INFO]: epoch 73: training loss 0.1766\n",
      "2025-05-29 21:03:53 [INFO]: epoch 74: training loss 0.1775\n",
      "2025-05-29 21:03:53 [INFO]: epoch 75: training loss 0.1647\n",
      "2025-05-29 21:03:53 [INFO]: epoch 76: training loss 0.1656\n",
      "2025-05-29 21:03:53 [INFO]: epoch 77: training loss 0.1689\n",
      "2025-05-29 21:03:53 [INFO]: epoch 78: training loss 0.1693\n",
      "2025-05-29 21:03:53 [INFO]: epoch 79: training loss 0.1712\n",
      "2025-05-29 21:03:53 [INFO]: epoch 80: training loss 0.1762\n",
      "2025-05-29 21:03:53 [INFO]: epoch 81: training loss 0.1518\n",
      "2025-05-29 21:03:53 [INFO]: epoch 82: training loss 0.1614\n",
      "2025-05-29 21:03:53 [INFO]: epoch 83: training loss 0.1442\n",
      "2025-05-29 21:03:53 [INFO]: epoch 84: training loss 0.1664\n",
      "2025-05-29 21:03:53 [INFO]: epoch 85: training loss 0.1731\n",
      "2025-05-29 21:03:53 [INFO]: epoch 86: training loss 0.1625\n",
      "2025-05-29 21:03:53 [INFO]: epoch 87: training loss 0.1493\n",
      "2025-05-29 21:03:53 [INFO]: epoch 88: training loss 0.1397\n",
      "2025-05-29 21:03:53 [INFO]: epoch 89: training loss 0.1752\n",
      "2025-05-29 21:03:53 [INFO]: epoch 90: training loss 0.1535\n",
      "2025-05-29 21:03:53 [INFO]: epoch 91: training loss 0.1670\n",
      "2025-05-29 21:03:53 [INFO]: epoch 92: training loss 0.1535\n",
      "2025-05-29 21:03:53 [INFO]: epoch 93: training loss 0.1377\n",
      "2025-05-29 21:03:53 [INFO]: epoch 94: training loss 0.1530\n",
      "2025-05-29 21:03:53 [INFO]: epoch 95: training loss 0.1482\n",
      "2025-05-29 21:03:53 [INFO]: epoch 96: training loss 0.1615\n",
      "2025-05-29 21:03:53 [INFO]: epoch 97: training loss 0.1402\n",
      "2025-05-29 21:03:53 [INFO]: epoch 98: training loss 0.1541\n",
      "2025-05-29 21:03:53 [INFO]: epoch 99: training loss 0.1637\n",
      "2025-05-29 21:03:53 [INFO]: epoch 100: training loss 0.1630\n",
      "2025-05-29 21:03:53 [INFO]: epoch 101: training loss 0.1402\n",
      "2025-05-29 21:03:53 [INFO]: epoch 102: training loss 0.1405\n",
      "2025-05-29 21:03:53 [INFO]: epoch 103: training loss 0.1649\n",
      "2025-05-29 21:03:53 [INFO]: epoch 104: training loss 0.1387\n",
      "2025-05-29 21:03:53 [INFO]: epoch 105: training loss 0.1435\n",
      "2025-05-29 21:03:53 [INFO]: epoch 106: training loss 0.1452\n",
      "2025-05-29 21:03:53 [INFO]: epoch 107: training loss 0.1282\n",
      "2025-05-29 21:03:53 [INFO]: epoch 108: training loss 0.1396\n",
      "2025-05-29 21:03:53 [INFO]: epoch 109: training loss 0.1361\n",
      "2025-05-29 21:03:53 [INFO]: epoch 110: training loss 0.1239\n",
      "2025-05-29 21:03:53 [INFO]: epoch 111: training loss 0.1218\n",
      "2025-05-29 21:03:53 [INFO]: epoch 112: training loss 0.1185\n",
      "2025-05-29 21:03:53 [INFO]: epoch 113: training loss 0.1135\n",
      "2025-05-29 21:03:53 [INFO]: epoch 114: training loss 0.1253\n",
      "2025-05-29 21:03:53 [INFO]: epoch 115: training loss 0.1422\n",
      "2025-05-29 21:03:53 [INFO]: epoch 116: training loss 0.1181\n",
      "2025-05-29 21:03:53 [INFO]: epoch 117: training loss 0.1224\n",
      "2025-05-29 21:03:53 [INFO]: epoch 118: training loss 0.1324\n",
      "2025-05-29 21:03:53 [INFO]: epoch 119: training loss 0.1311\n",
      "2025-05-29 21:03:53 [INFO]: epoch 120: training loss 0.1337\n",
      "2025-05-29 21:03:53 [INFO]: epoch 121: training loss 0.1134\n",
      "2025-05-29 21:03:53 [INFO]: epoch 122: training loss 0.1494\n",
      "2025-05-29 21:03:53 [INFO]: epoch 123: training loss 0.1373\n",
      "2025-05-29 21:03:53 [INFO]: epoch 124: training loss 0.1227\n",
      "2025-05-29 21:03:53 [INFO]: epoch 125: training loss 0.1293\n",
      "2025-05-29 21:03:53 [INFO]: epoch 126: training loss 0.1338\n",
      "2025-05-29 21:03:53 [INFO]: epoch 127: training loss 0.1301\n",
      "2025-05-29 21:03:53 [INFO]: epoch 128: training loss 0.1125\n",
      "2025-05-29 21:03:53 [INFO]: epoch 129: training loss 0.1145\n",
      "2025-05-29 21:03:53 [INFO]: epoch 130: training loss 0.1418\n",
      "2025-05-29 21:03:53 [INFO]: epoch 131: training loss 0.1297\n",
      "2025-05-29 21:03:53 [INFO]: epoch 132: training loss 0.1055\n",
      "2025-05-29 21:03:53 [INFO]: epoch 133: training loss 0.1048\n",
      "2025-05-29 21:03:53 [INFO]: epoch 134: training loss 0.1184\n",
      "2025-05-29 21:03:53 [INFO]: epoch 135: training loss 0.1609\n",
      "2025-05-29 21:03:53 [INFO]: epoch 136: training loss 0.1395\n",
      "2025-05-29 21:03:53 [INFO]: epoch 137: training loss 0.1031\n",
      "2025-05-29 21:03:53 [INFO]: epoch 138: training loss 0.1037\n",
      "2025-05-29 21:03:53 [INFO]: epoch 139: training loss 0.1134\n",
      "2025-05-29 21:03:53 [INFO]: epoch 140: training loss 0.1234\n",
      "2025-05-29 21:03:54 [INFO]: epoch 141: training loss 0.1017\n",
      "2025-05-29 21:03:54 [INFO]: epoch 142: training loss 0.1121\n",
      "2025-05-29 21:03:54 [INFO]: epoch 143: training loss 0.1383\n",
      "2025-05-29 21:03:54 [INFO]: epoch 144: training loss 0.1018\n",
      "2025-05-29 21:03:54 [INFO]: epoch 145: training loss 0.0753\n",
      "2025-05-29 21:03:54 [INFO]: epoch 146: training loss 0.0979\n",
      "2025-05-29 21:03:54 [INFO]: epoch 147: training loss 0.1228\n",
      "2025-05-29 21:03:54 [INFO]: epoch 148: training loss 0.1076\n",
      "2025-05-29 21:03:54 [INFO]: epoch 149: training loss 0.0908\n",
      "2025-05-29 21:03:54 [INFO]: epoch 150: training loss 0.1176\n",
      "2025-05-29 21:03:54 [INFO]: epoch 151: training loss 0.1063\n",
      "2025-05-29 21:03:54 [INFO]: epoch 152: training loss 0.1015\n",
      "2025-05-29 21:03:54 [INFO]: epoch 153: training loss 0.1112\n",
      "2025-05-29 21:03:54 [INFO]: epoch 154: training loss 0.1274\n",
      "2025-05-29 21:03:54 [INFO]: epoch 155: training loss 0.0971\n",
      "2025-05-29 21:03:54 [INFO]: epoch 156: training loss 0.0898\n",
      "2025-05-29 21:03:54 [INFO]: epoch 157: training loss 0.1198\n",
      "2025-05-29 21:03:54 [INFO]: epoch 158: training loss 0.1045\n",
      "2025-05-29 21:03:54 [INFO]: epoch 159: training loss 0.0824\n",
      "2025-05-29 21:03:54 [INFO]: epoch 160: training loss 0.0793\n",
      "2025-05-29 21:03:54 [INFO]: epoch 161: training loss 0.0987\n",
      "2025-05-29 21:03:54 [INFO]: epoch 162: training loss 0.1075\n",
      "2025-05-29 21:03:54 [INFO]: epoch 163: training loss 0.0877\n",
      "2025-05-29 21:03:54 [INFO]: epoch 164: training loss 0.0935\n",
      "2025-05-29 21:03:54 [INFO]: epoch 165: training loss 0.0957\n",
      "2025-05-29 21:03:54 [INFO]: epoch 166: training loss 0.1005\n",
      "2025-05-29 21:03:54 [INFO]: epoch 167: training loss 0.0854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:03:54 [INFO]: epoch 168: training loss 0.0928\n",
      "2025-05-29 21:03:54 [INFO]: epoch 169: training loss 0.1026\n",
      "2025-05-29 21:03:54 [INFO]: epoch 170: training loss 0.0909\n",
      "2025-05-29 21:03:54 [INFO]: epoch 171: training loss 0.0805\n",
      "2025-05-29 21:03:54 [INFO]: epoch 172: training loss 0.0928\n",
      "2025-05-29 21:03:54 [INFO]: epoch 173: training loss 0.1022\n",
      "2025-05-29 21:03:54 [INFO]: epoch 174: training loss 0.0802\n",
      "2025-05-29 21:03:54 [INFO]: epoch 175: training loss 0.0719\n",
      "2025-05-29 21:03:54 [INFO]: epoch 176: training loss 0.0916\n",
      "2025-05-29 21:03:54 [INFO]: epoch 177: training loss 0.0952\n",
      "2025-05-29 21:03:54 [INFO]: epoch 178: training loss 0.0805\n",
      "2025-05-29 21:03:54 [INFO]: epoch 179: training loss 0.0772\n",
      "2025-05-29 21:03:54 [INFO]: epoch 180: training loss 0.1010\n",
      "2025-05-29 21:03:54 [INFO]: epoch 181: training loss 0.0842\n",
      "2025-05-29 21:03:54 [INFO]: epoch 182: training loss 0.0810\n",
      "2025-05-29 21:03:54 [INFO]: epoch 183: training loss 0.1076\n",
      "2025-05-29 21:03:54 [INFO]: epoch 184: training loss 0.0940\n",
      "2025-05-29 21:03:54 [INFO]: epoch 185: training loss 0.0776\n",
      "2025-05-29 21:03:54 [INFO]: epoch 186: training loss 0.0826\n",
      "2025-05-29 21:03:54 [INFO]: epoch 187: training loss 0.0956\n",
      "2025-05-29 21:03:54 [INFO]: epoch 188: training loss 0.0972\n",
      "2025-05-29 21:03:54 [INFO]: epoch 189: training loss 0.0743\n",
      "2025-05-29 21:03:54 [INFO]: epoch 190: training loss 0.0782\n",
      "2025-05-29 21:03:54 [INFO]: epoch 191: training loss 0.0894\n",
      "2025-05-29 21:03:54 [INFO]: epoch 192: training loss 0.0813\n",
      "2025-05-29 21:03:54 [INFO]: epoch 193: training loss 0.0760\n",
      "2025-05-29 21:03:54 [INFO]: epoch 194: training loss 0.0676\n",
      "2025-05-29 21:03:54 [INFO]: epoch 195: training loss 0.0818\n",
      "2025-05-29 21:03:54 [INFO]: epoch 196: training loss 0.0809\n",
      "2025-05-29 21:03:54 [INFO]: epoch 197: training loss 0.0785\n",
      "2025-05-29 21:03:54 [INFO]: epoch 198: training loss 0.0770\n",
      "2025-05-29 21:03:54 [INFO]: epoch 199: training loss 0.0673\n",
      "2025-05-29 21:03:54 [INFO]: epoch 200: training loss 0.0769\n",
      "2025-05-29 21:03:54 [INFO]: epoch 201: training loss 0.0804\n",
      "2025-05-29 21:03:54 [INFO]: epoch 202: training loss 0.0711\n",
      "2025-05-29 21:03:54 [INFO]: epoch 203: training loss 0.0682\n",
      "2025-05-29 21:03:54 [INFO]: epoch 204: training loss 0.0663\n",
      "2025-05-29 21:03:54 [INFO]: epoch 205: training loss 0.0717\n",
      "2025-05-29 21:03:54 [INFO]: epoch 206: training loss 0.0712\n",
      "2025-05-29 21:03:54 [INFO]: epoch 207: training loss 0.0709\n",
      "2025-05-29 21:03:54 [INFO]: epoch 208: training loss 0.0624\n",
      "2025-05-29 21:03:54 [INFO]: epoch 209: training loss 0.0625\n",
      "2025-05-29 21:03:54 [INFO]: epoch 210: training loss 0.0688\n",
      "2025-05-29 21:03:54 [INFO]: epoch 211: training loss 0.0847\n",
      "2025-05-29 21:03:54 [INFO]: epoch 212: training loss 0.0559\n",
      "2025-05-29 21:03:54 [INFO]: epoch 213: training loss 0.0591\n",
      "2025-05-29 21:03:54 [INFO]: epoch 214: training loss 0.0667\n",
      "2025-05-29 21:03:54 [INFO]: epoch 215: training loss 0.0661\n",
      "2025-05-29 21:03:54 [INFO]: epoch 216: training loss 0.0548\n",
      "2025-05-29 21:03:54 [INFO]: epoch 217: training loss 0.0599\n",
      "2025-05-29 21:03:54 [INFO]: epoch 218: training loss 0.0655\n",
      "2025-05-29 21:03:54 [INFO]: epoch 219: training loss 0.0639\n",
      "2025-05-29 21:03:54 [INFO]: epoch 220: training loss 0.0570\n",
      "2025-05-29 21:03:54 [INFO]: epoch 221: training loss 0.0597\n",
      "2025-05-29 21:03:54 [INFO]: epoch 222: training loss 0.0696\n",
      "2025-05-29 21:03:54 [INFO]: epoch 223: training loss 0.0660\n",
      "2025-05-29 21:03:55 [INFO]: epoch 224: training loss 0.0659\n",
      "2025-05-29 21:03:55 [INFO]: epoch 225: training loss 0.0612\n",
      "2025-05-29 21:03:55 [INFO]: epoch 226: training loss 0.0626\n",
      "2025-05-29 21:03:55 [INFO]: epoch 227: training loss 0.0599\n",
      "2025-05-29 21:03:55 [INFO]: epoch 228: training loss 0.0590\n",
      "2025-05-29 21:03:55 [INFO]: epoch 229: training loss 0.0645\n",
      "2025-05-29 21:03:55 [INFO]: epoch 230: training loss 0.0785\n",
      "2025-05-29 21:03:55 [INFO]: epoch 231: training loss 0.0542\n",
      "2025-05-29 21:03:55 [INFO]: epoch 232: training loss 0.0614\n",
      "2025-05-29 21:03:55 [INFO]: epoch 233: training loss 0.0711\n",
      "2025-05-29 21:03:55 [INFO]: epoch 234: training loss 0.0552\n",
      "2025-05-29 21:03:55 [INFO]: epoch 235: training loss 0.0651\n",
      "2025-05-29 21:03:55 [INFO]: epoch 236: training loss 0.0536\n",
      "2025-05-29 21:03:55 [INFO]: epoch 237: training loss 0.0640\n",
      "2025-05-29 21:03:55 [INFO]: epoch 238: training loss 0.0598\n",
      "2025-05-29 21:03:55 [INFO]: epoch 239: training loss 0.0681\n",
      "2025-05-29 21:03:55 [INFO]: epoch 240: training loss 0.0551\n",
      "2025-05-29 21:03:55 [INFO]: epoch 241: training loss 0.0559\n",
      "2025-05-29 21:03:55 [INFO]: epoch 242: training loss 0.0590\n",
      "2025-05-29 21:03:55 [INFO]: epoch 243: training loss 0.0719\n",
      "2025-05-29 21:03:55 [INFO]: epoch 244: training loss 0.0552\n",
      "2025-05-29 21:03:55 [INFO]: epoch 245: training loss 0.0465\n",
      "2025-05-29 21:03:55 [INFO]: epoch 246: training loss 0.0557\n",
      "2025-05-29 21:03:55 [INFO]: epoch 247: training loss 0.0658\n",
      "2025-05-29 21:03:55 [INFO]: epoch 248: training loss 0.0535\n",
      "2025-05-29 21:03:55 [INFO]: epoch 249: training loss 0.0572\n",
      "2025-05-29 21:03:55 [INFO]: epoch 250: training loss 0.0498\n",
      "2025-05-29 21:03:55 [INFO]: epoch 251: training loss 0.0539\n",
      "2025-05-29 21:03:55 [INFO]: epoch 252: training loss 0.0600\n",
      "2025-05-29 21:03:55 [INFO]: epoch 253: training loss 0.0481\n",
      "2025-05-29 21:03:55 [INFO]: epoch 254: training loss 0.0490\n",
      "2025-05-29 21:03:55 [INFO]: epoch 255: training loss 0.0470\n",
      "2025-05-29 21:03:55 [INFO]: epoch 256: training loss 0.0607\n",
      "2025-05-29 21:03:55 [INFO]: epoch 257: training loss 0.0565\n",
      "2025-05-29 21:03:55 [INFO]: epoch 258: training loss 0.0485\n",
      "2025-05-29 21:03:55 [INFO]: epoch 259: training loss 0.0543\n",
      "2025-05-29 21:03:55 [INFO]: epoch 260: training loss 0.0506\n",
      "2025-05-29 21:03:55 [INFO]: epoch 261: training loss 0.0435\n",
      "2025-05-29 21:03:55 [INFO]: epoch 262: training loss 0.0706\n",
      "2025-05-29 21:03:55 [INFO]: epoch 263: training loss 0.0600\n",
      "2025-05-29 21:03:55 [INFO]: epoch 264: training loss 0.0483\n",
      "2025-05-29 21:03:55 [INFO]: epoch 265: training loss 0.0570\n",
      "2025-05-29 21:03:55 [INFO]: epoch 266: training loss 0.0680\n",
      "2025-05-29 21:03:55 [INFO]: epoch 267: training loss 0.0482\n",
      "2025-05-29 21:03:55 [INFO]: epoch 268: training loss 0.0563\n",
      "2025-05-29 21:03:55 [INFO]: epoch 269: training loss 0.0514\n",
      "2025-05-29 21:03:55 [INFO]: epoch 270: training loss 0.0576\n",
      "2025-05-29 21:03:55 [INFO]: epoch 271: training loss 0.0552\n",
      "2025-05-29 21:03:55 [INFO]: epoch 272: training loss 0.0442\n",
      "2025-05-29 21:03:55 [INFO]: epoch 273: training loss 0.0456\n",
      "2025-05-29 21:03:55 [INFO]: epoch 274: training loss 0.0570\n",
      "2025-05-29 21:03:55 [INFO]: epoch 275: training loss 0.0533\n",
      "2025-05-29 21:03:55 [INFO]: epoch 276: training loss 0.0457\n",
      "2025-05-29 21:03:55 [INFO]: epoch 277: training loss 0.0483\n",
      "2025-05-29 21:03:55 [INFO]: epoch 278: training loss 0.0464\n",
      "2025-05-29 21:03:55 [INFO]: epoch 279: training loss 0.0524\n",
      "2025-05-29 21:03:55 [INFO]: epoch 280: training loss 0.0450\n",
      "2025-05-29 21:03:55 [INFO]: epoch 281: training loss 0.0407\n",
      "2025-05-29 21:03:55 [INFO]: epoch 282: training loss 0.0470\n",
      "2025-05-29 21:03:55 [INFO]: epoch 283: training loss 0.0491\n",
      "2025-05-29 21:03:55 [INFO]: epoch 284: training loss 0.0506\n",
      "2025-05-29 21:03:55 [INFO]: epoch 285: training loss 0.0454\n",
      "2025-05-29 21:03:55 [INFO]: epoch 286: training loss 0.0497\n",
      "2025-05-29 21:03:55 [INFO]: epoch 287: training loss 0.0558\n",
      "2025-05-29 21:03:55 [INFO]: epoch 288: training loss 0.0575\n",
      "2025-05-29 21:03:55 [INFO]: epoch 289: training loss 0.0554\n",
      "2025-05-29 21:03:55 [INFO]: epoch 290: training loss 0.0463\n",
      "2025-05-29 21:03:55 [INFO]: epoch 291: training loss 0.0472\n",
      "2025-05-29 21:03:55 [INFO]: epoch 292: training loss 0.0572\n",
      "2025-05-29 21:03:55 [INFO]: epoch 293: training loss 0.0612\n",
      "2025-05-29 21:03:55 [INFO]: epoch 294: training loss 0.0645\n",
      "2025-05-29 21:03:55 [INFO]: epoch 295: training loss 0.0451\n",
      "2025-05-29 21:03:55 [INFO]: epoch 296: training loss 0.0465\n",
      "2025-05-29 21:03:55 [INFO]: epoch 297: training loss 0.0496\n",
      "2025-05-29 21:03:55 [INFO]: epoch 298: training loss 0.0575\n",
      "2025-05-29 21:03:55 [INFO]: epoch 299: training loss 0.0614\n",
      "2025-05-29 21:03:55 [INFO]: Finished training.\n",
      "2025-05-29 21:03:55 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:15<00:03,  3.83s/it]2025-05-29 21:03:55 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:03:55 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:03:56 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:03:56 [INFO]: epoch 0: training loss 1.1054\n",
      "2025-05-29 21:03:56 [INFO]: epoch 1: training loss 0.6087\n",
      "2025-05-29 21:03:56 [INFO]: epoch 2: training loss 0.6529\n",
      "2025-05-29 21:03:56 [INFO]: epoch 3: training loss 0.7324\n",
      "2025-05-29 21:03:56 [INFO]: epoch 4: training loss 0.7179\n",
      "2025-05-29 21:03:56 [INFO]: epoch 5: training loss 0.5980\n",
      "2025-05-29 21:03:56 [INFO]: epoch 6: training loss 0.4974\n",
      "2025-05-29 21:03:56 [INFO]: epoch 7: training loss 0.5055\n",
      "2025-05-29 21:03:56 [INFO]: epoch 8: training loss 0.5097\n",
      "2025-05-29 21:03:56 [INFO]: epoch 9: training loss 0.5320\n",
      "2025-05-29 21:03:56 [INFO]: epoch 10: training loss 0.5367\n",
      "2025-05-29 21:03:56 [INFO]: epoch 11: training loss 0.5430\n",
      "2025-05-29 21:03:56 [INFO]: epoch 12: training loss 0.4857\n",
      "2025-05-29 21:03:56 [INFO]: epoch 13: training loss 0.4417\n",
      "2025-05-29 21:03:56 [INFO]: epoch 14: training loss 0.4340\n",
      "2025-05-29 21:03:56 [INFO]: epoch 15: training loss 0.4422\n",
      "2025-05-29 21:03:56 [INFO]: epoch 16: training loss 0.4759\n",
      "2025-05-29 21:03:56 [INFO]: epoch 17: training loss 0.4312\n",
      "2025-05-29 21:03:56 [INFO]: epoch 18: training loss 0.4071\n",
      "2025-05-29 21:03:56 [INFO]: epoch 19: training loss 0.4271\n",
      "2025-05-29 21:03:56 [INFO]: epoch 20: training loss 0.4353\n",
      "2025-05-29 21:03:56 [INFO]: epoch 21: training loss 0.3848\n",
      "2025-05-29 21:03:56 [INFO]: epoch 22: training loss 0.4115\n",
      "2025-05-29 21:03:56 [INFO]: epoch 23: training loss 0.3704\n",
      "2025-05-29 21:03:56 [INFO]: epoch 24: training loss 0.3963\n",
      "2025-05-29 21:03:56 [INFO]: epoch 25: training loss 0.3787\n",
      "2025-05-29 21:03:56 [INFO]: epoch 26: training loss 0.4307\n",
      "2025-05-29 21:03:56 [INFO]: epoch 27: training loss 0.3981\n",
      "2025-05-29 21:03:56 [INFO]: epoch 28: training loss 0.3645\n",
      "2025-05-29 21:03:56 [INFO]: epoch 29: training loss 0.3866\n",
      "2025-05-29 21:03:56 [INFO]: epoch 30: training loss 0.3727\n",
      "2025-05-29 21:03:56 [INFO]: epoch 31: training loss 0.3535\n",
      "2025-05-29 21:03:56 [INFO]: epoch 32: training loss 0.3883\n",
      "2025-05-29 21:03:56 [INFO]: epoch 33: training loss 0.3718\n",
      "2025-05-29 21:03:56 [INFO]: epoch 34: training loss 0.3991\n",
      "2025-05-29 21:03:56 [INFO]: epoch 35: training loss 0.3902\n",
      "2025-05-29 21:03:56 [INFO]: epoch 36: training loss 0.3764\n",
      "2025-05-29 21:03:56 [INFO]: epoch 37: training loss 0.3487\n",
      "2025-05-29 21:03:56 [INFO]: epoch 38: training loss 0.3711\n",
      "2025-05-29 21:03:56 [INFO]: epoch 39: training loss 0.3603\n",
      "2025-05-29 21:03:56 [INFO]: epoch 40: training loss 0.3839\n",
      "2025-05-29 21:03:56 [INFO]: epoch 41: training loss 0.3510\n",
      "2025-05-29 21:03:56 [INFO]: epoch 42: training loss 0.3587\n",
      "2025-05-29 21:03:56 [INFO]: epoch 43: training loss 0.3734\n",
      "2025-05-29 21:03:56 [INFO]: epoch 44: training loss 0.3389\n",
      "2025-05-29 21:03:56 [INFO]: epoch 45: training loss 0.3333\n",
      "2025-05-29 21:03:56 [INFO]: epoch 46: training loss 0.3640\n",
      "2025-05-29 21:03:56 [INFO]: epoch 47: training loss 0.3502\n",
      "2025-05-29 21:03:56 [INFO]: epoch 48: training loss 0.3593\n",
      "2025-05-29 21:03:56 [INFO]: epoch 49: training loss 0.3961\n",
      "2025-05-29 21:03:56 [INFO]: epoch 50: training loss 0.3531\n",
      "2025-05-29 21:03:56 [INFO]: epoch 51: training loss 0.3484\n",
      "2025-05-29 21:03:56 [INFO]: epoch 52: training loss 0.3700\n",
      "2025-05-29 21:03:56 [INFO]: epoch 53: training loss 0.3402\n",
      "2025-05-29 21:03:56 [INFO]: epoch 54: training loss 0.3402\n",
      "2025-05-29 21:03:56 [INFO]: epoch 55: training loss 0.3482\n",
      "2025-05-29 21:03:56 [INFO]: epoch 56: training loss 0.3473\n",
      "2025-05-29 21:03:56 [INFO]: epoch 57: training loss 0.3192\n",
      "2025-05-29 21:03:56 [INFO]: epoch 58: training loss 0.3346\n",
      "2025-05-29 21:03:56 [INFO]: epoch 59: training loss 0.3258\n",
      "2025-05-29 21:03:56 [INFO]: epoch 60: training loss 0.3144\n",
      "2025-05-29 21:03:56 [INFO]: epoch 61: training loss 0.3246\n",
      "2025-05-29 21:03:56 [INFO]: epoch 62: training loss 0.3240\n",
      "2025-05-29 21:03:56 [INFO]: epoch 63: training loss 0.3125\n",
      "2025-05-29 21:03:56 [INFO]: epoch 64: training loss 0.3049\n",
      "2025-05-29 21:03:56 [INFO]: epoch 65: training loss 0.3250\n",
      "2025-05-29 21:03:56 [INFO]: epoch 66: training loss 0.2967\n",
      "2025-05-29 21:03:56 [INFO]: epoch 67: training loss 0.3071\n",
      "2025-05-29 21:03:56 [INFO]: epoch 68: training loss 0.3049\n",
      "2025-05-29 21:03:56 [INFO]: epoch 69: training loss 0.3221\n",
      "2025-05-29 21:03:56 [INFO]: epoch 70: training loss 0.3343\n",
      "2025-05-29 21:03:56 [INFO]: epoch 71: training loss 0.2945\n",
      "2025-05-29 21:03:56 [INFO]: epoch 72: training loss 0.3063\n",
      "2025-05-29 21:03:56 [INFO]: epoch 73: training loss 0.3033\n",
      "2025-05-29 21:03:56 [INFO]: epoch 74: training loss 0.2872\n",
      "2025-05-29 21:03:56 [INFO]: epoch 75: training loss 0.2867\n",
      "2025-05-29 21:03:56 [INFO]: epoch 76: training loss 0.2647\n",
      "2025-05-29 21:03:56 [INFO]: epoch 77: training loss 0.3093\n",
      "2025-05-29 21:03:56 [INFO]: epoch 78: training loss 0.3064\n",
      "2025-05-29 21:03:57 [INFO]: epoch 79: training loss 0.2926\n",
      "2025-05-29 21:03:57 [INFO]: epoch 80: training loss 0.2725\n",
      "2025-05-29 21:03:57 [INFO]: epoch 81: training loss 0.3220\n",
      "2025-05-29 21:03:57 [INFO]: epoch 82: training loss 0.2921\n",
      "2025-05-29 21:03:57 [INFO]: epoch 83: training loss 0.2779\n",
      "2025-05-29 21:03:57 [INFO]: epoch 84: training loss 0.2725\n",
      "2025-05-29 21:03:57 [INFO]: epoch 85: training loss 0.2935\n",
      "2025-05-29 21:03:57 [INFO]: epoch 86: training loss 0.2705\n",
      "2025-05-29 21:03:57 [INFO]: epoch 87: training loss 0.2547\n",
      "2025-05-29 21:03:57 [INFO]: epoch 88: training loss 0.2781\n",
      "2025-05-29 21:03:57 [INFO]: epoch 89: training loss 0.2628\n",
      "2025-05-29 21:03:57 [INFO]: epoch 90: training loss 0.2474\n",
      "2025-05-29 21:03:57 [INFO]: epoch 91: training loss 0.2469\n",
      "2025-05-29 21:03:57 [INFO]: epoch 92: training loss 0.2716\n",
      "2025-05-29 21:03:57 [INFO]: epoch 93: training loss 0.2768\n",
      "2025-05-29 21:03:57 [INFO]: epoch 94: training loss 0.2389\n",
      "2025-05-29 21:03:57 [INFO]: epoch 95: training loss 0.2539\n",
      "2025-05-29 21:03:57 [INFO]: epoch 96: training loss 0.2622\n",
      "2025-05-29 21:03:57 [INFO]: epoch 97: training loss 0.2648\n",
      "2025-05-29 21:03:57 [INFO]: epoch 98: training loss 0.2503\n",
      "2025-05-29 21:03:57 [INFO]: epoch 99: training loss 0.2489\n",
      "2025-05-29 21:03:57 [INFO]: epoch 100: training loss 0.2580\n",
      "2025-05-29 21:03:57 [INFO]: epoch 101: training loss 0.2646\n",
      "2025-05-29 21:03:57 [INFO]: epoch 102: training loss 0.2502\n",
      "2025-05-29 21:03:57 [INFO]: epoch 103: training loss 0.2262\n",
      "2025-05-29 21:03:57 [INFO]: epoch 104: training loss 0.2456\n",
      "2025-05-29 21:03:57 [INFO]: epoch 105: training loss 0.2500\n",
      "2025-05-29 21:03:57 [INFO]: epoch 106: training loss 0.2487\n",
      "2025-05-29 21:03:57 [INFO]: epoch 107: training loss 0.2330\n",
      "2025-05-29 21:03:57 [INFO]: epoch 108: training loss 0.2346\n",
      "2025-05-29 21:03:57 [INFO]: epoch 109: training loss 0.2242\n",
      "2025-05-29 21:03:57 [INFO]: epoch 110: training loss 0.2565\n",
      "2025-05-29 21:03:57 [INFO]: epoch 111: training loss 0.2529\n",
      "2025-05-29 21:03:57 [INFO]: epoch 112: training loss 0.2272\n",
      "2025-05-29 21:03:57 [INFO]: epoch 113: training loss 0.2321\n",
      "2025-05-29 21:03:57 [INFO]: epoch 114: training loss 0.2335\n",
      "2025-05-29 21:03:57 [INFO]: epoch 115: training loss 0.2361\n",
      "2025-05-29 21:03:57 [INFO]: epoch 116: training loss 0.2377\n",
      "2025-05-29 21:03:57 [INFO]: epoch 117: training loss 0.2694\n",
      "2025-05-29 21:03:57 [INFO]: epoch 118: training loss 0.2491\n",
      "2025-05-29 21:03:57 [INFO]: epoch 119: training loss 0.2331\n",
      "2025-05-29 21:03:57 [INFO]: epoch 120: training loss 0.2278\n",
      "2025-05-29 21:03:57 [INFO]: epoch 121: training loss 0.2429\n",
      "2025-05-29 21:03:57 [INFO]: epoch 122: training loss 0.2325\n",
      "2025-05-29 21:03:57 [INFO]: epoch 123: training loss 0.2289\n",
      "2025-05-29 21:03:57 [INFO]: epoch 124: training loss 0.2187\n",
      "2025-05-29 21:03:57 [INFO]: epoch 125: training loss 0.2304\n",
      "2025-05-29 21:03:57 [INFO]: epoch 126: training loss 0.2337\n",
      "2025-05-29 21:03:57 [INFO]: epoch 127: training loss 0.2366\n",
      "2025-05-29 21:03:57 [INFO]: epoch 128: training loss 0.2060\n",
      "2025-05-29 21:03:57 [INFO]: epoch 129: training loss 0.2218\n",
      "2025-05-29 21:03:57 [INFO]: epoch 130: training loss 0.2012\n",
      "2025-05-29 21:03:57 [INFO]: epoch 131: training loss 0.2203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:03:57 [INFO]: epoch 132: training loss 0.2234\n",
      "2025-05-29 21:03:57 [INFO]: epoch 133: training loss 0.2076\n",
      "2025-05-29 21:03:57 [INFO]: epoch 134: training loss 0.2131\n",
      "2025-05-29 21:03:57 [INFO]: epoch 135: training loss 0.2174\n",
      "2025-05-29 21:03:57 [INFO]: epoch 136: training loss 0.1972\n",
      "2025-05-29 21:03:57 [INFO]: epoch 137: training loss 0.2077\n",
      "2025-05-29 21:03:57 [INFO]: epoch 138: training loss 0.1926\n",
      "2025-05-29 21:03:57 [INFO]: epoch 139: training loss 0.1942\n",
      "2025-05-29 21:03:57 [INFO]: epoch 140: training loss 0.2084\n",
      "2025-05-29 21:03:57 [INFO]: epoch 141: training loss 0.2072\n",
      "2025-05-29 21:03:57 [INFO]: epoch 142: training loss 0.2024\n",
      "2025-05-29 21:03:57 [INFO]: epoch 143: training loss 0.2007\n",
      "2025-05-29 21:03:57 [INFO]: epoch 144: training loss 0.1866\n",
      "2025-05-29 21:03:57 [INFO]: epoch 145: training loss 0.2049\n",
      "2025-05-29 21:03:57 [INFO]: epoch 146: training loss 0.2078\n",
      "2025-05-29 21:03:57 [INFO]: epoch 147: training loss 0.2097\n",
      "2025-05-29 21:03:57 [INFO]: epoch 148: training loss 0.1896\n",
      "2025-05-29 21:03:57 [INFO]: epoch 149: training loss 0.1891\n",
      "2025-05-29 21:03:57 [INFO]: epoch 150: training loss 0.1987\n",
      "2025-05-29 21:03:57 [INFO]: epoch 151: training loss 0.1807\n",
      "2025-05-29 21:03:57 [INFO]: epoch 152: training loss 0.1969\n",
      "2025-05-29 21:03:57 [INFO]: epoch 153: training loss 0.1993\n",
      "2025-05-29 21:03:57 [INFO]: epoch 154: training loss 0.1959\n",
      "2025-05-29 21:03:57 [INFO]: epoch 155: training loss 0.1832\n",
      "2025-05-29 21:03:57 [INFO]: epoch 156: training loss 0.1833\n",
      "2025-05-29 21:03:57 [INFO]: epoch 157: training loss 0.1921\n",
      "2025-05-29 21:03:57 [INFO]: epoch 158: training loss 0.2081\n",
      "2025-05-29 21:03:58 [INFO]: epoch 159: training loss 0.1698\n",
      "2025-05-29 21:03:58 [INFO]: epoch 160: training loss 0.1830\n",
      "2025-05-29 21:03:58 [INFO]: epoch 161: training loss 0.1853\n",
      "2025-05-29 21:03:58 [INFO]: epoch 162: training loss 0.1864\n",
      "2025-05-29 21:03:58 [INFO]: epoch 163: training loss 0.1893\n",
      "2025-05-29 21:03:58 [INFO]: epoch 164: training loss 0.1708\n",
      "2025-05-29 21:03:58 [INFO]: epoch 165: training loss 0.1823\n",
      "2025-05-29 21:03:58 [INFO]: epoch 166: training loss 0.1770\n",
      "2025-05-29 21:03:58 [INFO]: epoch 167: training loss 0.1779\n",
      "2025-05-29 21:03:58 [INFO]: epoch 168: training loss 0.1775\n",
      "2025-05-29 21:03:58 [INFO]: epoch 169: training loss 0.1698\n",
      "2025-05-29 21:03:58 [INFO]: epoch 170: training loss 0.1712\n",
      "2025-05-29 21:03:58 [INFO]: epoch 171: training loss 0.1852\n",
      "2025-05-29 21:03:58 [INFO]: epoch 172: training loss 0.1750\n",
      "2025-05-29 21:03:58 [INFO]: epoch 173: training loss 0.1702\n",
      "2025-05-29 21:03:58 [INFO]: epoch 174: training loss 0.1698\n",
      "2025-05-29 21:03:58 [INFO]: epoch 175: training loss 0.1759\n",
      "2025-05-29 21:03:58 [INFO]: epoch 176: training loss 0.1798\n",
      "2025-05-29 21:03:58 [INFO]: epoch 177: training loss 0.1692\n",
      "2025-05-29 21:03:58 [INFO]: epoch 178: training loss 0.1848\n",
      "2025-05-29 21:03:58 [INFO]: epoch 179: training loss 0.1695\n",
      "2025-05-29 21:03:58 [INFO]: epoch 180: training loss 0.1714\n",
      "2025-05-29 21:03:58 [INFO]: epoch 181: training loss 0.1672\n",
      "2025-05-29 21:03:58 [INFO]: epoch 182: training loss 0.1689\n",
      "2025-05-29 21:03:58 [INFO]: epoch 183: training loss 0.1719\n",
      "2025-05-29 21:03:58 [INFO]: epoch 184: training loss 0.1754\n",
      "2025-05-29 21:03:58 [INFO]: epoch 185: training loss 0.1704\n",
      "2025-05-29 21:03:58 [INFO]: epoch 186: training loss 0.1799\n",
      "2025-05-29 21:03:58 [INFO]: epoch 187: training loss 0.1568\n",
      "2025-05-29 21:03:58 [INFO]: epoch 188: training loss 0.1633\n",
      "2025-05-29 21:03:58 [INFO]: epoch 189: training loss 0.1656\n",
      "2025-05-29 21:03:58 [INFO]: epoch 190: training loss 0.1810\n",
      "2025-05-29 21:03:58 [INFO]: epoch 191: training loss 0.1816\n",
      "2025-05-29 21:03:58 [INFO]: epoch 192: training loss 0.1606\n",
      "2025-05-29 21:03:58 [INFO]: epoch 193: training loss 0.1546\n",
      "2025-05-29 21:03:58 [INFO]: epoch 194: training loss 0.1696\n",
      "2025-05-29 21:03:58 [INFO]: epoch 195: training loss 0.1736\n",
      "2025-05-29 21:03:58 [INFO]: epoch 196: training loss 0.1821\n",
      "2025-05-29 21:03:58 [INFO]: epoch 197: training loss 0.1541\n",
      "2025-05-29 21:03:58 [INFO]: epoch 198: training loss 0.1552\n",
      "2025-05-29 21:03:58 [INFO]: epoch 199: training loss 0.1662\n",
      "2025-05-29 21:03:58 [INFO]: epoch 200: training loss 0.1706\n",
      "2025-05-29 21:03:58 [INFO]: epoch 201: training loss 0.1436\n",
      "2025-05-29 21:03:58 [INFO]: epoch 202: training loss 0.1438\n",
      "2025-05-29 21:03:58 [INFO]: epoch 203: training loss 0.1324\n",
      "2025-05-29 21:03:58 [INFO]: epoch 204: training loss 0.1547\n",
      "2025-05-29 21:03:58 [INFO]: epoch 205: training loss 0.1682\n",
      "2025-05-29 21:03:58 [INFO]: epoch 206: training loss 0.1486\n",
      "2025-05-29 21:03:58 [INFO]: epoch 207: training loss 0.1382\n",
      "2025-05-29 21:03:58 [INFO]: epoch 208: training loss 0.1590\n",
      "2025-05-29 21:03:58 [INFO]: epoch 209: training loss 0.1478\n",
      "2025-05-29 21:03:58 [INFO]: epoch 210: training loss 0.1504\n",
      "2025-05-29 21:03:58 [INFO]: epoch 211: training loss 0.1432\n",
      "2025-05-29 21:03:58 [INFO]: epoch 212: training loss 0.1518\n",
      "2025-05-29 21:03:58 [INFO]: epoch 213: training loss 0.1484\n",
      "2025-05-29 21:03:58 [INFO]: epoch 214: training loss 0.1533\n",
      "2025-05-29 21:03:58 [INFO]: epoch 215: training loss 0.1400\n",
      "2025-05-29 21:03:58 [INFO]: epoch 216: training loss 0.1355\n",
      "2025-05-29 21:03:58 [INFO]: epoch 217: training loss 0.1512\n",
      "2025-05-29 21:03:58 [INFO]: epoch 218: training loss 0.1336\n",
      "2025-05-29 21:03:58 [INFO]: epoch 219: training loss 0.1476\n",
      "2025-05-29 21:03:58 [INFO]: epoch 220: training loss 0.1550\n",
      "2025-05-29 21:03:58 [INFO]: epoch 221: training loss 0.1285\n",
      "2025-05-29 21:03:58 [INFO]: epoch 222: training loss 0.1396\n",
      "2025-05-29 21:03:58 [INFO]: epoch 223: training loss 0.1380\n",
      "2025-05-29 21:03:58 [INFO]: epoch 224: training loss 0.1475\n",
      "2025-05-29 21:03:58 [INFO]: epoch 225: training loss 0.1381\n",
      "2025-05-29 21:03:58 [INFO]: epoch 226: training loss 0.1605\n",
      "2025-05-29 21:03:58 [INFO]: epoch 227: training loss 0.1371\n",
      "2025-05-29 21:03:58 [INFO]: epoch 228: training loss 0.1608\n",
      "2025-05-29 21:03:58 [INFO]: epoch 229: training loss 0.1417\n",
      "2025-05-29 21:03:58 [INFO]: epoch 230: training loss 0.1340\n",
      "2025-05-29 21:03:58 [INFO]: epoch 231: training loss 0.1155\n",
      "2025-05-29 21:03:58 [INFO]: epoch 232: training loss 0.1466\n",
      "2025-05-29 21:03:58 [INFO]: epoch 233: training loss 0.1533\n",
      "2025-05-29 21:03:58 [INFO]: epoch 234: training loss 0.1469\n",
      "2025-05-29 21:03:58 [INFO]: epoch 235: training loss 0.1463\n",
      "2025-05-29 21:03:58 [INFO]: epoch 236: training loss 0.1393\n",
      "2025-05-29 21:03:58 [INFO]: epoch 237: training loss 0.1382\n",
      "2025-05-29 21:03:58 [INFO]: epoch 238: training loss 0.1372\n",
      "2025-05-29 21:03:58 [INFO]: epoch 239: training loss 0.1533\n",
      "2025-05-29 21:03:58 [INFO]: epoch 240: training loss 0.1578\n",
      "2025-05-29 21:03:59 [INFO]: epoch 241: training loss 0.1426\n",
      "2025-05-29 21:03:59 [INFO]: epoch 242: training loss 0.1441\n",
      "2025-05-29 21:03:59 [INFO]: epoch 243: training loss 0.1515\n",
      "2025-05-29 21:03:59 [INFO]: epoch 244: training loss 0.1413\n",
      "2025-05-29 21:03:59 [INFO]: epoch 245: training loss 0.1373\n",
      "2025-05-29 21:03:59 [INFO]: epoch 246: training loss 0.1412\n",
      "2025-05-29 21:03:59 [INFO]: epoch 247: training loss 0.1369\n",
      "2025-05-29 21:03:59 [INFO]: epoch 248: training loss 0.1321\n",
      "2025-05-29 21:03:59 [INFO]: epoch 249: training loss 0.1352\n",
      "2025-05-29 21:03:59 [INFO]: epoch 250: training loss 0.1276\n",
      "2025-05-29 21:03:59 [INFO]: epoch 251: training loss 0.1241\n",
      "2025-05-29 21:03:59 [INFO]: epoch 252: training loss 0.1097\n",
      "2025-05-29 21:03:59 [INFO]: epoch 253: training loss 0.1214\n",
      "2025-05-29 21:03:59 [INFO]: epoch 254: training loss 0.1352\n",
      "2025-05-29 21:03:59 [INFO]: epoch 255: training loss 0.1160\n",
      "2025-05-29 21:03:59 [INFO]: epoch 256: training loss 0.1106\n",
      "2025-05-29 21:03:59 [INFO]: epoch 257: training loss 0.1249\n",
      "2025-05-29 21:03:59 [INFO]: epoch 258: training loss 0.1129\n",
      "2025-05-29 21:03:59 [INFO]: epoch 259: training loss 0.1157\n",
      "2025-05-29 21:03:59 [INFO]: epoch 260: training loss 0.1175\n",
      "2025-05-29 21:03:59 [INFO]: epoch 261: training loss 0.1218\n",
      "2025-05-29 21:03:59 [INFO]: epoch 262: training loss 0.1241\n",
      "2025-05-29 21:03:59 [INFO]: epoch 263: training loss 0.1222\n",
      "2025-05-29 21:03:59 [INFO]: epoch 264: training loss 0.1211\n",
      "2025-05-29 21:03:59 [INFO]: epoch 265: training loss 0.1166\n",
      "2025-05-29 21:03:59 [INFO]: epoch 266: training loss 0.1021\n",
      "2025-05-29 21:03:59 [INFO]: epoch 267: training loss 0.1141\n",
      "2025-05-29 21:03:59 [INFO]: epoch 268: training loss 0.1121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:03:59 [INFO]: epoch 269: training loss 0.1227\n",
      "2025-05-29 21:03:59 [INFO]: epoch 270: training loss 0.1132\n",
      "2025-05-29 21:03:59 [INFO]: epoch 271: training loss 0.1049\n",
      "2025-05-29 21:03:59 [INFO]: epoch 272: training loss 0.1229\n",
      "2025-05-29 21:03:59 [INFO]: epoch 273: training loss 0.1151\n",
      "2025-05-29 21:03:59 [INFO]: epoch 274: training loss 0.1098\n",
      "2025-05-29 21:03:59 [INFO]: epoch 275: training loss 0.1074\n",
      "2025-05-29 21:03:59 [INFO]: epoch 276: training loss 0.1135\n",
      "2025-05-29 21:03:59 [INFO]: epoch 277: training loss 0.1132\n",
      "2025-05-29 21:03:59 [INFO]: epoch 278: training loss 0.1126\n",
      "2025-05-29 21:03:59 [INFO]: epoch 279: training loss 0.1148\n",
      "2025-05-29 21:03:59 [INFO]: epoch 280: training loss 0.1100\n",
      "2025-05-29 21:03:59 [INFO]: epoch 281: training loss 0.1039\n",
      "2025-05-29 21:03:59 [INFO]: epoch 282: training loss 0.1224\n",
      "2025-05-29 21:03:59 [INFO]: epoch 283: training loss 0.1195\n",
      "2025-05-29 21:03:59 [INFO]: epoch 284: training loss 0.1109\n",
      "2025-05-29 21:03:59 [INFO]: epoch 285: training loss 0.1053\n",
      "2025-05-29 21:03:59 [INFO]: epoch 286: training loss 0.1072\n",
      "2025-05-29 21:03:59 [INFO]: epoch 287: training loss 0.1075\n",
      "2025-05-29 21:03:59 [INFO]: epoch 288: training loss 0.1137\n",
      "2025-05-29 21:03:59 [INFO]: epoch 289: training loss 0.1096\n",
      "2025-05-29 21:03:59 [INFO]: epoch 290: training loss 0.1093\n",
      "2025-05-29 21:03:59 [INFO]: epoch 291: training loss 0.0945\n",
      "2025-05-29 21:03:59 [INFO]: epoch 292: training loss 0.1036\n",
      "2025-05-29 21:03:59 [INFO]: epoch 293: training loss 0.1093\n",
      "2025-05-29 21:03:59 [INFO]: epoch 294: training loss 0.1115\n",
      "2025-05-29 21:03:59 [INFO]: epoch 295: training loss 0.1042\n",
      "2025-05-29 21:03:59 [INFO]: epoch 296: training loss 0.0998\n",
      "2025-05-29 21:03:59 [INFO]: epoch 297: training loss 0.1056\n",
      "2025-05-29 21:03:59 [INFO]: epoch 298: training loss 0.1078\n",
      "2025-05-29 21:03:59 [INFO]: epoch 299: training loss 0.1143\n",
      "2025-05-29 21:03:59 [INFO]: Finished training.\n",
      "2025-05-29 21:03:59 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.84s/it]\n",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]2025-05-29 21:03:59 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:03:59 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:03:59 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:03:59 [INFO]: epoch 0: training loss 0.8504\n",
      "2025-05-29 21:03:59 [INFO]: epoch 1: training loss 0.6935\n",
      "2025-05-29 21:03:59 [INFO]: epoch 2: training loss 0.4922\n",
      "2025-05-29 21:03:59 [INFO]: epoch 3: training loss 0.5522\n",
      "2025-05-29 21:03:59 [INFO]: epoch 4: training loss 0.4344\n",
      "2025-05-29 21:03:59 [INFO]: epoch 5: training loss 0.3450\n",
      "2025-05-29 21:03:59 [INFO]: epoch 6: training loss 0.3432\n",
      "2025-05-29 21:03:59 [INFO]: epoch 7: training loss 0.3731\n",
      "2025-05-29 21:03:59 [INFO]: epoch 8: training loss 0.3436\n",
      "2025-05-29 21:03:59 [INFO]: epoch 9: training loss 0.3047\n",
      "2025-05-29 21:03:59 [INFO]: epoch 10: training loss 0.2919\n",
      "2025-05-29 21:03:59 [INFO]: epoch 11: training loss 0.2849\n",
      "2025-05-29 21:03:59 [INFO]: epoch 12: training loss 0.2509\n",
      "2025-05-29 21:03:59 [INFO]: epoch 13: training loss 0.2678\n",
      "2025-05-29 21:03:59 [INFO]: epoch 14: training loss 0.2157\n",
      "2025-05-29 21:04:00 [INFO]: epoch 15: training loss 0.2220\n",
      "2025-05-29 21:04:00 [INFO]: epoch 16: training loss 0.2550\n",
      "2025-05-29 21:04:00 [INFO]: epoch 17: training loss 0.2409\n",
      "2025-05-29 21:04:00 [INFO]: epoch 18: training loss 0.2200\n",
      "2025-05-29 21:04:00 [INFO]: epoch 19: training loss 0.2126\n",
      "2025-05-29 21:04:00 [INFO]: epoch 20: training loss 0.2134\n",
      "2025-05-29 21:04:00 [INFO]: epoch 21: training loss 0.2207\n",
      "2025-05-29 21:04:00 [INFO]: epoch 22: training loss 0.2002\n",
      "2025-05-29 21:04:00 [INFO]: epoch 23: training loss 0.1951\n",
      "2025-05-29 21:04:00 [INFO]: epoch 24: training loss 0.1984\n",
      "2025-05-29 21:04:00 [INFO]: epoch 25: training loss 0.2380\n",
      "2025-05-29 21:04:00 [INFO]: epoch 26: training loss 0.2267\n",
      "2025-05-29 21:04:00 [INFO]: epoch 27: training loss 0.1933\n",
      "2025-05-29 21:04:00 [INFO]: epoch 28: training loss 0.2011\n",
      "2025-05-29 21:04:00 [INFO]: epoch 29: training loss 0.2240\n",
      "2025-05-29 21:04:00 [INFO]: epoch 30: training loss 0.2043\n",
      "2025-05-29 21:04:00 [INFO]: epoch 31: training loss 0.1737\n",
      "2025-05-29 21:04:00 [INFO]: epoch 32: training loss 0.2062\n",
      "2025-05-29 21:04:00 [INFO]: epoch 33: training loss 0.2439\n",
      "2025-05-29 21:04:00 [INFO]: epoch 34: training loss 0.2225\n",
      "2025-05-29 21:04:00 [INFO]: epoch 35: training loss 0.1790\n",
      "2025-05-29 21:04:00 [INFO]: epoch 36: training loss 0.1844\n",
      "2025-05-29 21:04:00 [INFO]: epoch 37: training loss 0.1904\n",
      "2025-05-29 21:04:00 [INFO]: epoch 38: training loss 0.1915\n",
      "2025-05-29 21:04:00 [INFO]: epoch 39: training loss 0.1770\n",
      "2025-05-29 21:04:00 [INFO]: epoch 40: training loss 0.1864\n",
      "2025-05-29 21:04:00 [INFO]: epoch 41: training loss 0.1848\n",
      "2025-05-29 21:04:00 [INFO]: epoch 42: training loss 0.1767\n",
      "2025-05-29 21:04:00 [INFO]: epoch 43: training loss 0.1900\n",
      "2025-05-29 21:04:00 [INFO]: epoch 44: training loss 0.1869\n",
      "2025-05-29 21:04:00 [INFO]: epoch 45: training loss 0.1538\n",
      "2025-05-29 21:04:00 [INFO]: epoch 46: training loss 0.1438\n",
      "2025-05-29 21:04:00 [INFO]: epoch 47: training loss 0.1702\n",
      "2025-05-29 21:04:00 [INFO]: epoch 48: training loss 0.1747\n",
      "2025-05-29 21:04:00 [INFO]: epoch 49: training loss 0.1626\n",
      "2025-05-29 21:04:00 [INFO]: epoch 50: training loss 0.1733\n",
      "2025-05-29 21:04:00 [INFO]: epoch 51: training loss 0.1486\n",
      "2025-05-29 21:04:00 [INFO]: epoch 52: training loss 0.1603\n",
      "2025-05-29 21:04:00 [INFO]: epoch 53: training loss 0.1591\n",
      "2025-05-29 21:04:00 [INFO]: epoch 54: training loss 0.1643\n",
      "2025-05-29 21:04:00 [INFO]: epoch 55: training loss 0.1335\n",
      "2025-05-29 21:04:00 [INFO]: epoch 56: training loss 0.1473\n",
      "2025-05-29 21:04:00 [INFO]: epoch 57: training loss 0.1567\n",
      "2025-05-29 21:04:00 [INFO]: epoch 58: training loss 0.1610\n",
      "2025-05-29 21:04:00 [INFO]: epoch 59: training loss 0.1504\n",
      "2025-05-29 21:04:00 [INFO]: epoch 60: training loss 0.1384\n",
      "2025-05-29 21:04:00 [INFO]: epoch 61: training loss 0.1510\n",
      "2025-05-29 21:04:00 [INFO]: epoch 62: training loss 0.1682\n",
      "2025-05-29 21:04:00 [INFO]: epoch 63: training loss 0.1352\n",
      "2025-05-29 21:04:00 [INFO]: epoch 64: training loss 0.1219\n",
      "2025-05-29 21:04:00 [INFO]: epoch 65: training loss 0.1191\n",
      "2025-05-29 21:04:00 [INFO]: epoch 66: training loss 0.1351\n",
      "2025-05-29 21:04:00 [INFO]: epoch 67: training loss 0.1654\n",
      "2025-05-29 21:04:00 [INFO]: epoch 68: training loss 0.1475\n",
      "2025-05-29 21:04:00 [INFO]: epoch 69: training loss 0.1326\n",
      "2025-05-29 21:04:00 [INFO]: epoch 70: training loss 0.1252\n",
      "2025-05-29 21:04:00 [INFO]: epoch 71: training loss 0.1289\n",
      "2025-05-29 21:04:00 [INFO]: epoch 72: training loss 0.1364\n",
      "2025-05-29 21:04:00 [INFO]: epoch 73: training loss 0.1381\n",
      "2025-05-29 21:04:00 [INFO]: epoch 74: training loss 0.1553\n",
      "2025-05-29 21:04:00 [INFO]: epoch 75: training loss 0.1538\n",
      "2025-05-29 21:04:00 [INFO]: epoch 76: training loss 0.1409\n",
      "2025-05-29 21:04:00 [INFO]: epoch 77: training loss 0.1309\n",
      "2025-05-29 21:04:00 [INFO]: epoch 78: training loss 0.1446\n",
      "2025-05-29 21:04:00 [INFO]: epoch 79: training loss 0.1197\n",
      "2025-05-29 21:04:00 [INFO]: epoch 80: training loss 0.1289\n",
      "2025-05-29 21:04:00 [INFO]: epoch 81: training loss 0.1190\n",
      "2025-05-29 21:04:00 [INFO]: epoch 82: training loss 0.1451\n",
      "2025-05-29 21:04:00 [INFO]: epoch 83: training loss 0.1327\n",
      "2025-05-29 21:04:00 [INFO]: epoch 84: training loss 0.1210\n",
      "2025-05-29 21:04:00 [INFO]: epoch 85: training loss 0.1110\n",
      "2025-05-29 21:04:00 [INFO]: epoch 86: training loss 0.1457\n",
      "2025-05-29 21:04:00 [INFO]: epoch 87: training loss 0.1302\n",
      "2025-05-29 21:04:00 [INFO]: epoch 88: training loss 0.1153\n",
      "2025-05-29 21:04:00 [INFO]: epoch 89: training loss 0.1252\n",
      "2025-05-29 21:04:00 [INFO]: epoch 90: training loss 0.1412\n",
      "2025-05-29 21:04:00 [INFO]: epoch 91: training loss 0.1489\n",
      "2025-05-29 21:04:00 [INFO]: epoch 92: training loss 0.1438\n",
      "2025-05-29 21:04:00 [INFO]: epoch 93: training loss 0.1405\n",
      "2025-05-29 21:04:01 [INFO]: epoch 94: training loss 0.1195\n",
      "2025-05-29 21:04:01 [INFO]: epoch 95: training loss 0.1339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:01 [INFO]: epoch 96: training loss 0.1252\n",
      "2025-05-29 21:04:01 [INFO]: epoch 97: training loss 0.1161\n",
      "2025-05-29 21:04:01 [INFO]: epoch 98: training loss 0.1241\n",
      "2025-05-29 21:04:01 [INFO]: epoch 99: training loss 0.1149\n",
      "2025-05-29 21:04:01 [INFO]: epoch 100: training loss 0.1213\n",
      "2025-05-29 21:04:01 [INFO]: epoch 101: training loss 0.1282\n",
      "2025-05-29 21:04:01 [INFO]: epoch 102: training loss 0.1230\n",
      "2025-05-29 21:04:01 [INFO]: epoch 103: training loss 0.1162\n",
      "2025-05-29 21:04:01 [INFO]: epoch 104: training loss 0.1150\n",
      "2025-05-29 21:04:01 [INFO]: epoch 105: training loss 0.1040\n",
      "2025-05-29 21:04:01 [INFO]: epoch 106: training loss 0.1338\n",
      "2025-05-29 21:04:01 [INFO]: epoch 107: training loss 0.1107\n",
      "2025-05-29 21:04:01 [INFO]: epoch 108: training loss 0.1157\n",
      "2025-05-29 21:04:01 [INFO]: epoch 109: training loss 0.1190\n",
      "2025-05-29 21:04:01 [INFO]: epoch 110: training loss 0.1154\n",
      "2025-05-29 21:04:01 [INFO]: epoch 111: training loss 0.1170\n",
      "2025-05-29 21:04:01 [INFO]: epoch 112: training loss 0.1076\n",
      "2025-05-29 21:04:01 [INFO]: epoch 113: training loss 0.1215\n",
      "2025-05-29 21:04:01 [INFO]: epoch 114: training loss 0.1046\n",
      "2025-05-29 21:04:01 [INFO]: epoch 115: training loss 0.1175\n",
      "2025-05-29 21:04:01 [INFO]: epoch 116: training loss 0.1141\n",
      "2025-05-29 21:04:01 [INFO]: epoch 117: training loss 0.1111\n",
      "2025-05-29 21:04:01 [INFO]: epoch 118: training loss 0.1132\n",
      "2025-05-29 21:04:01 [INFO]: epoch 119: training loss 0.1275\n",
      "2025-05-29 21:04:01 [INFO]: epoch 120: training loss 0.1147\n",
      "2025-05-29 21:04:01 [INFO]: epoch 121: training loss 0.1125\n",
      "2025-05-29 21:04:01 [INFO]: epoch 122: training loss 0.1323\n",
      "2025-05-29 21:04:01 [INFO]: epoch 123: training loss 0.1355\n",
      "2025-05-29 21:04:01 [INFO]: epoch 124: training loss 0.1070\n",
      "2025-05-29 21:04:01 [INFO]: epoch 125: training loss 0.1251\n",
      "2025-05-29 21:04:01 [INFO]: epoch 126: training loss 0.1435\n",
      "2025-05-29 21:04:01 [INFO]: epoch 127: training loss 0.1191\n",
      "2025-05-29 21:04:01 [INFO]: epoch 128: training loss 0.0893\n",
      "2025-05-29 21:04:01 [INFO]: epoch 129: training loss 0.1362\n",
      "2025-05-29 21:04:01 [INFO]: epoch 130: training loss 0.0998\n",
      "2025-05-29 21:04:01 [INFO]: epoch 131: training loss 0.1001\n",
      "2025-05-29 21:04:01 [INFO]: epoch 132: training loss 0.1028\n",
      "2025-05-29 21:04:01 [INFO]: epoch 133: training loss 0.1095\n",
      "2025-05-29 21:04:01 [INFO]: epoch 134: training loss 0.0915\n",
      "2025-05-29 21:04:01 [INFO]: epoch 135: training loss 0.0920\n",
      "2025-05-29 21:04:01 [INFO]: epoch 136: training loss 0.1069\n",
      "2025-05-29 21:04:01 [INFO]: epoch 137: training loss 0.0898\n",
      "2025-05-29 21:04:01 [INFO]: epoch 138: training loss 0.0862\n",
      "2025-05-29 21:04:01 [INFO]: epoch 139: training loss 0.0991\n",
      "2025-05-29 21:04:01 [INFO]: epoch 140: training loss 0.0855\n",
      "2025-05-29 21:04:01 [INFO]: epoch 141: training loss 0.0987\n",
      "2025-05-29 21:04:01 [INFO]: epoch 142: training loss 0.1098\n",
      "2025-05-29 21:04:01 [INFO]: epoch 143: training loss 0.1021\n",
      "2025-05-29 21:04:01 [INFO]: epoch 144: training loss 0.1050\n",
      "2025-05-29 21:04:01 [INFO]: epoch 145: training loss 0.1030\n",
      "2025-05-29 21:04:01 [INFO]: epoch 146: training loss 0.0836\n",
      "2025-05-29 21:04:01 [INFO]: epoch 147: training loss 0.0909\n",
      "2025-05-29 21:04:01 [INFO]: epoch 148: training loss 0.0871\n",
      "2025-05-29 21:04:01 [INFO]: epoch 149: training loss 0.0925\n",
      "2025-05-29 21:04:01 [INFO]: epoch 150: training loss 0.0779\n",
      "2025-05-29 21:04:01 [INFO]: epoch 151: training loss 0.0819\n",
      "2025-05-29 21:04:01 [INFO]: epoch 152: training loss 0.0844\n",
      "2025-05-29 21:04:01 [INFO]: epoch 153: training loss 0.0885\n",
      "2025-05-29 21:04:01 [INFO]: epoch 154: training loss 0.0993\n",
      "2025-05-29 21:04:01 [INFO]: epoch 155: training loss 0.0829\n",
      "2025-05-29 21:04:01 [INFO]: epoch 156: training loss 0.0740\n",
      "2025-05-29 21:04:01 [INFO]: epoch 157: training loss 0.0834\n",
      "2025-05-29 21:04:01 [INFO]: epoch 158: training loss 0.0895\n",
      "2025-05-29 21:04:01 [INFO]: epoch 159: training loss 0.0791\n",
      "2025-05-29 21:04:01 [INFO]: epoch 160: training loss 0.0924\n",
      "2025-05-29 21:04:01 [INFO]: epoch 161: training loss 0.0919\n",
      "2025-05-29 21:04:01 [INFO]: epoch 162: training loss 0.0766\n",
      "2025-05-29 21:04:01 [INFO]: epoch 163: training loss 0.0804\n",
      "2025-05-29 21:04:01 [INFO]: epoch 164: training loss 0.0760\n",
      "2025-05-29 21:04:01 [INFO]: epoch 165: training loss 0.0850\n",
      "2025-05-29 21:04:01 [INFO]: epoch 166: training loss 0.0796\n",
      "2025-05-29 21:04:01 [INFO]: epoch 167: training loss 0.0939\n",
      "2025-05-29 21:04:01 [INFO]: epoch 168: training loss 0.0822\n",
      "2025-05-29 21:04:01 [INFO]: epoch 169: training loss 0.0841\n",
      "2025-05-29 21:04:01 [INFO]: epoch 170: training loss 0.0817\n",
      "2025-05-29 21:04:01 [INFO]: epoch 171: training loss 0.0875\n",
      "2025-05-29 21:04:01 [INFO]: epoch 172: training loss 0.0775\n",
      "2025-05-29 21:04:01 [INFO]: epoch 173: training loss 0.0862\n",
      "2025-05-29 21:04:02 [INFO]: epoch 174: training loss 0.0784\n",
      "2025-05-29 21:04:02 [INFO]: epoch 175: training loss 0.0765\n",
      "2025-05-29 21:04:02 [INFO]: epoch 176: training loss 0.0850\n",
      "2025-05-29 21:04:02 [INFO]: epoch 177: training loss 0.0787\n",
      "2025-05-29 21:04:02 [INFO]: epoch 178: training loss 0.0768\n",
      "2025-05-29 21:04:02 [INFO]: epoch 179: training loss 0.0710\n",
      "2025-05-29 21:04:02 [INFO]: epoch 180: training loss 0.0902\n",
      "2025-05-29 21:04:02 [INFO]: epoch 181: training loss 0.0915\n",
      "2025-05-29 21:04:02 [INFO]: epoch 182: training loss 0.0710\n",
      "2025-05-29 21:04:02 [INFO]: epoch 183: training loss 0.0751\n",
      "2025-05-29 21:04:02 [INFO]: epoch 184: training loss 0.0916\n",
      "2025-05-29 21:04:02 [INFO]: epoch 185: training loss 0.0796\n",
      "2025-05-29 21:04:02 [INFO]: epoch 186: training loss 0.0825\n",
      "2025-05-29 21:04:02 [INFO]: epoch 187: training loss 0.0757\n",
      "2025-05-29 21:04:02 [INFO]: epoch 188: training loss 0.0780\n",
      "2025-05-29 21:04:02 [INFO]: epoch 189: training loss 0.0702\n",
      "2025-05-29 21:04:02 [INFO]: epoch 190: training loss 0.0968\n",
      "2025-05-29 21:04:02 [INFO]: epoch 191: training loss 0.0792\n",
      "2025-05-29 21:04:02 [INFO]: epoch 192: training loss 0.0773\n",
      "2025-05-29 21:04:02 [INFO]: epoch 193: training loss 0.0759\n",
      "2025-05-29 21:04:02 [INFO]: epoch 194: training loss 0.0749\n",
      "2025-05-29 21:04:02 [INFO]: epoch 195: training loss 0.0760\n",
      "2025-05-29 21:04:02 [INFO]: epoch 196: training loss 0.0867\n",
      "2025-05-29 21:04:02 [INFO]: epoch 197: training loss 0.0815\n",
      "2025-05-29 21:04:02 [INFO]: epoch 198: training loss 0.0853\n",
      "2025-05-29 21:04:02 [INFO]: epoch 199: training loss 0.0747\n",
      "2025-05-29 21:04:02 [INFO]: epoch 200: training loss 0.0785\n",
      "2025-05-29 21:04:02 [INFO]: epoch 201: training loss 0.0775\n",
      "2025-05-29 21:04:02 [INFO]: epoch 202: training loss 0.0698\n",
      "2025-05-29 21:04:02 [INFO]: epoch 203: training loss 0.0731\n",
      "2025-05-29 21:04:02 [INFO]: epoch 204: training loss 0.0698\n",
      "2025-05-29 21:04:02 [INFO]: epoch 205: training loss 0.0731\n",
      "2025-05-29 21:04:02 [INFO]: epoch 206: training loss 0.0766\n",
      "2025-05-29 21:04:02 [INFO]: epoch 207: training loss 0.0698\n",
      "2025-05-29 21:04:02 [INFO]: epoch 208: training loss 0.0650\n",
      "2025-05-29 21:04:02 [INFO]: epoch 209: training loss 0.0875\n",
      "2025-05-29 21:04:02 [INFO]: epoch 210: training loss 0.0818\n",
      "2025-05-29 21:04:02 [INFO]: epoch 211: training loss 0.0805\n",
      "2025-05-29 21:04:02 [INFO]: epoch 212: training loss 0.0841\n",
      "2025-05-29 21:04:02 [INFO]: epoch 213: training loss 0.0726\n",
      "2025-05-29 21:04:02 [INFO]: epoch 214: training loss 0.0768\n",
      "2025-05-29 21:04:02 [INFO]: epoch 215: training loss 0.0705\n",
      "2025-05-29 21:04:02 [INFO]: epoch 216: training loss 0.0701\n",
      "2025-05-29 21:04:02 [INFO]: epoch 217: training loss 0.0814\n",
      "2025-05-29 21:04:02 [INFO]: epoch 218: training loss 0.0823\n",
      "2025-05-29 21:04:02 [INFO]: epoch 219: training loss 0.0766\n",
      "2025-05-29 21:04:02 [INFO]: epoch 220: training loss 0.0752\n",
      "2025-05-29 21:04:02 [INFO]: epoch 221: training loss 0.0794\n",
      "2025-05-29 21:04:02 [INFO]: epoch 222: training loss 0.0625\n",
      "2025-05-29 21:04:02 [INFO]: epoch 223: training loss 0.0591\n",
      "2025-05-29 21:04:02 [INFO]: epoch 224: training loss 0.0649\n",
      "2025-05-29 21:04:02 [INFO]: epoch 225: training loss 0.0681\n",
      "2025-05-29 21:04:02 [INFO]: epoch 226: training loss 0.0607\n",
      "2025-05-29 21:04:02 [INFO]: epoch 227: training loss 0.0598\n",
      "2025-05-29 21:04:02 [INFO]: epoch 228: training loss 0.0579\n",
      "2025-05-29 21:04:02 [INFO]: epoch 229: training loss 0.0554\n",
      "2025-05-29 21:04:02 [INFO]: epoch 230: training loss 0.0612\n",
      "2025-05-29 21:04:02 [INFO]: epoch 231: training loss 0.0709\n",
      "2025-05-29 21:04:02 [INFO]: epoch 232: training loss 0.0581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:02 [INFO]: epoch 233: training loss 0.0538\n",
      "2025-05-29 21:04:02 [INFO]: epoch 234: training loss 0.0565\n",
      "2025-05-29 21:04:02 [INFO]: epoch 235: training loss 0.0589\n",
      "2025-05-29 21:04:02 [INFO]: epoch 236: training loss 0.0556\n",
      "2025-05-29 21:04:02 [INFO]: epoch 237: training loss 0.0514\n",
      "2025-05-29 21:04:02 [INFO]: epoch 238: training loss 0.0530\n",
      "2025-05-29 21:04:02 [INFO]: epoch 239: training loss 0.0590\n",
      "2025-05-29 21:04:02 [INFO]: epoch 240: training loss 0.0569\n",
      "2025-05-29 21:04:02 [INFO]: epoch 241: training loss 0.0535\n",
      "2025-05-29 21:04:02 [INFO]: epoch 242: training loss 0.0598\n",
      "2025-05-29 21:04:02 [INFO]: epoch 243: training loss 0.0593\n",
      "2025-05-29 21:04:02 [INFO]: epoch 244: training loss 0.0591\n",
      "2025-05-29 21:04:02 [INFO]: epoch 245: training loss 0.0643\n",
      "2025-05-29 21:04:02 [INFO]: epoch 246: training loss 0.0667\n",
      "2025-05-29 21:04:02 [INFO]: epoch 247: training loss 0.0655\n",
      "2025-05-29 21:04:02 [INFO]: epoch 248: training loss 0.0674\n",
      "2025-05-29 21:04:02 [INFO]: epoch 249: training loss 0.0703\n",
      "2025-05-29 21:04:02 [INFO]: epoch 250: training loss 0.0614\n",
      "2025-05-29 21:04:02 [INFO]: epoch 251: training loss 0.0718\n",
      "2025-05-29 21:04:02 [INFO]: epoch 252: training loss 0.0611\n",
      "2025-05-29 21:04:02 [INFO]: epoch 253: training loss 0.0517\n",
      "2025-05-29 21:04:03 [INFO]: epoch 254: training loss 0.0628\n",
      "2025-05-29 21:04:03 [INFO]: epoch 255: training loss 0.0595\n",
      "2025-05-29 21:04:03 [INFO]: epoch 256: training loss 0.0494\n",
      "2025-05-29 21:04:03 [INFO]: epoch 257: training loss 0.0627\n",
      "2025-05-29 21:04:03 [INFO]: epoch 258: training loss 0.0615\n",
      "2025-05-29 21:04:03 [INFO]: epoch 259: training loss 0.0443\n",
      "2025-05-29 21:04:03 [INFO]: epoch 260: training loss 0.0512\n",
      "2025-05-29 21:04:03 [INFO]: epoch 261: training loss 0.0493\n",
      "2025-05-29 21:04:03 [INFO]: epoch 262: training loss 0.0471\n",
      "2025-05-29 21:04:03 [INFO]: epoch 263: training loss 0.0565\n",
      "2025-05-29 21:04:03 [INFO]: epoch 264: training loss 0.0677\n",
      "2025-05-29 21:04:03 [INFO]: epoch 265: training loss 0.0533\n",
      "2025-05-29 21:04:03 [INFO]: epoch 266: training loss 0.0601\n",
      "2025-05-29 21:04:03 [INFO]: epoch 267: training loss 0.0781\n",
      "2025-05-29 21:04:03 [INFO]: epoch 268: training loss 0.0675\n",
      "2025-05-29 21:04:03 [INFO]: epoch 269: training loss 0.0597\n",
      "2025-05-29 21:04:03 [INFO]: epoch 270: training loss 0.0597\n",
      "2025-05-29 21:04:03 [INFO]: epoch 271: training loss 0.0611\n",
      "2025-05-29 21:04:03 [INFO]: epoch 272: training loss 0.0650\n",
      "2025-05-29 21:04:03 [INFO]: epoch 273: training loss 0.0587\n",
      "2025-05-29 21:04:03 [INFO]: epoch 274: training loss 0.0480\n",
      "2025-05-29 21:04:03 [INFO]: epoch 275: training loss 0.0485\n",
      "2025-05-29 21:04:03 [INFO]: epoch 276: training loss 0.0603\n",
      "2025-05-29 21:04:03 [INFO]: epoch 277: training loss 0.0504\n",
      "2025-05-29 21:04:03 [INFO]: epoch 278: training loss 0.0571\n",
      "2025-05-29 21:04:03 [INFO]: epoch 279: training loss 0.0540\n",
      "2025-05-29 21:04:03 [INFO]: epoch 280: training loss 0.0548\n",
      "2025-05-29 21:04:03 [INFO]: epoch 281: training loss 0.0508\n",
      "2025-05-29 21:04:03 [INFO]: epoch 282: training loss 0.0587\n",
      "2025-05-29 21:04:03 [INFO]: epoch 283: training loss 0.0453\n",
      "2025-05-29 21:04:03 [INFO]: epoch 284: training loss 0.0432\n",
      "2025-05-29 21:04:03 [INFO]: epoch 285: training loss 0.0541\n",
      "2025-05-29 21:04:03 [INFO]: epoch 286: training loss 0.0440\n",
      "2025-05-29 21:04:03 [INFO]: epoch 287: training loss 0.0523\n",
      "2025-05-29 21:04:03 [INFO]: epoch 288: training loss 0.0464\n",
      "2025-05-29 21:04:03 [INFO]: epoch 289: training loss 0.0410\n",
      "2025-05-29 21:04:03 [INFO]: epoch 290: training loss 0.0469\n",
      "2025-05-29 21:04:03 [INFO]: epoch 291: training loss 0.0495\n",
      "2025-05-29 21:04:03 [INFO]: epoch 292: training loss 0.0454\n",
      "2025-05-29 21:04:03 [INFO]: epoch 293: training loss 0.0415\n",
      "2025-05-29 21:04:03 [INFO]: epoch 294: training loss 0.0437\n",
      "2025-05-29 21:04:03 [INFO]: epoch 295: training loss 0.0427\n",
      "2025-05-29 21:04:03 [INFO]: epoch 296: training loss 0.0451\n",
      "2025-05-29 21:04:03 [INFO]: epoch 297: training loss 0.0573\n",
      "2025-05-29 21:04:03 [INFO]: epoch 298: training loss 0.0600\n",
      "2025-05-29 21:04:03 [INFO]: epoch 299: training loss 0.0554\n",
      "2025-05-29 21:04:03 [INFO]: epoch 300: training loss 0.0528\n",
      "2025-05-29 21:04:03 [INFO]: epoch 301: training loss 0.0650\n",
      "2025-05-29 21:04:03 [INFO]: epoch 302: training loss 0.0583\n",
      "2025-05-29 21:04:03 [INFO]: epoch 303: training loss 0.0559\n",
      "2025-05-29 21:04:03 [INFO]: epoch 304: training loss 0.0643\n",
      "2025-05-29 21:04:03 [INFO]: epoch 305: training loss 0.0466\n",
      "2025-05-29 21:04:03 [INFO]: epoch 306: training loss 0.0425\n",
      "2025-05-29 21:04:03 [INFO]: epoch 307: training loss 0.0489\n",
      "2025-05-29 21:04:03 [INFO]: epoch 308: training loss 0.0552\n",
      "2025-05-29 21:04:03 [INFO]: epoch 309: training loss 0.0481\n",
      "2025-05-29 21:04:03 [INFO]: epoch 310: training loss 0.0388\n",
      "2025-05-29 21:04:03 [INFO]: epoch 311: training loss 0.0445\n",
      "2025-05-29 21:04:03 [INFO]: epoch 312: training loss 0.0496\n",
      "2025-05-29 21:04:03 [INFO]: epoch 313: training loss 0.0418\n",
      "2025-05-29 21:04:03 [INFO]: epoch 314: training loss 0.0479\n",
      "2025-05-29 21:04:03 [INFO]: epoch 315: training loss 0.0411\n",
      "2025-05-29 21:04:03 [INFO]: epoch 316: training loss 0.0428\n",
      "2025-05-29 21:04:03 [INFO]: epoch 317: training loss 0.0494\n",
      "2025-05-29 21:04:03 [INFO]: epoch 318: training loss 0.0367\n",
      "2025-05-29 21:04:03 [INFO]: epoch 319: training loss 0.0403\n",
      "2025-05-29 21:04:03 [INFO]: epoch 320: training loss 0.0429\n",
      "2025-05-29 21:04:03 [INFO]: epoch 321: training loss 0.0447\n",
      "2025-05-29 21:04:03 [INFO]: epoch 322: training loss 0.0406\n",
      "2025-05-29 21:04:03 [INFO]: epoch 323: training loss 0.0396\n",
      "2025-05-29 21:04:03 [INFO]: epoch 324: training loss 0.0450\n",
      "2025-05-29 21:04:03 [INFO]: epoch 325: training loss 0.0370\n",
      "2025-05-29 21:04:03 [INFO]: epoch 326: training loss 0.0431\n",
      "2025-05-29 21:04:03 [INFO]: epoch 327: training loss 0.0376\n",
      "2025-05-29 21:04:03 [INFO]: epoch 328: training loss 0.0357\n",
      "2025-05-29 21:04:03 [INFO]: epoch 329: training loss 0.0393\n",
      "2025-05-29 21:04:03 [INFO]: epoch 330: training loss 0.0330\n",
      "2025-05-29 21:04:03 [INFO]: epoch 331: training loss 0.0420\n",
      "2025-05-29 21:04:03 [INFO]: epoch 332: training loss 0.0373\n",
      "2025-05-29 21:04:03 [INFO]: epoch 333: training loss 0.0367\n",
      "2025-05-29 21:04:04 [INFO]: epoch 334: training loss 0.0388\n",
      "2025-05-29 21:04:04 [INFO]: epoch 335: training loss 0.0421\n",
      "2025-05-29 21:04:04 [INFO]: epoch 336: training loss 0.0393\n",
      "2025-05-29 21:04:04 [INFO]: epoch 337: training loss 0.0387\n",
      "2025-05-29 21:04:04 [INFO]: epoch 338: training loss 0.0426\n",
      "2025-05-29 21:04:04 [INFO]: epoch 339: training loss 0.0472\n",
      "2025-05-29 21:04:04 [INFO]: epoch 340: training loss 0.0373\n",
      "2025-05-29 21:04:04 [INFO]: epoch 341: training loss 0.0446\n",
      "2025-05-29 21:04:04 [INFO]: epoch 342: training loss 0.0430\n",
      "2025-05-29 21:04:04 [INFO]: epoch 343: training loss 0.0421\n",
      "2025-05-29 21:04:04 [INFO]: epoch 344: training loss 0.0331\n",
      "2025-05-29 21:04:04 [INFO]: epoch 345: training loss 0.0420\n",
      "2025-05-29 21:04:04 [INFO]: epoch 346: training loss 0.0399\n",
      "2025-05-29 21:04:04 [INFO]: epoch 347: training loss 0.0402\n",
      "2025-05-29 21:04:04 [INFO]: epoch 348: training loss 0.0362\n",
      "2025-05-29 21:04:04 [INFO]: epoch 349: training loss 0.0466\n",
      "2025-05-29 21:04:04 [INFO]: epoch 350: training loss 0.0441\n",
      "2025-05-29 21:04:04 [INFO]: epoch 351: training loss 0.0389\n",
      "2025-05-29 21:04:04 [INFO]: epoch 352: training loss 0.0351\n",
      "2025-05-29 21:04:04 [INFO]: epoch 353: training loss 0.0406\n",
      "2025-05-29 21:04:04 [INFO]: epoch 354: training loss 0.0388\n",
      "2025-05-29 21:04:04 [INFO]: epoch 355: training loss 0.0388\n",
      "2025-05-29 21:04:04 [INFO]: epoch 356: training loss 0.0368\n",
      "2025-05-29 21:04:04 [INFO]: epoch 357: training loss 0.0381\n",
      "2025-05-29 21:04:04 [INFO]: epoch 358: training loss 0.0342\n",
      "2025-05-29 21:04:04 [INFO]: epoch 359: training loss 0.0384\n",
      "2025-05-29 21:04:04 [INFO]: epoch 360: training loss 0.0361\n",
      "2025-05-29 21:04:04 [INFO]: epoch 361: training loss 0.0413\n",
      "2025-05-29 21:04:04 [INFO]: epoch 362: training loss 0.0368\n",
      "2025-05-29 21:04:04 [INFO]: epoch 363: training loss 0.0404\n",
      "2025-05-29 21:04:04 [INFO]: epoch 364: training loss 0.0382\n",
      "2025-05-29 21:04:04 [INFO]: epoch 365: training loss 0.0355\n",
      "2025-05-29 21:04:04 [INFO]: epoch 366: training loss 0.0364\n",
      "2025-05-29 21:04:04 [INFO]: epoch 367: training loss 0.0391\n",
      "2025-05-29 21:04:04 [INFO]: epoch 368: training loss 0.0407\n",
      "2025-05-29 21:04:04 [INFO]: epoch 369: training loss 0.0363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:04 [INFO]: epoch 370: training loss 0.0348\n",
      "2025-05-29 21:04:04 [INFO]: epoch 371: training loss 0.0391\n",
      "2025-05-29 21:04:04 [INFO]: epoch 372: training loss 0.0354\n",
      "2025-05-29 21:04:04 [INFO]: epoch 373: training loss 0.0317\n",
      "2025-05-29 21:04:04 [INFO]: epoch 374: training loss 0.0418\n",
      "2025-05-29 21:04:04 [INFO]: epoch 375: training loss 0.0354\n",
      "2025-05-29 21:04:04 [INFO]: epoch 376: training loss 0.0340\n",
      "2025-05-29 21:04:04 [INFO]: epoch 377: training loss 0.0346\n",
      "2025-05-29 21:04:04 [INFO]: epoch 378: training loss 0.0334\n",
      "2025-05-29 21:04:04 [INFO]: epoch 379: training loss 0.0377\n",
      "2025-05-29 21:04:04 [INFO]: epoch 380: training loss 0.0329\n",
      "2025-05-29 21:04:04 [INFO]: epoch 381: training loss 0.0323\n",
      "2025-05-29 21:04:04 [INFO]: epoch 382: training loss 0.0404\n",
      "2025-05-29 21:04:04 [INFO]: epoch 383: training loss 0.0340\n",
      "2025-05-29 21:04:04 [INFO]: epoch 384: training loss 0.0485\n",
      "2025-05-29 21:04:04 [INFO]: epoch 385: training loss 0.0329\n",
      "2025-05-29 21:04:04 [INFO]: epoch 386: training loss 0.0334\n",
      "2025-05-29 21:04:04 [INFO]: epoch 387: training loss 0.0396\n",
      "2025-05-29 21:04:04 [INFO]: epoch 388: training loss 0.0359\n",
      "2025-05-29 21:04:04 [INFO]: epoch 389: training loss 0.0377\n",
      "2025-05-29 21:04:04 [INFO]: epoch 390: training loss 0.0412\n",
      "2025-05-29 21:04:04 [INFO]: epoch 391: training loss 0.0377\n",
      "2025-05-29 21:04:04 [INFO]: epoch 392: training loss 0.0318\n",
      "2025-05-29 21:04:04 [INFO]: epoch 393: training loss 0.0442\n",
      "2025-05-29 21:04:04 [INFO]: epoch 394: training loss 0.0339\n",
      "2025-05-29 21:04:04 [INFO]: epoch 395: training loss 0.0438\n",
      "2025-05-29 21:04:04 [INFO]: epoch 396: training loss 0.0378\n",
      "2025-05-29 21:04:04 [INFO]: epoch 397: training loss 0.0358\n",
      "2025-05-29 21:04:04 [INFO]: epoch 398: training loss 0.0398\n",
      "2025-05-29 21:04:04 [INFO]: epoch 399: training loss 0.0370\n",
      "2025-05-29 21:04:04 [INFO]: epoch 400: training loss 0.0394\n",
      "2025-05-29 21:04:04 [INFO]: epoch 401: training loss 0.0474\n",
      "2025-05-29 21:04:04 [INFO]: epoch 402: training loss 0.0399\n",
      "2025-05-29 21:04:04 [INFO]: epoch 403: training loss 0.0352\n",
      "2025-05-29 21:04:04 [INFO]: epoch 404: training loss 0.0421\n",
      "2025-05-29 21:04:04 [INFO]: epoch 405: training loss 0.0520\n",
      "2025-05-29 21:04:04 [INFO]: epoch 406: training loss 0.0395\n",
      "2025-05-29 21:04:04 [INFO]: epoch 407: training loss 0.0325\n",
      "2025-05-29 21:04:04 [INFO]: epoch 408: training loss 0.0619\n",
      "2025-05-29 21:04:04 [INFO]: epoch 409: training loss 0.0468\n",
      "2025-05-29 21:04:04 [INFO]: epoch 410: training loss 0.0379\n",
      "2025-05-29 21:04:04 [INFO]: epoch 411: training loss 0.0464\n",
      "2025-05-29 21:04:04 [INFO]: epoch 412: training loss 0.0487\n",
      "2025-05-29 21:04:05 [INFO]: epoch 413: training loss 0.0434\n",
      "2025-05-29 21:04:05 [INFO]: epoch 414: training loss 0.0404\n",
      "2025-05-29 21:04:05 [INFO]: epoch 415: training loss 0.0307\n",
      "2025-05-29 21:04:05 [INFO]: epoch 416: training loss 0.0305\n",
      "2025-05-29 21:04:05 [INFO]: epoch 417: training loss 0.0456\n",
      "2025-05-29 21:04:05 [INFO]: epoch 418: training loss 0.0405\n",
      "2025-05-29 21:04:05 [INFO]: epoch 419: training loss 0.0301\n",
      "2025-05-29 21:04:05 [INFO]: epoch 420: training loss 0.0388\n",
      "2025-05-29 21:04:05 [INFO]: epoch 421: training loss 0.0348\n",
      "2025-05-29 21:04:05 [INFO]: epoch 422: training loss 0.0382\n",
      "2025-05-29 21:04:05 [INFO]: epoch 423: training loss 0.0341\n",
      "2025-05-29 21:04:05 [INFO]: epoch 424: training loss 0.0340\n",
      "2025-05-29 21:04:05 [INFO]: epoch 425: training loss 0.0386\n",
      "2025-05-29 21:04:05 [INFO]: epoch 426: training loss 0.0504\n",
      "2025-05-29 21:04:05 [INFO]: epoch 427: training loss 0.0457\n",
      "2025-05-29 21:04:05 [INFO]: epoch 428: training loss 0.0328\n",
      "2025-05-29 21:04:05 [INFO]: epoch 429: training loss 0.0401\n",
      "2025-05-29 21:04:05 [INFO]: epoch 430: training loss 0.0406\n",
      "2025-05-29 21:04:05 [INFO]: epoch 431: training loss 0.0303\n",
      "2025-05-29 21:04:05 [INFO]: epoch 432: training loss 0.0346\n",
      "2025-05-29 21:04:05 [INFO]: epoch 433: training loss 0.0452\n",
      "2025-05-29 21:04:05 [INFO]: epoch 434: training loss 0.0378\n",
      "2025-05-29 21:04:05 [INFO]: epoch 435: training loss 0.0336\n",
      "2025-05-29 21:04:05 [INFO]: epoch 436: training loss 0.0358\n",
      "2025-05-29 21:04:05 [INFO]: epoch 437: training loss 0.0346\n",
      "2025-05-29 21:04:05 [INFO]: epoch 438: training loss 0.0346\n",
      "2025-05-29 21:04:05 [INFO]: epoch 439: training loss 0.0328\n",
      "2025-05-29 21:04:05 [INFO]: epoch 440: training loss 0.0418\n",
      "2025-05-29 21:04:05 [INFO]: epoch 441: training loss 0.0339\n",
      "2025-05-29 21:04:05 [INFO]: epoch 442: training loss 0.0344\n",
      "2025-05-29 21:04:05 [INFO]: epoch 443: training loss 0.0294\n",
      "2025-05-29 21:04:05 [INFO]: epoch 444: training loss 0.0438\n",
      "2025-05-29 21:04:05 [INFO]: epoch 445: training loss 0.0374\n",
      "2025-05-29 21:04:05 [INFO]: epoch 446: training loss 0.0389\n",
      "2025-05-29 21:04:05 [INFO]: epoch 447: training loss 0.0362\n",
      "2025-05-29 21:04:05 [INFO]: epoch 448: training loss 0.0342\n",
      "2025-05-29 21:04:05 [INFO]: epoch 449: training loss 0.0398\n",
      "2025-05-29 21:04:05 [INFO]: epoch 450: training loss 0.0364\n",
      "2025-05-29 21:04:05 [INFO]: epoch 451: training loss 0.0255\n",
      "2025-05-29 21:04:05 [INFO]: epoch 452: training loss 0.0320\n",
      "2025-05-29 21:04:05 [INFO]: epoch 453: training loss 0.0324\n",
      "2025-05-29 21:04:05 [INFO]: epoch 454: training loss 0.0331\n",
      "2025-05-29 21:04:05 [INFO]: epoch 455: training loss 0.0364\n",
      "2025-05-29 21:04:05 [INFO]: epoch 456: training loss 0.0301\n",
      "2025-05-29 21:04:05 [INFO]: epoch 457: training loss 0.0289\n",
      "2025-05-29 21:04:05 [INFO]: epoch 458: training loss 0.0334\n",
      "2025-05-29 21:04:05 [INFO]: epoch 459: training loss 0.0294\n",
      "2025-05-29 21:04:05 [INFO]: epoch 460: training loss 0.0341\n",
      "2025-05-29 21:04:05 [INFO]: epoch 461: training loss 0.0338\n",
      "2025-05-29 21:04:05 [INFO]: epoch 462: training loss 0.0312\n",
      "2025-05-29 21:04:05 [INFO]: epoch 463: training loss 0.0285\n",
      "2025-05-29 21:04:05 [INFO]: epoch 464: training loss 0.0332\n",
      "2025-05-29 21:04:05 [INFO]: epoch 465: training loss 0.0312\n",
      "2025-05-29 21:04:05 [INFO]: epoch 466: training loss 0.0319\n",
      "2025-05-29 21:04:05 [INFO]: epoch 467: training loss 0.0305\n",
      "2025-05-29 21:04:05 [INFO]: epoch 468: training loss 0.0305\n",
      "2025-05-29 21:04:05 [INFO]: epoch 469: training loss 0.0333\n",
      "2025-05-29 21:04:05 [INFO]: epoch 470: training loss 0.0274\n",
      "2025-05-29 21:04:05 [INFO]: epoch 471: training loss 0.0271\n",
      "2025-05-29 21:04:05 [INFO]: epoch 472: training loss 0.0327\n",
      "2025-05-29 21:04:05 [INFO]: epoch 473: training loss 0.0327\n",
      "2025-05-29 21:04:05 [INFO]: epoch 474: training loss 0.0303\n",
      "2025-05-29 21:04:05 [INFO]: epoch 475: training loss 0.0263\n",
      "2025-05-29 21:04:05 [INFO]: epoch 476: training loss 0.0274\n",
      "2025-05-29 21:04:05 [INFO]: epoch 477: training loss 0.0278\n",
      "2025-05-29 21:04:05 [INFO]: epoch 478: training loss 0.0312\n",
      "2025-05-29 21:04:05 [INFO]: epoch 479: training loss 0.0283\n",
      "2025-05-29 21:04:05 [INFO]: epoch 480: training loss 0.0239\n",
      "2025-05-29 21:04:05 [INFO]: epoch 481: training loss 0.0299\n",
      "2025-05-29 21:04:05 [INFO]: epoch 482: training loss 0.0285\n",
      "2025-05-29 21:04:05 [INFO]: epoch 483: training loss 0.0308\n",
      "2025-05-29 21:04:05 [INFO]: epoch 484: training loss 0.0282\n",
      "2025-05-29 21:04:05 [INFO]: epoch 485: training loss 0.0277\n",
      "2025-05-29 21:04:05 [INFO]: epoch 486: training loss 0.0338\n",
      "2025-05-29 21:04:05 [INFO]: epoch 487: training loss 0.0371\n",
      "2025-05-29 21:04:05 [INFO]: epoch 488: training loss 0.0308\n",
      "2025-05-29 21:04:05 [INFO]: epoch 489: training loss 0.0331\n",
      "2025-05-29 21:04:05 [INFO]: epoch 490: training loss 0.0381\n",
      "2025-05-29 21:04:05 [INFO]: epoch 491: training loss 0.0269\n",
      "2025-05-29 21:04:05 [INFO]: epoch 492: training loss 0.0355\n",
      "2025-05-29 21:04:06 [INFO]: epoch 493: training loss 0.0344\n",
      "2025-05-29 21:04:06 [INFO]: epoch 494: training loss 0.0268\n",
      "2025-05-29 21:04:06 [INFO]: epoch 495: training loss 0.0375\n",
      "2025-05-29 21:04:06 [INFO]: epoch 496: training loss 0.0314\n",
      "2025-05-29 21:04:06 [INFO]: epoch 497: training loss 0.0275\n",
      "2025-05-29 21:04:06 [INFO]: epoch 498: training loss 0.0247\n",
      "2025-05-29 21:04:06 [INFO]: epoch 499: training loss 0.0313\n",
      "2025-05-29 21:04:06 [INFO]: epoch 500: training loss 0.0253\n",
      "2025-05-29 21:04:06 [INFO]: epoch 501: training loss 0.0299\n",
      "2025-05-29 21:04:06 [INFO]: epoch 502: training loss 0.0269\n",
      "2025-05-29 21:04:06 [INFO]: epoch 503: training loss 0.0289\n",
      "2025-05-29 21:04:06 [INFO]: epoch 504: training loss 0.0266\n",
      "2025-05-29 21:04:06 [INFO]: epoch 505: training loss 0.0308\n",
      "2025-05-29 21:04:06 [INFO]: epoch 506: training loss 0.0265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:06 [INFO]: epoch 507: training loss 0.0379\n",
      "2025-05-29 21:04:06 [INFO]: epoch 508: training loss 0.0293\n",
      "2025-05-29 21:04:06 [INFO]: epoch 509: training loss 0.0332\n",
      "2025-05-29 21:04:06 [INFO]: epoch 510: training loss 0.0547\n",
      "2025-05-29 21:04:06 [INFO]: epoch 511: training loss 0.0307\n",
      "2025-05-29 21:04:06 [INFO]: epoch 512: training loss 0.0368\n",
      "2025-05-29 21:04:06 [INFO]: epoch 513: training loss 0.0375\n",
      "2025-05-29 21:04:06 [INFO]: epoch 514: training loss 0.0282\n",
      "2025-05-29 21:04:06 [INFO]: epoch 515: training loss 0.0290\n",
      "2025-05-29 21:04:06 [INFO]: epoch 516: training loss 0.0352\n",
      "2025-05-29 21:04:06 [INFO]: epoch 517: training loss 0.0278\n",
      "2025-05-29 21:04:06 [INFO]: epoch 518: training loss 0.0265\n",
      "2025-05-29 21:04:06 [INFO]: epoch 519: training loss 0.0321\n",
      "2025-05-29 21:04:06 [INFO]: epoch 520: training loss 0.0318\n",
      "2025-05-29 21:04:06 [INFO]: epoch 521: training loss 0.0275\n",
      "2025-05-29 21:04:06 [INFO]: epoch 522: training loss 0.0275\n",
      "2025-05-29 21:04:06 [INFO]: epoch 523: training loss 0.0312\n",
      "2025-05-29 21:04:06 [INFO]: epoch 524: training loss 0.0261\n",
      "2025-05-29 21:04:06 [INFO]: epoch 525: training loss 0.0268\n",
      "2025-05-29 21:04:06 [INFO]: epoch 526: training loss 0.0269\n",
      "2025-05-29 21:04:06 [INFO]: epoch 527: training loss 0.0265\n",
      "2025-05-29 21:04:06 [INFO]: epoch 528: training loss 0.0312\n",
      "2025-05-29 21:04:06 [INFO]: epoch 529: training loss 0.0270\n",
      "2025-05-29 21:04:06 [INFO]: epoch 530: training loss 0.0296\n",
      "2025-05-29 21:04:06 [INFO]: epoch 531: training loss 0.0322\n",
      "2025-05-29 21:04:06 [INFO]: epoch 532: training loss 0.0260\n",
      "2025-05-29 21:04:06 [INFO]: epoch 533: training loss 0.0336\n",
      "2025-05-29 21:04:06 [INFO]: epoch 534: training loss 0.0336\n",
      "2025-05-29 21:04:06 [INFO]: epoch 535: training loss 0.0310\n",
      "2025-05-29 21:04:06 [INFO]: epoch 536: training loss 0.0270\n",
      "2025-05-29 21:04:06 [INFO]: epoch 537: training loss 0.0267\n",
      "2025-05-29 21:04:06 [INFO]: epoch 538: training loss 0.0242\n",
      "2025-05-29 21:04:06 [INFO]: epoch 539: training loss 0.0299\n",
      "2025-05-29 21:04:06 [INFO]: epoch 540: training loss 0.0278\n",
      "2025-05-29 21:04:06 [INFO]: epoch 541: training loss 0.0241\n",
      "2025-05-29 21:04:06 [INFO]: epoch 542: training loss 0.0277\n",
      "2025-05-29 21:04:06 [INFO]: epoch 543: training loss 0.0281\n",
      "2025-05-29 21:04:06 [INFO]: epoch 544: training loss 0.0223\n",
      "2025-05-29 21:04:06 [INFO]: epoch 545: training loss 0.0268\n",
      "2025-05-29 21:04:06 [INFO]: epoch 546: training loss 0.0275\n",
      "2025-05-29 21:04:06 [INFO]: epoch 547: training loss 0.0343\n",
      "2025-05-29 21:04:06 [INFO]: epoch 548: training loss 0.0273\n",
      "2025-05-29 21:04:06 [INFO]: epoch 549: training loss 0.0237\n",
      "2025-05-29 21:04:06 [INFO]: epoch 550: training loss 0.0289\n",
      "2025-05-29 21:04:06 [INFO]: epoch 551: training loss 0.0285\n",
      "2025-05-29 21:04:06 [INFO]: epoch 552: training loss 0.0249\n",
      "2025-05-29 21:04:06 [INFO]: epoch 553: training loss 0.0284\n",
      "2025-05-29 21:04:06 [INFO]: epoch 554: training loss 0.0303\n",
      "2025-05-29 21:04:06 [INFO]: epoch 555: training loss 0.0281\n",
      "2025-05-29 21:04:06 [INFO]: epoch 556: training loss 0.0244\n",
      "2025-05-29 21:04:06 [INFO]: epoch 557: training loss 0.0340\n",
      "2025-05-29 21:04:06 [INFO]: epoch 558: training loss 0.0298\n",
      "2025-05-29 21:04:06 [INFO]: epoch 559: training loss 0.0265\n",
      "2025-05-29 21:04:06 [INFO]: epoch 560: training loss 0.0288\n",
      "2025-05-29 21:04:06 [INFO]: epoch 561: training loss 0.0384\n",
      "2025-05-29 21:04:06 [INFO]: epoch 562: training loss 0.0295\n",
      "2025-05-29 21:04:06 [INFO]: epoch 563: training loss 0.0355\n",
      "2025-05-29 21:04:06 [INFO]: epoch 564: training loss 0.0256\n",
      "2025-05-29 21:04:06 [INFO]: epoch 565: training loss 0.0280\n",
      "2025-05-29 21:04:06 [INFO]: epoch 566: training loss 0.0358\n",
      "2025-05-29 21:04:06 [INFO]: epoch 567: training loss 0.0340\n",
      "2025-05-29 21:04:06 [INFO]: epoch 568: training loss 0.0239\n",
      "2025-05-29 21:04:06 [INFO]: epoch 569: training loss 0.0290\n",
      "2025-05-29 21:04:06 [INFO]: epoch 570: training loss 0.0349\n",
      "2025-05-29 21:04:06 [INFO]: epoch 571: training loss 0.0242\n",
      "2025-05-29 21:04:06 [INFO]: epoch 572: training loss 0.0271\n",
      "2025-05-29 21:04:06 [INFO]: epoch 573: training loss 0.0343\n",
      "2025-05-29 21:04:07 [INFO]: epoch 574: training loss 0.0275\n",
      "2025-05-29 21:04:07 [INFO]: epoch 575: training loss 0.0261\n",
      "2025-05-29 21:04:07 [INFO]: epoch 576: training loss 0.0255\n",
      "2025-05-29 21:04:07 [INFO]: epoch 577: training loss 0.0281\n",
      "2025-05-29 21:04:07 [INFO]: epoch 578: training loss 0.0270\n",
      "2025-05-29 21:04:07 [INFO]: epoch 579: training loss 0.0258\n",
      "2025-05-29 21:04:07 [INFO]: epoch 580: training loss 0.0282\n",
      "2025-05-29 21:04:07 [INFO]: epoch 581: training loss 0.0298\n",
      "2025-05-29 21:04:07 [INFO]: epoch 582: training loss 0.0258\n",
      "2025-05-29 21:04:07 [INFO]: epoch 583: training loss 0.0266\n",
      "2025-05-29 21:04:07 [INFO]: epoch 584: training loss 0.0240\n",
      "2025-05-29 21:04:07 [INFO]: epoch 585: training loss 0.0249\n",
      "2025-05-29 21:04:07 [INFO]: epoch 586: training loss 0.0261\n",
      "2025-05-29 21:04:07 [INFO]: epoch 587: training loss 0.0255\n",
      "2025-05-29 21:04:07 [INFO]: epoch 588: training loss 0.0295\n",
      "2025-05-29 21:04:07 [INFO]: epoch 589: training loss 0.0269\n",
      "2025-05-29 21:04:07 [INFO]: epoch 590: training loss 0.0246\n",
      "2025-05-29 21:04:07 [INFO]: epoch 591: training loss 0.0295\n",
      "2025-05-29 21:04:07 [INFO]: epoch 592: training loss 0.0255\n",
      "2025-05-29 21:04:07 [INFO]: epoch 593: training loss 0.0249\n",
      "2025-05-29 21:04:07 [INFO]: epoch 594: training loss 0.0276\n",
      "2025-05-29 21:04:07 [INFO]: epoch 595: training loss 0.0263\n",
      "2025-05-29 21:04:07 [INFO]: epoch 596: training loss 0.0250\n",
      "2025-05-29 21:04:07 [INFO]: epoch 597: training loss 0.0242\n",
      "2025-05-29 21:04:07 [INFO]: epoch 598: training loss 0.0257\n",
      "2025-05-29 21:04:07 [INFO]: epoch 599: training loss 0.0318\n",
      "2025-05-29 21:04:07 [INFO]: Finished training.\n",
      "2025-05-29 21:04:07 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 20%|████████████████▊                                                                   | 1/5 [00:07<00:30,  7.59s/it]2025-05-29 21:04:07 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:04:07 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:04:07 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:04:07 [INFO]: epoch 0: training loss 1.3619\n",
      "2025-05-29 21:04:07 [INFO]: epoch 1: training loss 0.8279\n",
      "2025-05-29 21:04:07 [INFO]: epoch 2: training loss 0.6108\n",
      "2025-05-29 21:04:07 [INFO]: epoch 3: training loss 0.6818\n",
      "2025-05-29 21:04:07 [INFO]: epoch 4: training loss 0.6532\n",
      "2025-05-29 21:04:07 [INFO]: epoch 5: training loss 0.6249\n",
      "2025-05-29 21:04:07 [INFO]: epoch 6: training loss 0.5837\n",
      "2025-05-29 21:04:07 [INFO]: epoch 7: training loss 0.5452\n",
      "2025-05-29 21:04:07 [INFO]: epoch 8: training loss 0.5641\n",
      "2025-05-29 21:04:07 [INFO]: epoch 9: training loss 0.5168\n",
      "2025-05-29 21:04:07 [INFO]: epoch 10: training loss 0.5121\n",
      "2025-05-29 21:04:07 [INFO]: epoch 11: training loss 0.5347\n",
      "2025-05-29 21:04:07 [INFO]: epoch 12: training loss 0.5509\n",
      "2025-05-29 21:04:07 [INFO]: epoch 13: training loss 0.5252\n",
      "2025-05-29 21:04:07 [INFO]: epoch 14: training loss 0.5027\n",
      "2025-05-29 21:04:07 [INFO]: epoch 15: training loss 0.5145\n",
      "2025-05-29 21:04:07 [INFO]: epoch 16: training loss 0.4799\n",
      "2025-05-29 21:04:07 [INFO]: epoch 17: training loss 0.4706\n",
      "2025-05-29 21:04:07 [INFO]: epoch 18: training loss 0.4350\n",
      "2025-05-29 21:04:07 [INFO]: epoch 19: training loss 0.4519\n",
      "2025-05-29 21:04:07 [INFO]: epoch 20: training loss 0.4837\n",
      "2025-05-29 21:04:07 [INFO]: epoch 21: training loss 0.4825\n",
      "2025-05-29 21:04:07 [INFO]: epoch 22: training loss 0.4486\n",
      "2025-05-29 21:04:07 [INFO]: epoch 23: training loss 0.4719\n",
      "2025-05-29 21:04:07 [INFO]: epoch 24: training loss 0.4610\n",
      "2025-05-29 21:04:07 [INFO]: epoch 25: training loss 0.4491\n",
      "2025-05-29 21:04:07 [INFO]: epoch 26: training loss 0.4386\n",
      "2025-05-29 21:04:07 [INFO]: epoch 27: training loss 0.4222\n",
      "2025-05-29 21:04:07 [INFO]: epoch 28: training loss 0.4335\n",
      "2025-05-29 21:04:07 [INFO]: epoch 29: training loss 0.4266\n",
      "2025-05-29 21:04:07 [INFO]: epoch 30: training loss 0.4373\n",
      "2025-05-29 21:04:07 [INFO]: epoch 31: training loss 0.4373\n",
      "2025-05-29 21:04:07 [INFO]: epoch 32: training loss 0.4344\n",
      "2025-05-29 21:04:07 [INFO]: epoch 33: training loss 0.4383\n",
      "2025-05-29 21:04:07 [INFO]: epoch 34: training loss 0.4052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:07 [INFO]: epoch 35: training loss 0.4234\n",
      "2025-05-29 21:04:07 [INFO]: epoch 36: training loss 0.4040\n",
      "2025-05-29 21:04:07 [INFO]: epoch 37: training loss 0.4173\n",
      "2025-05-29 21:04:07 [INFO]: epoch 38: training loss 0.4130\n",
      "2025-05-29 21:04:07 [INFO]: epoch 39: training loss 0.4236\n",
      "2025-05-29 21:04:07 [INFO]: epoch 40: training loss 0.3951\n",
      "2025-05-29 21:04:07 [INFO]: epoch 41: training loss 0.3871\n",
      "2025-05-29 21:04:07 [INFO]: epoch 42: training loss 0.3930\n",
      "2025-05-29 21:04:07 [INFO]: epoch 43: training loss 0.3900\n",
      "2025-05-29 21:04:07 [INFO]: epoch 44: training loss 0.3951\n",
      "2025-05-29 21:04:07 [INFO]: epoch 45: training loss 0.3847\n",
      "2025-05-29 21:04:07 [INFO]: epoch 46: training loss 0.4117\n",
      "2025-05-29 21:04:08 [INFO]: epoch 47: training loss 0.4053\n",
      "2025-05-29 21:04:08 [INFO]: epoch 48: training loss 0.3818\n",
      "2025-05-29 21:04:08 [INFO]: epoch 49: training loss 0.3848\n",
      "2025-05-29 21:04:08 [INFO]: epoch 50: training loss 0.3585\n",
      "2025-05-29 21:04:08 [INFO]: epoch 51: training loss 0.3909\n",
      "2025-05-29 21:04:08 [INFO]: epoch 52: training loss 0.3813\n",
      "2025-05-29 21:04:08 [INFO]: epoch 53: training loss 0.3731\n",
      "2025-05-29 21:04:08 [INFO]: epoch 54: training loss 0.4023\n",
      "2025-05-29 21:04:08 [INFO]: epoch 55: training loss 0.3849\n",
      "2025-05-29 21:04:08 [INFO]: epoch 56: training loss 0.3819\n",
      "2025-05-29 21:04:08 [INFO]: epoch 57: training loss 0.3667\n",
      "2025-05-29 21:04:08 [INFO]: epoch 58: training loss 0.3501\n",
      "2025-05-29 21:04:08 [INFO]: epoch 59: training loss 0.3808\n",
      "2025-05-29 21:04:08 [INFO]: epoch 60: training loss 0.3587\n",
      "2025-05-29 21:04:08 [INFO]: epoch 61: training loss 0.3676\n",
      "2025-05-29 21:04:08 [INFO]: epoch 62: training loss 0.3565\n",
      "2025-05-29 21:04:08 [INFO]: epoch 63: training loss 0.3416\n",
      "2025-05-29 21:04:08 [INFO]: epoch 64: training loss 0.3531\n",
      "2025-05-29 21:04:08 [INFO]: epoch 65: training loss 0.3500\n",
      "2025-05-29 21:04:08 [INFO]: epoch 66: training loss 0.3320\n",
      "2025-05-29 21:04:08 [INFO]: epoch 67: training loss 0.3537\n",
      "2025-05-29 21:04:08 [INFO]: epoch 68: training loss 0.3478\n",
      "2025-05-29 21:04:08 [INFO]: epoch 69: training loss 0.3599\n",
      "2025-05-29 21:04:08 [INFO]: epoch 70: training loss 0.3514\n",
      "2025-05-29 21:04:08 [INFO]: epoch 71: training loss 0.3689\n",
      "2025-05-29 21:04:08 [INFO]: epoch 72: training loss 0.3347\n",
      "2025-05-29 21:04:08 [INFO]: epoch 73: training loss 0.3406\n",
      "2025-05-29 21:04:08 [INFO]: epoch 74: training loss 0.3636\n",
      "2025-05-29 21:04:08 [INFO]: epoch 75: training loss 0.3521\n",
      "2025-05-29 21:04:08 [INFO]: epoch 76: training loss 0.3369\n",
      "2025-05-29 21:04:08 [INFO]: epoch 77: training loss 0.3394\n",
      "2025-05-29 21:04:08 [INFO]: epoch 78: training loss 0.3324\n",
      "2025-05-29 21:04:08 [INFO]: epoch 79: training loss 0.3299\n",
      "2025-05-29 21:04:08 [INFO]: epoch 80: training loss 0.3463\n",
      "2025-05-29 21:04:08 [INFO]: epoch 81: training loss 0.3378\n",
      "2025-05-29 21:04:08 [INFO]: epoch 82: training loss 0.3458\n",
      "2025-05-29 21:04:08 [INFO]: epoch 83: training loss 0.3441\n",
      "2025-05-29 21:04:08 [INFO]: epoch 84: training loss 0.3508\n",
      "2025-05-29 21:04:08 [INFO]: epoch 85: training loss 0.3424\n",
      "2025-05-29 21:04:08 [INFO]: epoch 86: training loss 0.3401\n",
      "2025-05-29 21:04:08 [INFO]: epoch 87: training loss 0.3318\n",
      "2025-05-29 21:04:08 [INFO]: epoch 88: training loss 0.3365\n",
      "2025-05-29 21:04:08 [INFO]: epoch 89: training loss 0.3278\n",
      "2025-05-29 21:04:08 [INFO]: epoch 90: training loss 0.3214\n",
      "2025-05-29 21:04:08 [INFO]: epoch 91: training loss 0.3129\n",
      "2025-05-29 21:04:08 [INFO]: epoch 92: training loss 0.3355\n",
      "2025-05-29 21:04:08 [INFO]: epoch 93: training loss 0.3424\n",
      "2025-05-29 21:04:08 [INFO]: epoch 94: training loss 0.3270\n",
      "2025-05-29 21:04:08 [INFO]: epoch 95: training loss 0.3330\n",
      "2025-05-29 21:04:08 [INFO]: epoch 96: training loss 0.3137\n",
      "2025-05-29 21:04:08 [INFO]: epoch 97: training loss 0.3177\n",
      "2025-05-29 21:04:08 [INFO]: epoch 98: training loss 0.3327\n",
      "2025-05-29 21:04:08 [INFO]: epoch 99: training loss 0.3287\n",
      "2025-05-29 21:04:08 [INFO]: epoch 100: training loss 0.3359\n",
      "2025-05-29 21:04:08 [INFO]: epoch 101: training loss 0.3244\n",
      "2025-05-29 21:04:08 [INFO]: epoch 102: training loss 0.3272\n",
      "2025-05-29 21:04:08 [INFO]: epoch 103: training loss 0.3310\n",
      "2025-05-29 21:04:08 [INFO]: epoch 104: training loss 0.3651\n",
      "2025-05-29 21:04:08 [INFO]: epoch 105: training loss 0.3511\n",
      "2025-05-29 21:04:08 [INFO]: epoch 106: training loss 0.3360\n",
      "2025-05-29 21:04:08 [INFO]: epoch 107: training loss 0.3189\n",
      "2025-05-29 21:04:08 [INFO]: epoch 108: training loss 0.3346\n",
      "2025-05-29 21:04:08 [INFO]: epoch 109: training loss 0.3112\n",
      "2025-05-29 21:04:08 [INFO]: epoch 110: training loss 0.3234\n",
      "2025-05-29 21:04:08 [INFO]: epoch 111: training loss 0.3102\n",
      "2025-05-29 21:04:08 [INFO]: epoch 112: training loss 0.3277\n",
      "2025-05-29 21:04:08 [INFO]: epoch 113: training loss 0.3034\n",
      "2025-05-29 21:04:08 [INFO]: epoch 114: training loss 0.3299\n",
      "2025-05-29 21:04:08 [INFO]: epoch 115: training loss 0.3253\n",
      "2025-05-29 21:04:08 [INFO]: epoch 116: training loss 0.3092\n",
      "2025-05-29 21:04:08 [INFO]: epoch 117: training loss 0.3136\n",
      "2025-05-29 21:04:08 [INFO]: epoch 118: training loss 0.3031\n",
      "2025-05-29 21:04:08 [INFO]: epoch 119: training loss 0.3236\n",
      "2025-05-29 21:04:08 [INFO]: epoch 120: training loss 0.2808\n",
      "2025-05-29 21:04:08 [INFO]: epoch 121: training loss 0.2971\n",
      "2025-05-29 21:04:08 [INFO]: epoch 122: training loss 0.2993\n",
      "2025-05-29 21:04:08 [INFO]: epoch 123: training loss 0.2970\n",
      "2025-05-29 21:04:08 [INFO]: epoch 124: training loss 0.2973\n",
      "2025-05-29 21:04:08 [INFO]: epoch 125: training loss 0.2969\n",
      "2025-05-29 21:04:08 [INFO]: epoch 126: training loss 0.2811\n",
      "2025-05-29 21:04:09 [INFO]: epoch 127: training loss 0.2935\n",
      "2025-05-29 21:04:09 [INFO]: epoch 128: training loss 0.3073\n",
      "2025-05-29 21:04:09 [INFO]: epoch 129: training loss 0.2862\n",
      "2025-05-29 21:04:09 [INFO]: epoch 130: training loss 0.3035\n",
      "2025-05-29 21:04:09 [INFO]: epoch 131: training loss 0.2796\n",
      "2025-05-29 21:04:09 [INFO]: epoch 132: training loss 0.2887\n",
      "2025-05-29 21:04:09 [INFO]: epoch 133: training loss 0.2801\n",
      "2025-05-29 21:04:09 [INFO]: epoch 134: training loss 0.2922\n",
      "2025-05-29 21:04:09 [INFO]: epoch 135: training loss 0.2912\n",
      "2025-05-29 21:04:09 [INFO]: epoch 136: training loss 0.3085\n",
      "2025-05-29 21:04:09 [INFO]: epoch 137: training loss 0.2838\n",
      "2025-05-29 21:04:09 [INFO]: epoch 138: training loss 0.2903\n",
      "2025-05-29 21:04:09 [INFO]: epoch 139: training loss 0.2897\n",
      "2025-05-29 21:04:09 [INFO]: epoch 140: training loss 0.2855\n",
      "2025-05-29 21:04:09 [INFO]: epoch 141: training loss 0.2778\n",
      "2025-05-29 21:04:09 [INFO]: epoch 142: training loss 0.2794\n",
      "2025-05-29 21:04:09 [INFO]: epoch 143: training loss 0.2807\n",
      "2025-05-29 21:04:09 [INFO]: epoch 144: training loss 0.2866\n",
      "2025-05-29 21:04:09 [INFO]: epoch 145: training loss 0.3073\n",
      "2025-05-29 21:04:09 [INFO]: epoch 146: training loss 0.2861\n",
      "2025-05-29 21:04:09 [INFO]: epoch 147: training loss 0.2810\n",
      "2025-05-29 21:04:09 [INFO]: epoch 148: training loss 0.2740\n",
      "2025-05-29 21:04:09 [INFO]: epoch 149: training loss 0.2929\n",
      "2025-05-29 21:04:09 [INFO]: epoch 150: training loss 0.2790\n",
      "2025-05-29 21:04:09 [INFO]: epoch 151: training loss 0.2811\n",
      "2025-05-29 21:04:09 [INFO]: epoch 152: training loss 0.2721\n",
      "2025-05-29 21:04:09 [INFO]: epoch 153: training loss 0.2837\n",
      "2025-05-29 21:04:09 [INFO]: epoch 154: training loss 0.2809\n",
      "2025-05-29 21:04:09 [INFO]: epoch 155: training loss 0.2596\n",
      "2025-05-29 21:04:09 [INFO]: epoch 156: training loss 0.2758\n",
      "2025-05-29 21:04:09 [INFO]: epoch 157: training loss 0.2789\n",
      "2025-05-29 21:04:09 [INFO]: epoch 158: training loss 0.2712\n",
      "2025-05-29 21:04:09 [INFO]: epoch 159: training loss 0.2538\n",
      "2025-05-29 21:04:09 [INFO]: epoch 160: training loss 0.2609\n",
      "2025-05-29 21:04:09 [INFO]: epoch 161: training loss 0.2679\n",
      "2025-05-29 21:04:09 [INFO]: epoch 162: training loss 0.2872\n",
      "2025-05-29 21:04:09 [INFO]: epoch 163: training loss 0.2767\n",
      "2025-05-29 21:04:09 [INFO]: epoch 164: training loss 0.2785\n",
      "2025-05-29 21:04:09 [INFO]: epoch 165: training loss 0.2989\n",
      "2025-05-29 21:04:09 [INFO]: epoch 166: training loss 0.2814\n",
      "2025-05-29 21:04:09 [INFO]: epoch 167: training loss 0.2582\n",
      "2025-05-29 21:04:09 [INFO]: epoch 168: training loss 0.2625\n",
      "2025-05-29 21:04:09 [INFO]: epoch 169: training loss 0.2654\n",
      "2025-05-29 21:04:09 [INFO]: epoch 170: training loss 0.2718\n",
      "2025-05-29 21:04:09 [INFO]: epoch 171: training loss 0.2683\n",
      "2025-05-29 21:04:09 [INFO]: epoch 172: training loss 0.2760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:09 [INFO]: epoch 173: training loss 0.2680\n",
      "2025-05-29 21:04:09 [INFO]: epoch 174: training loss 0.2712\n",
      "2025-05-29 21:04:09 [INFO]: epoch 175: training loss 0.2741\n",
      "2025-05-29 21:04:09 [INFO]: epoch 176: training loss 0.2631\n",
      "2025-05-29 21:04:09 [INFO]: epoch 177: training loss 0.2660\n",
      "2025-05-29 21:04:09 [INFO]: epoch 178: training loss 0.2900\n",
      "2025-05-29 21:04:09 [INFO]: epoch 179: training loss 0.2698\n",
      "2025-05-29 21:04:09 [INFO]: epoch 180: training loss 0.2510\n",
      "2025-05-29 21:04:09 [INFO]: epoch 181: training loss 0.2821\n",
      "2025-05-29 21:04:09 [INFO]: epoch 182: training loss 0.2561\n",
      "2025-05-29 21:04:09 [INFO]: epoch 183: training loss 0.2688\n",
      "2025-05-29 21:04:09 [INFO]: epoch 184: training loss 0.2635\n",
      "2025-05-29 21:04:09 [INFO]: epoch 185: training loss 0.2664\n",
      "2025-05-29 21:04:09 [INFO]: epoch 186: training loss 0.2691\n",
      "2025-05-29 21:04:09 [INFO]: epoch 187: training loss 0.2772\n",
      "2025-05-29 21:04:09 [INFO]: epoch 188: training loss 0.2564\n",
      "2025-05-29 21:04:09 [INFO]: epoch 189: training loss 0.2442\n",
      "2025-05-29 21:04:09 [INFO]: epoch 190: training loss 0.2511\n",
      "2025-05-29 21:04:09 [INFO]: epoch 191: training loss 0.2660\n",
      "2025-05-29 21:04:09 [INFO]: epoch 192: training loss 0.2576\n",
      "2025-05-29 21:04:09 [INFO]: epoch 193: training loss 0.2576\n",
      "2025-05-29 21:04:09 [INFO]: epoch 194: training loss 0.2510\n",
      "2025-05-29 21:04:09 [INFO]: epoch 195: training loss 0.2762\n",
      "2025-05-29 21:04:09 [INFO]: epoch 196: training loss 0.2798\n",
      "2025-05-29 21:04:09 [INFO]: epoch 197: training loss 0.2817\n",
      "2025-05-29 21:04:09 [INFO]: epoch 198: training loss 0.2417\n",
      "2025-05-29 21:04:09 [INFO]: epoch 199: training loss 0.2597\n",
      "2025-05-29 21:04:09 [INFO]: epoch 200: training loss 0.2696\n",
      "2025-05-29 21:04:09 [INFO]: epoch 201: training loss 0.2481\n",
      "2025-05-29 21:04:09 [INFO]: epoch 202: training loss 0.2380\n",
      "2025-05-29 21:04:09 [INFO]: epoch 203: training loss 0.2661\n",
      "2025-05-29 21:04:09 [INFO]: epoch 204: training loss 0.2791\n",
      "2025-05-29 21:04:09 [INFO]: epoch 205: training loss 0.2568\n",
      "2025-05-29 21:04:10 [INFO]: epoch 206: training loss 0.2464\n",
      "2025-05-29 21:04:10 [INFO]: epoch 207: training loss 0.2600\n",
      "2025-05-29 21:04:10 [INFO]: epoch 208: training loss 0.2512\n",
      "2025-05-29 21:04:10 [INFO]: epoch 209: training loss 0.2728\n",
      "2025-05-29 21:04:10 [INFO]: epoch 210: training loss 0.2816\n",
      "2025-05-29 21:04:10 [INFO]: epoch 211: training loss 0.2649\n",
      "2025-05-29 21:04:10 [INFO]: epoch 212: training loss 0.2505\n",
      "2025-05-29 21:04:10 [INFO]: epoch 213: training loss 0.2450\n",
      "2025-05-29 21:04:10 [INFO]: epoch 214: training loss 0.2459\n",
      "2025-05-29 21:04:10 [INFO]: epoch 215: training loss 0.2486\n",
      "2025-05-29 21:04:10 [INFO]: epoch 216: training loss 0.2501\n",
      "2025-05-29 21:04:10 [INFO]: epoch 217: training loss 0.2358\n",
      "2025-05-29 21:04:10 [INFO]: epoch 218: training loss 0.2384\n",
      "2025-05-29 21:04:10 [INFO]: epoch 219: training loss 0.2423\n",
      "2025-05-29 21:04:10 [INFO]: epoch 220: training loss 0.2352\n",
      "2025-05-29 21:04:10 [INFO]: epoch 221: training loss 0.2354\n",
      "2025-05-29 21:04:10 [INFO]: epoch 222: training loss 0.2406\n",
      "2025-05-29 21:04:10 [INFO]: epoch 223: training loss 0.2322\n",
      "2025-05-29 21:04:10 [INFO]: epoch 224: training loss 0.2316\n",
      "2025-05-29 21:04:10 [INFO]: epoch 225: training loss 0.2354\n",
      "2025-05-29 21:04:10 [INFO]: epoch 226: training loss 0.2580\n",
      "2025-05-29 21:04:10 [INFO]: epoch 227: training loss 0.2468\n",
      "2025-05-29 21:04:10 [INFO]: epoch 228: training loss 0.2399\n",
      "2025-05-29 21:04:10 [INFO]: epoch 229: training loss 0.2226\n",
      "2025-05-29 21:04:10 [INFO]: epoch 230: training loss 0.2516\n",
      "2025-05-29 21:04:10 [INFO]: epoch 231: training loss 0.2523\n",
      "2025-05-29 21:04:10 [INFO]: epoch 232: training loss 0.2511\n",
      "2025-05-29 21:04:10 [INFO]: epoch 233: training loss 0.2504\n",
      "2025-05-29 21:04:10 [INFO]: epoch 234: training loss 0.2367\n",
      "2025-05-29 21:04:10 [INFO]: epoch 235: training loss 0.2365\n",
      "2025-05-29 21:04:10 [INFO]: epoch 236: training loss 0.2369\n",
      "2025-05-29 21:04:10 [INFO]: epoch 237: training loss 0.2387\n",
      "2025-05-29 21:04:10 [INFO]: epoch 238: training loss 0.2437\n",
      "2025-05-29 21:04:10 [INFO]: epoch 239: training loss 0.2275\n",
      "2025-05-29 21:04:10 [INFO]: epoch 240: training loss 0.2255\n",
      "2025-05-29 21:04:10 [INFO]: epoch 241: training loss 0.2244\n",
      "2025-05-29 21:04:10 [INFO]: epoch 242: training loss 0.2449\n",
      "2025-05-29 21:04:10 [INFO]: epoch 243: training loss 0.2428\n",
      "2025-05-29 21:04:10 [INFO]: epoch 244: training loss 0.2240\n",
      "2025-05-29 21:04:10 [INFO]: epoch 245: training loss 0.2391\n",
      "2025-05-29 21:04:10 [INFO]: epoch 246: training loss 0.2322\n",
      "2025-05-29 21:04:10 [INFO]: epoch 247: training loss 0.2313\n",
      "2025-05-29 21:04:10 [INFO]: epoch 248: training loss 0.2372\n",
      "2025-05-29 21:04:10 [INFO]: epoch 249: training loss 0.2238\n",
      "2025-05-29 21:04:10 [INFO]: epoch 250: training loss 0.2164\n",
      "2025-05-29 21:04:10 [INFO]: epoch 251: training loss 0.2383\n",
      "2025-05-29 21:04:10 [INFO]: epoch 252: training loss 0.2317\n",
      "2025-05-29 21:04:10 [INFO]: epoch 253: training loss 0.2208\n",
      "2025-05-29 21:04:10 [INFO]: epoch 254: training loss 0.2419\n",
      "2025-05-29 21:04:10 [INFO]: epoch 255: training loss 0.2463\n",
      "2025-05-29 21:04:10 [INFO]: epoch 256: training loss 0.2283\n",
      "2025-05-29 21:04:10 [INFO]: epoch 257: training loss 0.2184\n",
      "2025-05-29 21:04:10 [INFO]: epoch 258: training loss 0.2131\n",
      "2025-05-29 21:04:10 [INFO]: epoch 259: training loss 0.2247\n",
      "2025-05-29 21:04:10 [INFO]: epoch 260: training loss 0.2070\n",
      "2025-05-29 21:04:10 [INFO]: epoch 261: training loss 0.2205\n",
      "2025-05-29 21:04:10 [INFO]: epoch 262: training loss 0.2179\n",
      "2025-05-29 21:04:10 [INFO]: epoch 263: training loss 0.2335\n",
      "2025-05-29 21:04:10 [INFO]: epoch 264: training loss 0.2080\n",
      "2025-05-29 21:04:10 [INFO]: epoch 265: training loss 0.2182\n",
      "2025-05-29 21:04:10 [INFO]: epoch 266: training loss 0.2042\n",
      "2025-05-29 21:04:10 [INFO]: epoch 267: training loss 0.2195\n",
      "2025-05-29 21:04:10 [INFO]: epoch 268: training loss 0.2213\n",
      "2025-05-29 21:04:10 [INFO]: epoch 269: training loss 0.2192\n",
      "2025-05-29 21:04:10 [INFO]: epoch 270: training loss 0.2111\n",
      "2025-05-29 21:04:10 [INFO]: epoch 271: training loss 0.2217\n",
      "2025-05-29 21:04:10 [INFO]: epoch 272: training loss 0.2184\n",
      "2025-05-29 21:04:10 [INFO]: epoch 273: training loss 0.2090\n",
      "2025-05-29 21:04:10 [INFO]: epoch 274: training loss 0.2035\n",
      "2025-05-29 21:04:10 [INFO]: epoch 275: training loss 0.2131\n",
      "2025-05-29 21:04:10 [INFO]: epoch 276: training loss 0.2089\n",
      "2025-05-29 21:04:10 [INFO]: epoch 277: training loss 0.2158\n",
      "2025-05-29 21:04:10 [INFO]: epoch 278: training loss 0.2116\n",
      "2025-05-29 21:04:10 [INFO]: epoch 279: training loss 0.2174\n",
      "2025-05-29 21:04:10 [INFO]: epoch 280: training loss 0.2242\n",
      "2025-05-29 21:04:10 [INFO]: epoch 281: training loss 0.2122\n",
      "2025-05-29 21:04:10 [INFO]: epoch 282: training loss 0.2045\n",
      "2025-05-29 21:04:10 [INFO]: epoch 283: training loss 0.2044\n",
      "2025-05-29 21:04:10 [INFO]: epoch 284: training loss 0.2111\n",
      "2025-05-29 21:04:10 [INFO]: epoch 285: training loss 0.2099\n",
      "2025-05-29 21:04:10 [INFO]: epoch 286: training loss 0.1961\n",
      "2025-05-29 21:04:10 [INFO]: epoch 287: training loss 0.2149\n",
      "2025-05-29 21:04:11 [INFO]: epoch 288: training loss 0.2169\n",
      "2025-05-29 21:04:11 [INFO]: epoch 289: training loss 0.2083\n",
      "2025-05-29 21:04:11 [INFO]: epoch 290: training loss 0.2134\n",
      "2025-05-29 21:04:11 [INFO]: epoch 291: training loss 0.2142\n",
      "2025-05-29 21:04:11 [INFO]: epoch 292: training loss 0.2213\n",
      "2025-05-29 21:04:11 [INFO]: epoch 293: training loss 0.2115\n",
      "2025-05-29 21:04:11 [INFO]: epoch 294: training loss 0.1978\n",
      "2025-05-29 21:04:11 [INFO]: epoch 295: training loss 0.1985\n",
      "2025-05-29 21:04:11 [INFO]: epoch 296: training loss 0.2101\n",
      "2025-05-29 21:04:11 [INFO]: epoch 297: training loss 0.2099\n",
      "2025-05-29 21:04:11 [INFO]: epoch 298: training loss 0.2093\n",
      "2025-05-29 21:04:11 [INFO]: epoch 299: training loss 0.2038\n",
      "2025-05-29 21:04:11 [INFO]: epoch 300: training loss 0.2010\n",
      "2025-05-29 21:04:11 [INFO]: epoch 301: training loss 0.1991\n",
      "2025-05-29 21:04:11 [INFO]: epoch 302: training loss 0.2070\n",
      "2025-05-29 21:04:11 [INFO]: epoch 303: training loss 0.2085\n",
      "2025-05-29 21:04:11 [INFO]: epoch 304: training loss 0.2053\n",
      "2025-05-29 21:04:11 [INFO]: epoch 305: training loss 0.1854\n",
      "2025-05-29 21:04:11 [INFO]: epoch 306: training loss 0.2015\n",
      "2025-05-29 21:04:11 [INFO]: epoch 307: training loss 0.1920\n",
      "2025-05-29 21:04:11 [INFO]: epoch 308: training loss 0.1949\n",
      "2025-05-29 21:04:11 [INFO]: epoch 309: training loss 0.1919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:11 [INFO]: epoch 310: training loss 0.2061\n",
      "2025-05-29 21:04:11 [INFO]: epoch 311: training loss 0.2007\n",
      "2025-05-29 21:04:11 [INFO]: epoch 312: training loss 0.1961\n",
      "2025-05-29 21:04:11 [INFO]: epoch 313: training loss 0.1961\n",
      "2025-05-29 21:04:11 [INFO]: epoch 314: training loss 0.1915\n",
      "2025-05-29 21:04:11 [INFO]: epoch 315: training loss 0.2021\n",
      "2025-05-29 21:04:11 [INFO]: epoch 316: training loss 0.1929\n",
      "2025-05-29 21:04:11 [INFO]: epoch 317: training loss 0.1967\n",
      "2025-05-29 21:04:11 [INFO]: epoch 318: training loss 0.2091\n",
      "2025-05-29 21:04:11 [INFO]: epoch 319: training loss 0.1838\n",
      "2025-05-29 21:04:11 [INFO]: epoch 320: training loss 0.1916\n",
      "2025-05-29 21:04:11 [INFO]: epoch 321: training loss 0.1922\n",
      "2025-05-29 21:04:11 [INFO]: epoch 322: training loss 0.1974\n",
      "2025-05-29 21:04:11 [INFO]: epoch 323: training loss 0.1900\n",
      "2025-05-29 21:04:11 [INFO]: epoch 324: training loss 0.1827\n",
      "2025-05-29 21:04:11 [INFO]: epoch 325: training loss 0.1798\n",
      "2025-05-29 21:04:11 [INFO]: epoch 326: training loss 0.1753\n",
      "2025-05-29 21:04:11 [INFO]: epoch 327: training loss 0.1852\n",
      "2025-05-29 21:04:11 [INFO]: epoch 328: training loss 0.1794\n",
      "2025-05-29 21:04:11 [INFO]: epoch 329: training loss 0.1910\n",
      "2025-05-29 21:04:11 [INFO]: epoch 330: training loss 0.1939\n",
      "2025-05-29 21:04:11 [INFO]: epoch 331: training loss 0.1813\n",
      "2025-05-29 21:04:11 [INFO]: epoch 332: training loss 0.1797\n",
      "2025-05-29 21:04:11 [INFO]: epoch 333: training loss 0.1842\n",
      "2025-05-29 21:04:11 [INFO]: epoch 334: training loss 0.1735\n",
      "2025-05-29 21:04:11 [INFO]: epoch 335: training loss 0.1859\n",
      "2025-05-29 21:04:11 [INFO]: epoch 336: training loss 0.1883\n",
      "2025-05-29 21:04:11 [INFO]: epoch 337: training loss 0.1775\n",
      "2025-05-29 21:04:11 [INFO]: epoch 338: training loss 0.1756\n",
      "2025-05-29 21:04:11 [INFO]: epoch 339: training loss 0.1790\n",
      "2025-05-29 21:04:11 [INFO]: epoch 340: training loss 0.1818\n",
      "2025-05-29 21:04:11 [INFO]: epoch 341: training loss 0.1786\n",
      "2025-05-29 21:04:11 [INFO]: epoch 342: training loss 0.1717\n",
      "2025-05-29 21:04:11 [INFO]: epoch 343: training loss 0.1815\n",
      "2025-05-29 21:04:11 [INFO]: epoch 344: training loss 0.1766\n",
      "2025-05-29 21:04:11 [INFO]: epoch 345: training loss 0.1822\n",
      "2025-05-29 21:04:11 [INFO]: epoch 346: training loss 0.1645\n",
      "2025-05-29 21:04:11 [INFO]: epoch 347: training loss 0.1668\n",
      "2025-05-29 21:04:11 [INFO]: epoch 348: training loss 0.1711\n",
      "2025-05-29 21:04:11 [INFO]: epoch 349: training loss 0.1799\n",
      "2025-05-29 21:04:11 [INFO]: epoch 350: training loss 0.1801\n",
      "2025-05-29 21:04:11 [INFO]: epoch 351: training loss 0.1686\n",
      "2025-05-29 21:04:11 [INFO]: epoch 352: training loss 0.1785\n",
      "2025-05-29 21:04:11 [INFO]: epoch 353: training loss 0.1723\n",
      "2025-05-29 21:04:11 [INFO]: epoch 354: training loss 0.1761\n",
      "2025-05-29 21:04:11 [INFO]: epoch 355: training loss 0.1735\n",
      "2025-05-29 21:04:11 [INFO]: epoch 356: training loss 0.1582\n",
      "2025-05-29 21:04:11 [INFO]: epoch 357: training loss 0.1626\n",
      "2025-05-29 21:04:11 [INFO]: epoch 358: training loss 0.1720\n",
      "2025-05-29 21:04:11 [INFO]: epoch 359: training loss 0.1778\n",
      "2025-05-29 21:04:11 [INFO]: epoch 360: training loss 0.1784\n",
      "2025-05-29 21:04:11 [INFO]: epoch 361: training loss 0.1765\n",
      "2025-05-29 21:04:11 [INFO]: epoch 362: training loss 0.1676\n",
      "2025-05-29 21:04:11 [INFO]: epoch 363: training loss 0.1915\n",
      "2025-05-29 21:04:11 [INFO]: epoch 364: training loss 0.1789\n",
      "2025-05-29 21:04:11 [INFO]: epoch 365: training loss 0.1741\n",
      "2025-05-29 21:04:11 [INFO]: epoch 366: training loss 0.1671\n",
      "2025-05-29 21:04:11 [INFO]: epoch 367: training loss 0.1560\n",
      "2025-05-29 21:04:11 [INFO]: epoch 368: training loss 0.1849\n",
      "2025-05-29 21:04:12 [INFO]: epoch 369: training loss 0.1940\n",
      "2025-05-29 21:04:12 [INFO]: epoch 370: training loss 0.1883\n",
      "2025-05-29 21:04:12 [INFO]: epoch 371: training loss 0.1680\n",
      "2025-05-29 21:04:12 [INFO]: epoch 372: training loss 0.1641\n",
      "2025-05-29 21:04:12 [INFO]: epoch 373: training loss 0.1792\n",
      "2025-05-29 21:04:12 [INFO]: epoch 374: training loss 0.2080\n",
      "2025-05-29 21:04:12 [INFO]: epoch 375: training loss 0.1909\n",
      "2025-05-29 21:04:12 [INFO]: epoch 376: training loss 0.1699\n",
      "2025-05-29 21:04:12 [INFO]: epoch 377: training loss 0.1636\n",
      "2025-05-29 21:04:12 [INFO]: epoch 378: training loss 0.1599\n",
      "2025-05-29 21:04:12 [INFO]: epoch 379: training loss 0.1892\n",
      "2025-05-29 21:04:12 [INFO]: epoch 380: training loss 0.1816\n",
      "2025-05-29 21:04:12 [INFO]: epoch 381: training loss 0.1629\n",
      "2025-05-29 21:04:12 [INFO]: epoch 382: training loss 0.1657\n",
      "2025-05-29 21:04:12 [INFO]: epoch 383: training loss 0.1684\n",
      "2025-05-29 21:04:12 [INFO]: epoch 384: training loss 0.1695\n",
      "2025-05-29 21:04:12 [INFO]: epoch 385: training loss 0.1777\n",
      "2025-05-29 21:04:12 [INFO]: epoch 386: training loss 0.1756\n",
      "2025-05-29 21:04:12 [INFO]: epoch 387: training loss 0.1690\n",
      "2025-05-29 21:04:12 [INFO]: epoch 388: training loss 0.1669\n",
      "2025-05-29 21:04:12 [INFO]: epoch 389: training loss 0.1711\n",
      "2025-05-29 21:04:12 [INFO]: epoch 390: training loss 0.1757\n",
      "2025-05-29 21:04:12 [INFO]: epoch 391: training loss 0.1656\n",
      "2025-05-29 21:04:12 [INFO]: epoch 392: training loss 0.1626\n",
      "2025-05-29 21:04:12 [INFO]: epoch 393: training loss 0.1640\n",
      "2025-05-29 21:04:12 [INFO]: epoch 394: training loss 0.1532\n",
      "2025-05-29 21:04:12 [INFO]: epoch 395: training loss 0.1590\n",
      "2025-05-29 21:04:12 [INFO]: epoch 396: training loss 0.1692\n",
      "2025-05-29 21:04:12 [INFO]: epoch 397: training loss 0.1574\n",
      "2025-05-29 21:04:12 [INFO]: epoch 398: training loss 0.1514\n",
      "2025-05-29 21:04:12 [INFO]: epoch 399: training loss 0.1596\n",
      "2025-05-29 21:04:12 [INFO]: epoch 400: training loss 0.1618\n",
      "2025-05-29 21:04:12 [INFO]: epoch 401: training loss 0.1778\n",
      "2025-05-29 21:04:12 [INFO]: epoch 402: training loss 0.1664\n",
      "2025-05-29 21:04:12 [INFO]: epoch 403: training loss 0.1569\n",
      "2025-05-29 21:04:12 [INFO]: epoch 404: training loss 0.1549\n",
      "2025-05-29 21:04:12 [INFO]: epoch 405: training loss 0.1552\n",
      "2025-05-29 21:04:12 [INFO]: epoch 406: training loss 0.1680\n",
      "2025-05-29 21:04:12 [INFO]: epoch 407: training loss 0.1643\n",
      "2025-05-29 21:04:12 [INFO]: epoch 408: training loss 0.1564\n",
      "2025-05-29 21:04:12 [INFO]: epoch 409: training loss 0.1545\n",
      "2025-05-29 21:04:12 [INFO]: epoch 410: training loss 0.1477\n",
      "2025-05-29 21:04:12 [INFO]: epoch 411: training loss 0.1576\n",
      "2025-05-29 21:04:12 [INFO]: epoch 412: training loss 0.1605\n",
      "2025-05-29 21:04:12 [INFO]: epoch 413: training loss 0.1504\n",
      "2025-05-29 21:04:12 [INFO]: epoch 414: training loss 0.1461\n",
      "2025-05-29 21:04:12 [INFO]: epoch 415: training loss 0.1510\n",
      "2025-05-29 21:04:12 [INFO]: epoch 416: training loss 0.1491\n",
      "2025-05-29 21:04:12 [INFO]: epoch 417: training loss 0.1547\n",
      "2025-05-29 21:04:12 [INFO]: epoch 418: training loss 0.1503\n",
      "2025-05-29 21:04:12 [INFO]: epoch 419: training loss 0.1497\n",
      "2025-05-29 21:04:12 [INFO]: epoch 420: training loss 0.1390\n",
      "2025-05-29 21:04:12 [INFO]: epoch 421: training loss 0.1545\n",
      "2025-05-29 21:04:12 [INFO]: epoch 422: training loss 0.1614\n",
      "2025-05-29 21:04:12 [INFO]: epoch 423: training loss 0.1423\n",
      "2025-05-29 21:04:12 [INFO]: epoch 424: training loss 0.1562\n",
      "2025-05-29 21:04:12 [INFO]: epoch 425: training loss 0.1450\n",
      "2025-05-29 21:04:12 [INFO]: epoch 426: training loss 0.1420\n",
      "2025-05-29 21:04:12 [INFO]: epoch 427: training loss 0.1491\n",
      "2025-05-29 21:04:12 [INFO]: epoch 428: training loss 0.1519\n",
      "2025-05-29 21:04:12 [INFO]: epoch 429: training loss 0.1451\n",
      "2025-05-29 21:04:12 [INFO]: epoch 430: training loss 0.1419\n",
      "2025-05-29 21:04:12 [INFO]: epoch 431: training loss 0.1419\n",
      "2025-05-29 21:04:12 [INFO]: epoch 432: training loss 0.1518\n",
      "2025-05-29 21:04:12 [INFO]: epoch 433: training loss 0.1499\n",
      "2025-05-29 21:04:12 [INFO]: epoch 434: training loss 0.1394\n",
      "2025-05-29 21:04:12 [INFO]: epoch 435: training loss 0.1403\n",
      "2025-05-29 21:04:12 [INFO]: epoch 436: training loss 0.1486\n",
      "2025-05-29 21:04:12 [INFO]: epoch 437: training loss 0.1436\n",
      "2025-05-29 21:04:12 [INFO]: epoch 438: training loss 0.1476\n",
      "2025-05-29 21:04:12 [INFO]: epoch 439: training loss 0.1505\n",
      "2025-05-29 21:04:12 [INFO]: epoch 440: training loss 0.1481\n",
      "2025-05-29 21:04:12 [INFO]: epoch 441: training loss 0.1413\n",
      "2025-05-29 21:04:12 [INFO]: epoch 442: training loss 0.1404\n",
      "2025-05-29 21:04:12 [INFO]: epoch 443: training loss 0.1484\n",
      "2025-05-29 21:04:12 [INFO]: epoch 444: training loss 0.1458\n",
      "2025-05-29 21:04:12 [INFO]: epoch 445: training loss 0.1422\n",
      "2025-05-29 21:04:12 [INFO]: epoch 446: training loss 0.1418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:12 [INFO]: epoch 447: training loss 0.1376\n",
      "2025-05-29 21:04:12 [INFO]: epoch 448: training loss 0.1486\n",
      "2025-05-29 21:04:13 [INFO]: epoch 449: training loss 0.1345\n",
      "2025-05-29 21:04:13 [INFO]: epoch 450: training loss 0.1353\n",
      "2025-05-29 21:04:13 [INFO]: epoch 451: training loss 0.1388\n",
      "2025-05-29 21:04:13 [INFO]: epoch 452: training loss 0.1384\n",
      "2025-05-29 21:04:13 [INFO]: epoch 453: training loss 0.1300\n",
      "2025-05-29 21:04:13 [INFO]: epoch 454: training loss 0.1292\n",
      "2025-05-29 21:04:13 [INFO]: epoch 455: training loss 0.1302\n",
      "2025-05-29 21:04:13 [INFO]: epoch 456: training loss 0.1391\n",
      "2025-05-29 21:04:13 [INFO]: epoch 457: training loss 0.1386\n",
      "2025-05-29 21:04:13 [INFO]: epoch 458: training loss 0.1355\n",
      "2025-05-29 21:04:13 [INFO]: epoch 459: training loss 0.1309\n",
      "2025-05-29 21:04:13 [INFO]: epoch 460: training loss 0.1371\n",
      "2025-05-29 21:04:13 [INFO]: epoch 461: training loss 0.1419\n",
      "2025-05-29 21:04:13 [INFO]: epoch 462: training loss 0.1346\n",
      "2025-05-29 21:04:13 [INFO]: epoch 463: training loss 0.1397\n",
      "2025-05-29 21:04:13 [INFO]: epoch 464: training loss 0.1372\n",
      "2025-05-29 21:04:13 [INFO]: epoch 465: training loss 0.1303\n",
      "2025-05-29 21:04:13 [INFO]: epoch 466: training loss 0.1378\n",
      "2025-05-29 21:04:13 [INFO]: epoch 467: training loss 0.1275\n",
      "2025-05-29 21:04:13 [INFO]: epoch 468: training loss 0.1344\n",
      "2025-05-29 21:04:13 [INFO]: epoch 469: training loss 0.1282\n",
      "2025-05-29 21:04:13 [INFO]: epoch 470: training loss 0.1412\n",
      "2025-05-29 21:04:13 [INFO]: epoch 471: training loss 0.1264\n",
      "2025-05-29 21:04:13 [INFO]: epoch 472: training loss 0.1328\n",
      "2025-05-29 21:04:13 [INFO]: epoch 473: training loss 0.1238\n",
      "2025-05-29 21:04:13 [INFO]: epoch 474: training loss 0.1334\n",
      "2025-05-29 21:04:13 [INFO]: epoch 475: training loss 0.1314\n",
      "2025-05-29 21:04:13 [INFO]: epoch 476: training loss 0.1229\n",
      "2025-05-29 21:04:13 [INFO]: epoch 477: training loss 0.1209\n",
      "2025-05-29 21:04:13 [INFO]: epoch 478: training loss 0.1348\n",
      "2025-05-29 21:04:13 [INFO]: epoch 479: training loss 0.1266\n",
      "2025-05-29 21:04:13 [INFO]: epoch 480: training loss 0.1241\n",
      "2025-05-29 21:04:13 [INFO]: epoch 481: training loss 0.1261\n",
      "2025-05-29 21:04:13 [INFO]: epoch 482: training loss 0.1278\n",
      "2025-05-29 21:04:13 [INFO]: epoch 483: training loss 0.1230\n",
      "2025-05-29 21:04:13 [INFO]: epoch 484: training loss 0.1293\n",
      "2025-05-29 21:04:13 [INFO]: epoch 485: training loss 0.1259\n",
      "2025-05-29 21:04:13 [INFO]: epoch 486: training loss 0.1275\n",
      "2025-05-29 21:04:13 [INFO]: epoch 487: training loss 0.1261\n",
      "2025-05-29 21:04:13 [INFO]: epoch 488: training loss 0.1231\n",
      "2025-05-29 21:04:13 [INFO]: epoch 489: training loss 0.1269\n",
      "2025-05-29 21:04:13 [INFO]: epoch 490: training loss 0.1231\n",
      "2025-05-29 21:04:13 [INFO]: epoch 491: training loss 0.1288\n",
      "2025-05-29 21:04:13 [INFO]: epoch 492: training loss 0.1194\n",
      "2025-05-29 21:04:13 [INFO]: epoch 493: training loss 0.1281\n",
      "2025-05-29 21:04:13 [INFO]: epoch 494: training loss 0.1243\n",
      "2025-05-29 21:04:13 [INFO]: epoch 495: training loss 0.1256\n",
      "2025-05-29 21:04:13 [INFO]: epoch 496: training loss 0.1243\n",
      "2025-05-29 21:04:13 [INFO]: epoch 497: training loss 0.1207\n",
      "2025-05-29 21:04:13 [INFO]: epoch 498: training loss 0.1245\n",
      "2025-05-29 21:04:13 [INFO]: epoch 499: training loss 0.1243\n",
      "2025-05-29 21:04:13 [INFO]: epoch 500: training loss 0.1256\n",
      "2025-05-29 21:04:13 [INFO]: epoch 501: training loss 0.1205\n",
      "2025-05-29 21:04:13 [INFO]: epoch 502: training loss 0.1225\n",
      "2025-05-29 21:04:13 [INFO]: epoch 503: training loss 0.1214\n",
      "2025-05-29 21:04:13 [INFO]: epoch 504: training loss 0.1174\n",
      "2025-05-29 21:04:13 [INFO]: epoch 505: training loss 0.1301\n",
      "2025-05-29 21:04:13 [INFO]: epoch 506: training loss 0.1286\n",
      "2025-05-29 21:04:13 [INFO]: epoch 507: training loss 0.1156\n",
      "2025-05-29 21:04:13 [INFO]: epoch 508: training loss 0.1191\n",
      "2025-05-29 21:04:13 [INFO]: epoch 509: training loss 0.1197\n",
      "2025-05-29 21:04:13 [INFO]: epoch 510: training loss 0.1284\n",
      "2025-05-29 21:04:13 [INFO]: epoch 511: training loss 0.1270\n",
      "2025-05-29 21:04:13 [INFO]: epoch 512: training loss 0.1128\n",
      "2025-05-29 21:04:13 [INFO]: epoch 513: training loss 0.1132\n",
      "2025-05-29 21:04:13 [INFO]: epoch 514: training loss 0.1225\n",
      "2025-05-29 21:04:13 [INFO]: epoch 515: training loss 0.1208\n",
      "2025-05-29 21:04:13 [INFO]: epoch 516: training loss 0.1208\n",
      "2025-05-29 21:04:13 [INFO]: epoch 517: training loss 0.1214\n",
      "2025-05-29 21:04:13 [INFO]: epoch 518: training loss 0.1174\n",
      "2025-05-29 21:04:13 [INFO]: epoch 519: training loss 0.1230\n",
      "2025-05-29 21:04:13 [INFO]: epoch 520: training loss 0.1264\n",
      "2025-05-29 21:04:13 [INFO]: epoch 521: training loss 0.1253\n",
      "2025-05-29 21:04:13 [INFO]: epoch 522: training loss 0.1210\n",
      "2025-05-29 21:04:13 [INFO]: epoch 523: training loss 0.1125\n",
      "2025-05-29 21:04:13 [INFO]: epoch 524: training loss 0.1227\n",
      "2025-05-29 21:04:13 [INFO]: epoch 525: training loss 0.1370\n",
      "2025-05-29 21:04:13 [INFO]: epoch 526: training loss 0.1418\n",
      "2025-05-29 21:04:13 [INFO]: epoch 527: training loss 0.1260\n",
      "2025-05-29 21:04:13 [INFO]: epoch 528: training loss 0.1132\n",
      "2025-05-29 21:04:14 [INFO]: epoch 529: training loss 0.1260\n",
      "2025-05-29 21:04:14 [INFO]: epoch 530: training loss 0.1358\n",
      "2025-05-29 21:04:14 [INFO]: epoch 531: training loss 0.1397\n",
      "2025-05-29 21:04:14 [INFO]: epoch 532: training loss 0.1307\n",
      "2025-05-29 21:04:14 [INFO]: epoch 533: training loss 0.1212\n",
      "2025-05-29 21:04:14 [INFO]: epoch 534: training loss 0.1207\n",
      "2025-05-29 21:04:14 [INFO]: epoch 535: training loss 0.1240\n",
      "2025-05-29 21:04:14 [INFO]: epoch 536: training loss 0.1278\n",
      "2025-05-29 21:04:14 [INFO]: epoch 537: training loss 0.1203\n",
      "2025-05-29 21:04:14 [INFO]: epoch 538: training loss 0.1202\n",
      "2025-05-29 21:04:14 [INFO]: epoch 539: training loss 0.1173\n",
      "2025-05-29 21:04:14 [INFO]: epoch 540: training loss 0.1154\n",
      "2025-05-29 21:04:14 [INFO]: epoch 541: training loss 0.1166\n",
      "2025-05-29 21:04:14 [INFO]: epoch 542: training loss 0.1153\n",
      "2025-05-29 21:04:14 [INFO]: epoch 543: training loss 0.1207\n",
      "2025-05-29 21:04:14 [INFO]: epoch 544: training loss 0.1189\n",
      "2025-05-29 21:04:14 [INFO]: epoch 545: training loss 0.1079\n",
      "2025-05-29 21:04:14 [INFO]: epoch 546: training loss 0.1135\n",
      "2025-05-29 21:04:14 [INFO]: epoch 547: training loss 0.1131\n",
      "2025-05-29 21:04:14 [INFO]: epoch 548: training loss 0.1043\n",
      "2025-05-29 21:04:14 [INFO]: epoch 549: training loss 0.1129\n",
      "2025-05-29 21:04:14 [INFO]: epoch 550: training loss 0.1129\n",
      "2025-05-29 21:04:14 [INFO]: epoch 551: training loss 0.1026\n",
      "2025-05-29 21:04:14 [INFO]: epoch 552: training loss 0.1064\n",
      "2025-05-29 21:04:14 [INFO]: epoch 553: training loss 0.1119\n",
      "2025-05-29 21:04:14 [INFO]: epoch 554: training loss 0.1137\n",
      "2025-05-29 21:04:14 [INFO]: epoch 555: training loss 0.1071\n",
      "2025-05-29 21:04:14 [INFO]: epoch 556: training loss 0.1006\n",
      "2025-05-29 21:04:14 [INFO]: epoch 557: training loss 0.1038\n",
      "2025-05-29 21:04:14 [INFO]: epoch 558: training loss 0.1058\n",
      "2025-05-29 21:04:14 [INFO]: epoch 559: training loss 0.1028\n",
      "2025-05-29 21:04:14 [INFO]: epoch 560: training loss 0.1045\n",
      "2025-05-29 21:04:14 [INFO]: epoch 561: training loss 0.1033\n",
      "2025-05-29 21:04:14 [INFO]: epoch 562: training loss 0.1073\n",
      "2025-05-29 21:04:14 [INFO]: epoch 563: training loss 0.1145\n",
      "2025-05-29 21:04:14 [INFO]: epoch 564: training loss 0.1050\n",
      "2025-05-29 21:04:14 [INFO]: epoch 565: training loss 0.1089\n",
      "2025-05-29 21:04:14 [INFO]: epoch 566: training loss 0.1040\n",
      "2025-05-29 21:04:14 [INFO]: epoch 567: training loss 0.1013\n",
      "2025-05-29 21:04:14 [INFO]: epoch 568: training loss 0.0992\n",
      "2025-05-29 21:04:14 [INFO]: epoch 569: training loss 0.1025\n",
      "2025-05-29 21:04:14 [INFO]: epoch 570: training loss 0.0979\n",
      "2025-05-29 21:04:14 [INFO]: epoch 571: training loss 0.1056\n",
      "2025-05-29 21:04:14 [INFO]: epoch 572: training loss 0.1057\n",
      "2025-05-29 21:04:14 [INFO]: epoch 573: training loss 0.0959\n",
      "2025-05-29 21:04:14 [INFO]: epoch 574: training loss 0.1006\n",
      "2025-05-29 21:04:14 [INFO]: epoch 575: training loss 0.0981\n",
      "2025-05-29 21:04:14 [INFO]: epoch 576: training loss 0.1084\n",
      "2025-05-29 21:04:14 [INFO]: epoch 577: training loss 0.1043\n",
      "2025-05-29 21:04:14 [INFO]: epoch 578: training loss 0.1035\n",
      "2025-05-29 21:04:14 [INFO]: epoch 579: training loss 0.0993\n",
      "2025-05-29 21:04:14 [INFO]: epoch 580: training loss 0.1055\n",
      "2025-05-29 21:04:14 [INFO]: epoch 581: training loss 0.1002\n",
      "2025-05-29 21:04:14 [INFO]: epoch 582: training loss 0.1023\n",
      "2025-05-29 21:04:14 [INFO]: epoch 583: training loss 0.1037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:14 [INFO]: epoch 584: training loss 0.0894\n",
      "2025-05-29 21:04:14 [INFO]: epoch 585: training loss 0.1036\n",
      "2025-05-29 21:04:14 [INFO]: epoch 586: training loss 0.1001\n",
      "2025-05-29 21:04:14 [INFO]: epoch 587: training loss 0.0995\n",
      "2025-05-29 21:04:14 [INFO]: epoch 588: training loss 0.0948\n",
      "2025-05-29 21:04:14 [INFO]: epoch 589: training loss 0.0914\n",
      "2025-05-29 21:04:14 [INFO]: epoch 590: training loss 0.0979\n",
      "2025-05-29 21:04:14 [INFO]: epoch 591: training loss 0.0995\n",
      "2025-05-29 21:04:14 [INFO]: epoch 592: training loss 0.0931\n",
      "2025-05-29 21:04:14 [INFO]: epoch 593: training loss 0.1038\n",
      "2025-05-29 21:04:14 [INFO]: epoch 594: training loss 0.1030\n",
      "2025-05-29 21:04:14 [INFO]: epoch 595: training loss 0.0963\n",
      "2025-05-29 21:04:14 [INFO]: epoch 596: training loss 0.1004\n",
      "2025-05-29 21:04:14 [INFO]: epoch 597: training loss 0.0943\n",
      "2025-05-29 21:04:14 [INFO]: epoch 598: training loss 0.0966\n",
      "2025-05-29 21:04:14 [INFO]: epoch 599: training loss 0.0987\n",
      "2025-05-29 21:04:14 [INFO]: Finished training.\n",
      "2025-05-29 21:04:14 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:15<00:22,  7.56s/it]2025-05-29 21:04:14 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:04:14 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:04:14 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:04:14 [INFO]: epoch 0: training loss 1.2160\n",
      "2025-05-29 21:04:14 [INFO]: epoch 1: training loss 0.6079\n",
      "2025-05-29 21:04:14 [INFO]: epoch 2: training loss 0.6058\n",
      "2025-05-29 21:04:14 [INFO]: epoch 3: training loss 0.6640\n",
      "2025-05-29 21:04:15 [INFO]: epoch 4: training loss 0.5969\n",
      "2025-05-29 21:04:15 [INFO]: epoch 5: training loss 0.4300\n",
      "2025-05-29 21:04:15 [INFO]: epoch 6: training loss 0.4537\n",
      "2025-05-29 21:04:15 [INFO]: epoch 7: training loss 0.4306\n",
      "2025-05-29 21:04:15 [INFO]: epoch 8: training loss 0.4407\n",
      "2025-05-29 21:04:15 [INFO]: epoch 9: training loss 0.3988\n",
      "2025-05-29 21:04:15 [INFO]: epoch 10: training loss 0.3861\n",
      "2025-05-29 21:04:15 [INFO]: epoch 11: training loss 0.3729\n",
      "2025-05-29 21:04:15 [INFO]: epoch 12: training loss 0.3916\n",
      "2025-05-29 21:04:15 [INFO]: epoch 13: training loss 0.3706\n",
      "2025-05-29 21:04:15 [INFO]: epoch 14: training loss 0.3273\n",
      "2025-05-29 21:04:15 [INFO]: epoch 15: training loss 0.3358\n",
      "2025-05-29 21:04:15 [INFO]: epoch 16: training loss 0.3305\n",
      "2025-05-29 21:04:15 [INFO]: epoch 17: training loss 0.2917\n",
      "2025-05-29 21:04:15 [INFO]: epoch 18: training loss 0.3012\n",
      "2025-05-29 21:04:15 [INFO]: epoch 19: training loss 0.3137\n",
      "2025-05-29 21:04:15 [INFO]: epoch 20: training loss 0.3026\n",
      "2025-05-29 21:04:15 [INFO]: epoch 21: training loss 0.3095\n",
      "2025-05-29 21:04:15 [INFO]: epoch 22: training loss 0.2754\n",
      "2025-05-29 21:04:15 [INFO]: epoch 23: training loss 0.2996\n",
      "2025-05-29 21:04:15 [INFO]: epoch 24: training loss 0.2944\n",
      "2025-05-29 21:04:15 [INFO]: epoch 25: training loss 0.2698\n",
      "2025-05-29 21:04:15 [INFO]: epoch 26: training loss 0.2655\n",
      "2025-05-29 21:04:15 [INFO]: epoch 27: training loss 0.2929\n",
      "2025-05-29 21:04:15 [INFO]: epoch 28: training loss 0.2833\n",
      "2025-05-29 21:04:15 [INFO]: epoch 29: training loss 0.2874\n",
      "2025-05-29 21:04:15 [INFO]: epoch 30: training loss 0.2527\n",
      "2025-05-29 21:04:15 [INFO]: epoch 31: training loss 0.2654\n",
      "2025-05-29 21:04:15 [INFO]: epoch 32: training loss 0.2580\n",
      "2025-05-29 21:04:15 [INFO]: epoch 33: training loss 0.2481\n",
      "2025-05-29 21:04:15 [INFO]: epoch 34: training loss 0.2506\n",
      "2025-05-29 21:04:15 [INFO]: epoch 35: training loss 0.2567\n",
      "2025-05-29 21:04:15 [INFO]: epoch 36: training loss 0.2473\n",
      "2025-05-29 21:04:15 [INFO]: epoch 37: training loss 0.2397\n",
      "2025-05-29 21:04:15 [INFO]: epoch 38: training loss 0.2244\n",
      "2025-05-29 21:04:15 [INFO]: epoch 39: training loss 0.2558\n",
      "2025-05-29 21:04:15 [INFO]: epoch 40: training loss 0.2509\n",
      "2025-05-29 21:04:15 [INFO]: epoch 41: training loss 0.2570\n",
      "2025-05-29 21:04:15 [INFO]: epoch 42: training loss 0.2425\n",
      "2025-05-29 21:04:15 [INFO]: epoch 43: training loss 0.2582\n",
      "2025-05-29 21:04:15 [INFO]: epoch 44: training loss 0.2436\n",
      "2025-05-29 21:04:15 [INFO]: epoch 45: training loss 0.2184\n",
      "2025-05-29 21:04:15 [INFO]: epoch 46: training loss 0.2359\n",
      "2025-05-29 21:04:15 [INFO]: epoch 47: training loss 0.2458\n",
      "2025-05-29 21:04:15 [INFO]: epoch 48: training loss 0.2376\n",
      "2025-05-29 21:04:15 [INFO]: epoch 49: training loss 0.2263\n",
      "2025-05-29 21:04:15 [INFO]: epoch 50: training loss 0.2228\n",
      "2025-05-29 21:04:15 [INFO]: epoch 51: training loss 0.2062\n",
      "2025-05-29 21:04:15 [INFO]: epoch 52: training loss 0.2084\n",
      "2025-05-29 21:04:15 [INFO]: epoch 53: training loss 0.2360\n",
      "2025-05-29 21:04:15 [INFO]: epoch 54: training loss 0.2418\n",
      "2025-05-29 21:04:15 [INFO]: epoch 55: training loss 0.2231\n",
      "2025-05-29 21:04:15 [INFO]: epoch 56: training loss 0.2056\n",
      "2025-05-29 21:04:15 [INFO]: epoch 57: training loss 0.2334\n",
      "2025-05-29 21:04:15 [INFO]: epoch 58: training loss 0.2164\n",
      "2025-05-29 21:04:15 [INFO]: epoch 59: training loss 0.2451\n",
      "2025-05-29 21:04:15 [INFO]: epoch 60: training loss 0.1863\n",
      "2025-05-29 21:04:15 [INFO]: epoch 61: training loss 0.2055\n",
      "2025-05-29 21:04:15 [INFO]: epoch 62: training loss 0.2134\n",
      "2025-05-29 21:04:15 [INFO]: epoch 63: training loss 0.2266\n",
      "2025-05-29 21:04:15 [INFO]: epoch 64: training loss 0.2005\n",
      "2025-05-29 21:04:15 [INFO]: epoch 65: training loss 0.2134\n",
      "2025-05-29 21:04:15 [INFO]: epoch 66: training loss 0.1794\n",
      "2025-05-29 21:04:15 [INFO]: epoch 67: training loss 0.2036\n",
      "2025-05-29 21:04:15 [INFO]: epoch 68: training loss 0.1881\n",
      "2025-05-29 21:04:15 [INFO]: epoch 69: training loss 0.1733\n",
      "2025-05-29 21:04:15 [INFO]: epoch 70: training loss 0.1742\n",
      "2025-05-29 21:04:15 [INFO]: epoch 71: training loss 0.1893\n",
      "2025-05-29 21:04:15 [INFO]: epoch 72: training loss 0.1820\n",
      "2025-05-29 21:04:15 [INFO]: epoch 73: training loss 0.1916\n",
      "2025-05-29 21:04:15 [INFO]: epoch 74: training loss 0.1823\n",
      "2025-05-29 21:04:15 [INFO]: epoch 75: training loss 0.1802\n",
      "2025-05-29 21:04:15 [INFO]: epoch 76: training loss 0.1839\n",
      "2025-05-29 21:04:15 [INFO]: epoch 77: training loss 0.1819\n",
      "2025-05-29 21:04:15 [INFO]: epoch 78: training loss 0.1736\n",
      "2025-05-29 21:04:15 [INFO]: epoch 79: training loss 0.1755\n",
      "2025-05-29 21:04:15 [INFO]: epoch 80: training loss 0.1848\n",
      "2025-05-29 21:04:15 [INFO]: epoch 81: training loss 0.1842\n",
      "2025-05-29 21:04:15 [INFO]: epoch 82: training loss 0.1646\n",
      "2025-05-29 21:04:15 [INFO]: epoch 83: training loss 0.1542\n",
      "2025-05-29 21:04:16 [INFO]: epoch 84: training loss 0.1744\n",
      "2025-05-29 21:04:16 [INFO]: epoch 85: training loss 0.1875\n",
      "2025-05-29 21:04:16 [INFO]: epoch 86: training loss 0.1596\n",
      "2025-05-29 21:04:16 [INFO]: epoch 87: training loss 0.1774\n",
      "2025-05-29 21:04:16 [INFO]: epoch 88: training loss 0.1702\n",
      "2025-05-29 21:04:16 [INFO]: epoch 89: training loss 0.1564\n",
      "2025-05-29 21:04:16 [INFO]: epoch 90: training loss 0.1487\n",
      "2025-05-29 21:04:16 [INFO]: epoch 91: training loss 0.1502\n",
      "2025-05-29 21:04:16 [INFO]: epoch 92: training loss 0.1617\n",
      "2025-05-29 21:04:16 [INFO]: epoch 93: training loss 0.1692\n",
      "2025-05-29 21:04:16 [INFO]: epoch 94: training loss 0.1574\n",
      "2025-05-29 21:04:16 [INFO]: epoch 95: training loss 0.1533\n",
      "2025-05-29 21:04:16 [INFO]: epoch 96: training loss 0.1418\n",
      "2025-05-29 21:04:16 [INFO]: epoch 97: training loss 0.1469\n",
      "2025-05-29 21:04:16 [INFO]: epoch 98: training loss 0.1504\n",
      "2025-05-29 21:04:16 [INFO]: epoch 99: training loss 0.1344\n",
      "2025-05-29 21:04:16 [INFO]: epoch 100: training loss 0.1413\n",
      "2025-05-29 21:04:16 [INFO]: epoch 101: training loss 0.1517\n",
      "2025-05-29 21:04:16 [INFO]: epoch 102: training loss 0.1470\n",
      "2025-05-29 21:04:16 [INFO]: epoch 103: training loss 0.1585\n",
      "2025-05-29 21:04:16 [INFO]: epoch 104: training loss 0.1836\n",
      "2025-05-29 21:04:16 [INFO]: epoch 105: training loss 0.1564\n",
      "2025-05-29 21:04:16 [INFO]: epoch 106: training loss 0.1509\n",
      "2025-05-29 21:04:16 [INFO]: epoch 107: training loss 0.1474\n",
      "2025-05-29 21:04:16 [INFO]: epoch 108: training loss 0.1774\n",
      "2025-05-29 21:04:16 [INFO]: epoch 109: training loss 0.1700\n",
      "2025-05-29 21:04:16 [INFO]: epoch 110: training loss 0.1478\n",
      "2025-05-29 21:04:16 [INFO]: epoch 111: training loss 0.1648\n",
      "2025-05-29 21:04:16 [INFO]: epoch 112: training loss 0.1412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:16 [INFO]: epoch 113: training loss 0.1262\n",
      "2025-05-29 21:04:16 [INFO]: epoch 114: training loss 0.1603\n",
      "2025-05-29 21:04:16 [INFO]: epoch 115: training loss 0.1482\n",
      "2025-05-29 21:04:16 [INFO]: epoch 116: training loss 0.1400\n",
      "2025-05-29 21:04:16 [INFO]: epoch 117: training loss 0.1359\n",
      "2025-05-29 21:04:16 [INFO]: epoch 118: training loss 0.1538\n",
      "2025-05-29 21:04:16 [INFO]: epoch 119: training loss 0.1404\n",
      "2025-05-29 21:04:16 [INFO]: epoch 120: training loss 0.1348\n",
      "2025-05-29 21:04:16 [INFO]: epoch 121: training loss 0.1484\n",
      "2025-05-29 21:04:16 [INFO]: epoch 122: training loss 0.1198\n",
      "2025-05-29 21:04:16 [INFO]: epoch 123: training loss 0.1219\n",
      "2025-05-29 21:04:16 [INFO]: epoch 124: training loss 0.1405\n",
      "2025-05-29 21:04:16 [INFO]: epoch 125: training loss 0.1242\n",
      "2025-05-29 21:04:16 [INFO]: epoch 126: training loss 0.1431\n",
      "2025-05-29 21:04:16 [INFO]: epoch 127: training loss 0.1402\n",
      "2025-05-29 21:04:16 [INFO]: epoch 128: training loss 0.1653\n",
      "2025-05-29 21:04:16 [INFO]: epoch 129: training loss 0.1488\n",
      "2025-05-29 21:04:16 [INFO]: epoch 130: training loss 0.1211\n",
      "2025-05-29 21:04:16 [INFO]: epoch 131: training loss 0.1231\n",
      "2025-05-29 21:04:16 [INFO]: epoch 132: training loss 0.1460\n",
      "2025-05-29 21:04:16 [INFO]: epoch 133: training loss 0.1273\n",
      "2025-05-29 21:04:16 [INFO]: epoch 134: training loss 0.1124\n",
      "2025-05-29 21:04:16 [INFO]: epoch 135: training loss 0.1226\n",
      "2025-05-29 21:04:16 [INFO]: epoch 136: training loss 0.1407\n",
      "2025-05-29 21:04:16 [INFO]: epoch 137: training loss 0.1105\n",
      "2025-05-29 21:04:16 [INFO]: epoch 138: training loss 0.1292\n",
      "2025-05-29 21:04:16 [INFO]: epoch 139: training loss 0.1206\n",
      "2025-05-29 21:04:16 [INFO]: epoch 140: training loss 0.1305\n",
      "2025-05-29 21:04:16 [INFO]: epoch 141: training loss 0.1254\n",
      "2025-05-29 21:04:16 [INFO]: epoch 142: training loss 0.1362\n",
      "2025-05-29 21:04:16 [INFO]: epoch 143: training loss 0.1221\n",
      "2025-05-29 21:04:16 [INFO]: epoch 144: training loss 0.1149\n",
      "2025-05-29 21:04:16 [INFO]: epoch 145: training loss 0.1076\n",
      "2025-05-29 21:04:16 [INFO]: epoch 146: training loss 0.1314\n",
      "2025-05-29 21:04:16 [INFO]: epoch 147: training loss 0.1364\n",
      "2025-05-29 21:04:16 [INFO]: epoch 148: training loss 0.1286\n",
      "2025-05-29 21:04:16 [INFO]: epoch 149: training loss 0.1293\n",
      "2025-05-29 21:04:16 [INFO]: epoch 150: training loss 0.1116\n",
      "2025-05-29 21:04:16 [INFO]: epoch 151: training loss 0.1218\n",
      "2025-05-29 21:04:16 [INFO]: epoch 152: training loss 0.1129\n",
      "2025-05-29 21:04:16 [INFO]: epoch 153: training loss 0.1155\n",
      "2025-05-29 21:04:16 [INFO]: epoch 154: training loss 0.1194\n",
      "2025-05-29 21:04:16 [INFO]: epoch 155: training loss 0.1179\n",
      "2025-05-29 21:04:16 [INFO]: epoch 156: training loss 0.1222\n",
      "2025-05-29 21:04:16 [INFO]: epoch 157: training loss 0.1021\n",
      "2025-05-29 21:04:16 [INFO]: epoch 158: training loss 0.1075\n",
      "2025-05-29 21:04:16 [INFO]: epoch 159: training loss 0.1295\n",
      "2025-05-29 21:04:16 [INFO]: epoch 160: training loss 0.1163\n",
      "2025-05-29 21:04:16 [INFO]: epoch 161: training loss 0.1271\n",
      "2025-05-29 21:04:16 [INFO]: epoch 162: training loss 0.1093\n",
      "2025-05-29 21:04:16 [INFO]: epoch 163: training loss 0.1124\n",
      "2025-05-29 21:04:16 [INFO]: epoch 164: training loss 0.1374\n",
      "2025-05-29 21:04:16 [INFO]: epoch 165: training loss 0.1163\n",
      "2025-05-29 21:04:17 [INFO]: epoch 166: training loss 0.1116\n",
      "2025-05-29 21:04:17 [INFO]: epoch 167: training loss 0.1172\n",
      "2025-05-29 21:04:17 [INFO]: epoch 168: training loss 0.1106\n",
      "2025-05-29 21:04:17 [INFO]: epoch 169: training loss 0.0935\n",
      "2025-05-29 21:04:17 [INFO]: epoch 170: training loss 0.1022\n",
      "2025-05-29 21:04:17 [INFO]: epoch 171: training loss 0.1103\n",
      "2025-05-29 21:04:17 [INFO]: epoch 172: training loss 0.1097\n",
      "2025-05-29 21:04:17 [INFO]: epoch 173: training loss 0.1142\n",
      "2025-05-29 21:04:17 [INFO]: epoch 174: training loss 0.1081\n",
      "2025-05-29 21:04:17 [INFO]: epoch 175: training loss 0.0976\n",
      "2025-05-29 21:04:17 [INFO]: epoch 176: training loss 0.1207\n",
      "2025-05-29 21:04:17 [INFO]: epoch 177: training loss 0.1195\n",
      "2025-05-29 21:04:17 [INFO]: epoch 178: training loss 0.1085\n",
      "2025-05-29 21:04:17 [INFO]: epoch 179: training loss 0.1174\n",
      "2025-05-29 21:04:17 [INFO]: epoch 180: training loss 0.1064\n",
      "2025-05-29 21:04:17 [INFO]: epoch 181: training loss 0.1198\n",
      "2025-05-29 21:04:17 [INFO]: epoch 182: training loss 0.1138\n",
      "2025-05-29 21:04:17 [INFO]: epoch 183: training loss 0.0893\n",
      "2025-05-29 21:04:17 [INFO]: epoch 184: training loss 0.1159\n",
      "2025-05-29 21:04:17 [INFO]: epoch 185: training loss 0.1264\n",
      "2025-05-29 21:04:17 [INFO]: epoch 186: training loss 0.1088\n",
      "2025-05-29 21:04:17 [INFO]: epoch 187: training loss 0.1009\n",
      "2025-05-29 21:04:17 [INFO]: epoch 188: training loss 0.1113\n",
      "2025-05-29 21:04:17 [INFO]: epoch 189: training loss 0.1183\n",
      "2025-05-29 21:04:17 [INFO]: epoch 190: training loss 0.1112\n",
      "2025-05-29 21:04:17 [INFO]: epoch 191: training loss 0.1018\n",
      "2025-05-29 21:04:17 [INFO]: epoch 192: training loss 0.1021\n",
      "2025-05-29 21:04:17 [INFO]: epoch 193: training loss 0.1024\n",
      "2025-05-29 21:04:17 [INFO]: epoch 194: training loss 0.0975\n",
      "2025-05-29 21:04:17 [INFO]: epoch 195: training loss 0.1136\n",
      "2025-05-29 21:04:17 [INFO]: epoch 196: training loss 0.0917\n",
      "2025-05-29 21:04:17 [INFO]: epoch 197: training loss 0.1159\n",
      "2025-05-29 21:04:17 [INFO]: epoch 198: training loss 0.1009\n",
      "2025-05-29 21:04:17 [INFO]: epoch 199: training loss 0.0959\n",
      "2025-05-29 21:04:17 [INFO]: epoch 200: training loss 0.0944\n",
      "2025-05-29 21:04:17 [INFO]: epoch 201: training loss 0.0894\n",
      "2025-05-29 21:04:17 [INFO]: epoch 202: training loss 0.0963\n",
      "2025-05-29 21:04:17 [INFO]: epoch 203: training loss 0.0967\n",
      "2025-05-29 21:04:17 [INFO]: epoch 204: training loss 0.0965\n",
      "2025-05-29 21:04:17 [INFO]: epoch 205: training loss 0.0901\n",
      "2025-05-29 21:04:17 [INFO]: epoch 206: training loss 0.1014\n",
      "2025-05-29 21:04:17 [INFO]: epoch 207: training loss 0.0962\n",
      "2025-05-29 21:04:17 [INFO]: epoch 208: training loss 0.1067\n",
      "2025-05-29 21:04:17 [INFO]: epoch 209: training loss 0.0942\n",
      "2025-05-29 21:04:17 [INFO]: epoch 210: training loss 0.0815\n",
      "2025-05-29 21:04:17 [INFO]: epoch 211: training loss 0.0926\n",
      "2025-05-29 21:04:17 [INFO]: epoch 212: training loss 0.0920\n",
      "2025-05-29 21:04:17 [INFO]: epoch 213: training loss 0.0788\n",
      "2025-05-29 21:04:17 [INFO]: epoch 214: training loss 0.0909\n",
      "2025-05-29 21:04:17 [INFO]: epoch 215: training loss 0.0859\n",
      "2025-05-29 21:04:17 [INFO]: epoch 216: training loss 0.0755\n",
      "2025-05-29 21:04:17 [INFO]: epoch 217: training loss 0.0771\n",
      "2025-05-29 21:04:17 [INFO]: epoch 218: training loss 0.0834\n",
      "2025-05-29 21:04:17 [INFO]: epoch 219: training loss 0.0759\n",
      "2025-05-29 21:04:17 [INFO]: epoch 220: training loss 0.0677\n",
      "2025-05-29 21:04:17 [INFO]: epoch 221: training loss 0.0866\n",
      "2025-05-29 21:04:17 [INFO]: epoch 222: training loss 0.0764\n",
      "2025-05-29 21:04:17 [INFO]: epoch 223: training loss 0.0811\n",
      "2025-05-29 21:04:17 [INFO]: epoch 224: training loss 0.0810\n",
      "2025-05-29 21:04:17 [INFO]: epoch 225: training loss 0.0864\n",
      "2025-05-29 21:04:17 [INFO]: epoch 226: training loss 0.0957\n",
      "2025-05-29 21:04:17 [INFO]: epoch 227: training loss 0.0861\n",
      "2025-05-29 21:04:17 [INFO]: epoch 228: training loss 0.0924\n",
      "2025-05-29 21:04:17 [INFO]: epoch 229: training loss 0.1030\n",
      "2025-05-29 21:04:17 [INFO]: epoch 230: training loss 0.0919\n",
      "2025-05-29 21:04:17 [INFO]: epoch 231: training loss 0.0919\n",
      "2025-05-29 21:04:17 [INFO]: epoch 232: training loss 0.0844\n",
      "2025-05-29 21:04:17 [INFO]: epoch 233: training loss 0.0871\n",
      "2025-05-29 21:04:17 [INFO]: epoch 234: training loss 0.0866\n",
      "2025-05-29 21:04:17 [INFO]: epoch 235: training loss 0.0728\n",
      "2025-05-29 21:04:17 [INFO]: epoch 236: training loss 0.0795\n",
      "2025-05-29 21:04:17 [INFO]: epoch 237: training loss 0.0663\n",
      "2025-05-29 21:04:17 [INFO]: epoch 238: training loss 0.0660\n",
      "2025-05-29 21:04:17 [INFO]: epoch 239: training loss 0.0876\n",
      "2025-05-29 21:04:17 [INFO]: epoch 240: training loss 0.0859\n",
      "2025-05-29 21:04:17 [INFO]: epoch 241: training loss 0.0720\n",
      "2025-05-29 21:04:17 [INFO]: epoch 242: training loss 0.0723\n",
      "2025-05-29 21:04:17 [INFO]: epoch 243: training loss 0.0758\n",
      "2025-05-29 21:04:17 [INFO]: epoch 244: training loss 0.0744\n",
      "2025-05-29 21:04:17 [INFO]: epoch 245: training loss 0.0656\n",
      "2025-05-29 21:04:18 [INFO]: epoch 246: training loss 0.0668\n",
      "2025-05-29 21:04:18 [INFO]: epoch 247: training loss 0.0760\n",
      "2025-05-29 21:04:18 [INFO]: epoch 248: training loss 0.0799\n",
      "2025-05-29 21:04:18 [INFO]: epoch 249: training loss 0.0693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:18 [INFO]: epoch 250: training loss 0.0672\n",
      "2025-05-29 21:04:18 [INFO]: epoch 251: training loss 0.0672\n",
      "2025-05-29 21:04:18 [INFO]: epoch 252: training loss 0.0639\n",
      "2025-05-29 21:04:18 [INFO]: epoch 253: training loss 0.0607\n",
      "2025-05-29 21:04:18 [INFO]: epoch 254: training loss 0.0682\n",
      "2025-05-29 21:04:18 [INFO]: epoch 255: training loss 0.0775\n",
      "2025-05-29 21:04:18 [INFO]: epoch 256: training loss 0.0560\n",
      "2025-05-29 21:04:18 [INFO]: epoch 257: training loss 0.0597\n",
      "2025-05-29 21:04:18 [INFO]: epoch 258: training loss 0.0740\n",
      "2025-05-29 21:04:18 [INFO]: epoch 259: training loss 0.0690\n",
      "2025-05-29 21:04:18 [INFO]: epoch 260: training loss 0.0661\n",
      "2025-05-29 21:04:18 [INFO]: epoch 261: training loss 0.0728\n",
      "2025-05-29 21:04:18 [INFO]: epoch 262: training loss 0.0692\n",
      "2025-05-29 21:04:18 [INFO]: epoch 263: training loss 0.0632\n",
      "2025-05-29 21:04:18 [INFO]: epoch 264: training loss 0.0503\n",
      "2025-05-29 21:04:18 [INFO]: epoch 265: training loss 0.0546\n",
      "2025-05-29 21:04:18 [INFO]: epoch 266: training loss 0.0625\n",
      "2025-05-29 21:04:18 [INFO]: epoch 267: training loss 0.0654\n",
      "2025-05-29 21:04:18 [INFO]: epoch 268: training loss 0.0581\n",
      "2025-05-29 21:04:18 [INFO]: epoch 269: training loss 0.0539\n",
      "2025-05-29 21:04:18 [INFO]: epoch 270: training loss 0.0619\n",
      "2025-05-29 21:04:18 [INFO]: epoch 271: training loss 0.0553\n",
      "2025-05-29 21:04:18 [INFO]: epoch 272: training loss 0.0543\n",
      "2025-05-29 21:04:18 [INFO]: epoch 273: training loss 0.0503\n",
      "2025-05-29 21:04:18 [INFO]: epoch 274: training loss 0.0577\n",
      "2025-05-29 21:04:18 [INFO]: epoch 275: training loss 0.0585\n",
      "2025-05-29 21:04:18 [INFO]: epoch 276: training loss 0.0614\n",
      "2025-05-29 21:04:18 [INFO]: epoch 277: training loss 0.0488\n",
      "2025-05-29 21:04:18 [INFO]: epoch 278: training loss 0.0525\n",
      "2025-05-29 21:04:18 [INFO]: epoch 279: training loss 0.0521\n",
      "2025-05-29 21:04:18 [INFO]: epoch 280: training loss 0.0574\n",
      "2025-05-29 21:04:18 [INFO]: epoch 281: training loss 0.0577\n",
      "2025-05-29 21:04:18 [INFO]: epoch 282: training loss 0.0555\n",
      "2025-05-29 21:04:18 [INFO]: epoch 283: training loss 0.0556\n",
      "2025-05-29 21:04:18 [INFO]: epoch 284: training loss 0.0467\n",
      "2025-05-29 21:04:18 [INFO]: epoch 285: training loss 0.0508\n",
      "2025-05-29 21:04:18 [INFO]: epoch 286: training loss 0.0467\n",
      "2025-05-29 21:04:18 [INFO]: epoch 287: training loss 0.0456\n",
      "2025-05-29 21:04:18 [INFO]: epoch 288: training loss 0.0421\n",
      "2025-05-29 21:04:18 [INFO]: epoch 289: training loss 0.0457\n",
      "2025-05-29 21:04:18 [INFO]: epoch 290: training loss 0.0614\n",
      "2025-05-29 21:04:18 [INFO]: epoch 291: training loss 0.0439\n",
      "2025-05-29 21:04:18 [INFO]: epoch 292: training loss 0.0447\n",
      "2025-05-29 21:04:18 [INFO]: epoch 293: training loss 0.0517\n",
      "2025-05-29 21:04:18 [INFO]: epoch 294: training loss 0.0464\n",
      "2025-05-29 21:04:18 [INFO]: epoch 295: training loss 0.0530\n",
      "2025-05-29 21:04:18 [INFO]: epoch 296: training loss 0.0607\n",
      "2025-05-29 21:04:18 [INFO]: epoch 297: training loss 0.0470\n",
      "2025-05-29 21:04:18 [INFO]: epoch 298: training loss 0.0453\n",
      "2025-05-29 21:04:18 [INFO]: epoch 299: training loss 0.0679\n",
      "2025-05-29 21:04:18 [INFO]: epoch 300: training loss 0.0567\n",
      "2025-05-29 21:04:18 [INFO]: epoch 301: training loss 0.0430\n",
      "2025-05-29 21:04:18 [INFO]: epoch 302: training loss 0.0544\n",
      "2025-05-29 21:04:18 [INFO]: epoch 303: training loss 0.0573\n",
      "2025-05-29 21:04:18 [INFO]: epoch 304: training loss 0.0471\n",
      "2025-05-29 21:04:18 [INFO]: epoch 305: training loss 0.0624\n",
      "2025-05-29 21:04:18 [INFO]: epoch 306: training loss 0.0435\n",
      "2025-05-29 21:04:18 [INFO]: epoch 307: training loss 0.0422\n",
      "2025-05-29 21:04:18 [INFO]: epoch 308: training loss 0.0565\n",
      "2025-05-29 21:04:18 [INFO]: epoch 309: training loss 0.0444\n",
      "2025-05-29 21:04:18 [INFO]: epoch 310: training loss 0.0440\n",
      "2025-05-29 21:04:18 [INFO]: epoch 311: training loss 0.0456\n",
      "2025-05-29 21:04:18 [INFO]: epoch 312: training loss 0.0422\n",
      "2025-05-29 21:04:18 [INFO]: epoch 313: training loss 0.0426\n",
      "2025-05-29 21:04:18 [INFO]: epoch 314: training loss 0.0476\n",
      "2025-05-29 21:04:18 [INFO]: epoch 315: training loss 0.0444\n",
      "2025-05-29 21:04:18 [INFO]: epoch 316: training loss 0.0389\n",
      "2025-05-29 21:04:18 [INFO]: epoch 317: training loss 0.0455\n",
      "2025-05-29 21:04:18 [INFO]: epoch 318: training loss 0.0371\n",
      "2025-05-29 21:04:18 [INFO]: epoch 319: training loss 0.0395\n",
      "2025-05-29 21:04:18 [INFO]: epoch 320: training loss 0.0433\n",
      "2025-05-29 21:04:18 [INFO]: epoch 321: training loss 0.0387\n",
      "2025-05-29 21:04:18 [INFO]: epoch 322: training loss 0.0335\n",
      "2025-05-29 21:04:18 [INFO]: epoch 323: training loss 0.0415\n",
      "2025-05-29 21:04:19 [INFO]: epoch 324: training loss 0.0410\n",
      "2025-05-29 21:04:19 [INFO]: epoch 325: training loss 0.0377\n",
      "2025-05-29 21:04:19 [INFO]: epoch 326: training loss 0.0392\n",
      "2025-05-29 21:04:19 [INFO]: epoch 327: training loss 0.0427\n",
      "2025-05-29 21:04:19 [INFO]: epoch 328: training loss 0.0417\n",
      "2025-05-29 21:04:19 [INFO]: epoch 329: training loss 0.0450\n",
      "2025-05-29 21:04:19 [INFO]: epoch 330: training loss 0.0490\n",
      "2025-05-29 21:04:19 [INFO]: epoch 331: training loss 0.0426\n",
      "2025-05-29 21:04:19 [INFO]: epoch 332: training loss 0.0420\n",
      "2025-05-29 21:04:19 [INFO]: epoch 333: training loss 0.0418\n",
      "2025-05-29 21:04:19 [INFO]: epoch 334: training loss 0.0377\n",
      "2025-05-29 21:04:19 [INFO]: epoch 335: training loss 0.0409\n",
      "2025-05-29 21:04:19 [INFO]: epoch 336: training loss 0.0416\n",
      "2025-05-29 21:04:19 [INFO]: epoch 337: training loss 0.0442\n",
      "2025-05-29 21:04:19 [INFO]: epoch 338: training loss 0.0459\n",
      "2025-05-29 21:04:19 [INFO]: epoch 339: training loss 0.0380\n",
      "2025-05-29 21:04:19 [INFO]: epoch 340: training loss 0.0472\n",
      "2025-05-29 21:04:19 [INFO]: epoch 341: training loss 0.0496\n",
      "2025-05-29 21:04:19 [INFO]: epoch 342: training loss 0.0388\n",
      "2025-05-29 21:04:19 [INFO]: epoch 343: training loss 0.0422\n",
      "2025-05-29 21:04:19 [INFO]: epoch 344: training loss 0.0418\n",
      "2025-05-29 21:04:19 [INFO]: epoch 345: training loss 0.0356\n",
      "2025-05-29 21:04:19 [INFO]: epoch 346: training loss 0.0445\n",
      "2025-05-29 21:04:19 [INFO]: epoch 347: training loss 0.0497\n",
      "2025-05-29 21:04:19 [INFO]: epoch 348: training loss 0.0355\n",
      "2025-05-29 21:04:19 [INFO]: epoch 349: training loss 0.0521\n",
      "2025-05-29 21:04:19 [INFO]: epoch 350: training loss 0.0489\n",
      "2025-05-29 21:04:19 [INFO]: epoch 351: training loss 0.0383\n",
      "2025-05-29 21:04:19 [INFO]: epoch 352: training loss 0.0396\n",
      "2025-05-29 21:04:19 [INFO]: epoch 353: training loss 0.0519\n",
      "2025-05-29 21:04:19 [INFO]: epoch 354: training loss 0.0472\n",
      "2025-05-29 21:04:19 [INFO]: epoch 355: training loss 0.0371\n",
      "2025-05-29 21:04:19 [INFO]: epoch 356: training loss 0.0427\n",
      "2025-05-29 21:04:19 [INFO]: epoch 357: training loss 0.0383\n",
      "2025-05-29 21:04:19 [INFO]: epoch 358: training loss 0.0396\n",
      "2025-05-29 21:04:19 [INFO]: epoch 359: training loss 0.0456\n",
      "2025-05-29 21:04:19 [INFO]: epoch 360: training loss 0.0455\n",
      "2025-05-29 21:04:19 [INFO]: epoch 361: training loss 0.0372\n",
      "2025-05-29 21:04:19 [INFO]: epoch 362: training loss 0.0472\n",
      "2025-05-29 21:04:19 [INFO]: epoch 363: training loss 0.0398\n",
      "2025-05-29 21:04:19 [INFO]: epoch 364: training loss 0.0362\n",
      "2025-05-29 21:04:19 [INFO]: epoch 365: training loss 0.0526\n",
      "2025-05-29 21:04:19 [INFO]: epoch 366: training loss 0.0517\n",
      "2025-05-29 21:04:19 [INFO]: epoch 367: training loss 0.0361\n",
      "2025-05-29 21:04:19 [INFO]: epoch 368: training loss 0.0507\n",
      "2025-05-29 21:04:19 [INFO]: epoch 369: training loss 0.0536\n",
      "2025-05-29 21:04:19 [INFO]: epoch 370: training loss 0.0425\n",
      "2025-05-29 21:04:19 [INFO]: epoch 371: training loss 0.0329\n",
      "2025-05-29 21:04:19 [INFO]: epoch 372: training loss 0.0500\n",
      "2025-05-29 21:04:19 [INFO]: epoch 373: training loss 0.0457\n",
      "2025-05-29 21:04:19 [INFO]: epoch 374: training loss 0.0298\n",
      "2025-05-29 21:04:19 [INFO]: epoch 375: training loss 0.0385\n",
      "2025-05-29 21:04:19 [INFO]: epoch 376: training loss 0.0338\n",
      "2025-05-29 21:04:19 [INFO]: epoch 377: training loss 0.0259\n",
      "2025-05-29 21:04:19 [INFO]: epoch 378: training loss 0.0355\n",
      "2025-05-29 21:04:19 [INFO]: epoch 379: training loss 0.0326\n",
      "2025-05-29 21:04:19 [INFO]: epoch 380: training loss 0.0337\n",
      "2025-05-29 21:04:19 [INFO]: epoch 381: training loss 0.0332\n",
      "2025-05-29 21:04:19 [INFO]: epoch 382: training loss 0.0296\n",
      "2025-05-29 21:04:19 [INFO]: epoch 383: training loss 0.0320\n",
      "2025-05-29 21:04:19 [INFO]: epoch 384: training loss 0.0350\n",
      "2025-05-29 21:04:19 [INFO]: epoch 385: training loss 0.0260\n",
      "2025-05-29 21:04:19 [INFO]: epoch 386: training loss 0.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:19 [INFO]: epoch 387: training loss 0.0281\n",
      "2025-05-29 21:04:19 [INFO]: epoch 388: training loss 0.0322\n",
      "2025-05-29 21:04:19 [INFO]: epoch 389: training loss 0.0395\n",
      "2025-05-29 21:04:19 [INFO]: epoch 390: training loss 0.0262\n",
      "2025-05-29 21:04:19 [INFO]: epoch 391: training loss 0.0322\n",
      "2025-05-29 21:04:19 [INFO]: epoch 392: training loss 0.0313\n",
      "2025-05-29 21:04:19 [INFO]: epoch 393: training loss 0.0313\n",
      "2025-05-29 21:04:19 [INFO]: epoch 394: training loss 0.0316\n",
      "2025-05-29 21:04:19 [INFO]: epoch 395: training loss 0.0306\n",
      "2025-05-29 21:04:19 [INFO]: epoch 396: training loss 0.0247\n",
      "2025-05-29 21:04:19 [INFO]: epoch 397: training loss 0.0324\n",
      "2025-05-29 21:04:19 [INFO]: epoch 398: training loss 0.0371\n",
      "2025-05-29 21:04:19 [INFO]: epoch 399: training loss 0.0333\n",
      "2025-05-29 21:04:19 [INFO]: epoch 400: training loss 0.0298\n",
      "2025-05-29 21:04:19 [INFO]: epoch 401: training loss 0.0308\n",
      "2025-05-29 21:04:19 [INFO]: epoch 402: training loss 0.0344\n",
      "2025-05-29 21:04:19 [INFO]: epoch 403: training loss 0.0283\n",
      "2025-05-29 21:04:20 [INFO]: epoch 404: training loss 0.0358\n",
      "2025-05-29 21:04:20 [INFO]: epoch 405: training loss 0.0342\n",
      "2025-05-29 21:04:20 [INFO]: epoch 406: training loss 0.0296\n",
      "2025-05-29 21:04:20 [INFO]: epoch 407: training loss 0.0313\n",
      "2025-05-29 21:04:20 [INFO]: epoch 408: training loss 0.0334\n",
      "2025-05-29 21:04:20 [INFO]: epoch 409: training loss 0.0301\n",
      "2025-05-29 21:04:20 [INFO]: epoch 410: training loss 0.0286\n",
      "2025-05-29 21:04:20 [INFO]: epoch 411: training loss 0.0457\n",
      "2025-05-29 21:04:20 [INFO]: epoch 412: training loss 0.0337\n",
      "2025-05-29 21:04:20 [INFO]: epoch 413: training loss 0.0274\n",
      "2025-05-29 21:04:20 [INFO]: epoch 414: training loss 0.0301\n",
      "2025-05-29 21:04:20 [INFO]: epoch 415: training loss 0.0369\n",
      "2025-05-29 21:04:20 [INFO]: epoch 416: training loss 0.0338\n",
      "2025-05-29 21:04:20 [INFO]: epoch 417: training loss 0.0279\n",
      "2025-05-29 21:04:20 [INFO]: epoch 418: training loss 0.0426\n",
      "2025-05-29 21:04:20 [INFO]: epoch 419: training loss 0.0276\n",
      "2025-05-29 21:04:20 [INFO]: epoch 420: training loss 0.0258\n",
      "2025-05-29 21:04:20 [INFO]: epoch 421: training loss 0.0351\n",
      "2025-05-29 21:04:20 [INFO]: epoch 422: training loss 0.0360\n",
      "2025-05-29 21:04:20 [INFO]: epoch 423: training loss 0.0301\n",
      "2025-05-29 21:04:20 [INFO]: epoch 424: training loss 0.0404\n",
      "2025-05-29 21:04:20 [INFO]: epoch 425: training loss 0.0336\n",
      "2025-05-29 21:04:20 [INFO]: epoch 426: training loss 0.0265\n",
      "2025-05-29 21:04:20 [INFO]: epoch 427: training loss 0.0345\n",
      "2025-05-29 21:04:20 [INFO]: epoch 428: training loss 0.0363\n",
      "2025-05-29 21:04:20 [INFO]: epoch 429: training loss 0.0268\n",
      "2025-05-29 21:04:20 [INFO]: epoch 430: training loss 0.0404\n",
      "2025-05-29 21:04:20 [INFO]: epoch 431: training loss 0.0348\n",
      "2025-05-29 21:04:20 [INFO]: epoch 432: training loss 0.0275\n",
      "2025-05-29 21:04:20 [INFO]: epoch 433: training loss 0.0331\n",
      "2025-05-29 21:04:20 [INFO]: epoch 434: training loss 0.0329\n",
      "2025-05-29 21:04:20 [INFO]: epoch 435: training loss 0.0320\n",
      "2025-05-29 21:04:20 [INFO]: epoch 436: training loss 0.0346\n",
      "2025-05-29 21:04:20 [INFO]: epoch 437: training loss 0.0346\n",
      "2025-05-29 21:04:20 [INFO]: epoch 438: training loss 0.0279\n",
      "2025-05-29 21:04:20 [INFO]: epoch 439: training loss 0.0412\n",
      "2025-05-29 21:04:20 [INFO]: epoch 440: training loss 0.0425\n",
      "2025-05-29 21:04:20 [INFO]: epoch 441: training loss 0.0274\n",
      "2025-05-29 21:04:20 [INFO]: epoch 442: training loss 0.0357\n",
      "2025-05-29 21:04:20 [INFO]: epoch 443: training loss 0.0449\n",
      "2025-05-29 21:04:20 [INFO]: epoch 444: training loss 0.0377\n",
      "2025-05-29 21:04:20 [INFO]: epoch 445: training loss 0.0347\n",
      "2025-05-29 21:04:20 [INFO]: epoch 446: training loss 0.0407\n",
      "2025-05-29 21:04:20 [INFO]: epoch 447: training loss 0.0371\n",
      "2025-05-29 21:04:20 [INFO]: epoch 448: training loss 0.0333\n",
      "2025-05-29 21:04:20 [INFO]: epoch 449: training loss 0.0359\n",
      "2025-05-29 21:04:20 [INFO]: epoch 450: training loss 0.0361\n",
      "2025-05-29 21:04:20 [INFO]: epoch 451: training loss 0.0270\n",
      "2025-05-29 21:04:20 [INFO]: epoch 452: training loss 0.0321\n",
      "2025-05-29 21:04:20 [INFO]: epoch 453: training loss 0.0299\n",
      "2025-05-29 21:04:20 [INFO]: epoch 454: training loss 0.0328\n",
      "2025-05-29 21:04:20 [INFO]: epoch 455: training loss 0.0325\n",
      "2025-05-29 21:04:20 [INFO]: epoch 456: training loss 0.0289\n",
      "2025-05-29 21:04:20 [INFO]: epoch 457: training loss 0.0291\n",
      "2025-05-29 21:04:20 [INFO]: epoch 458: training loss 0.0276\n",
      "2025-05-29 21:04:20 [INFO]: epoch 459: training loss 0.0260\n",
      "2025-05-29 21:04:20 [INFO]: epoch 460: training loss 0.0207\n",
      "2025-05-29 21:04:20 [INFO]: epoch 461: training loss 0.0296\n",
      "2025-05-29 21:04:20 [INFO]: epoch 462: training loss 0.0298\n",
      "2025-05-29 21:04:20 [INFO]: epoch 463: training loss 0.0250\n",
      "2025-05-29 21:04:20 [INFO]: epoch 464: training loss 0.0251\n",
      "2025-05-29 21:04:20 [INFO]: epoch 465: training loss 0.0291\n",
      "2025-05-29 21:04:20 [INFO]: epoch 466: training loss 0.0273\n",
      "2025-05-29 21:04:20 [INFO]: epoch 467: training loss 0.0265\n",
      "2025-05-29 21:04:20 [INFO]: epoch 468: training loss 0.0272\n",
      "2025-05-29 21:04:20 [INFO]: epoch 469: training loss 0.0243\n",
      "2025-05-29 21:04:20 [INFO]: epoch 470: training loss 0.0258\n",
      "2025-05-29 21:04:20 [INFO]: epoch 471: training loss 0.0324\n",
      "2025-05-29 21:04:20 [INFO]: epoch 472: training loss 0.0257\n",
      "2025-05-29 21:04:20 [INFO]: epoch 473: training loss 0.0249\n",
      "2025-05-29 21:04:20 [INFO]: epoch 474: training loss 0.0319\n",
      "2025-05-29 21:04:20 [INFO]: epoch 475: training loss 0.0347\n",
      "2025-05-29 21:04:20 [INFO]: epoch 476: training loss 0.0255\n",
      "2025-05-29 21:04:20 [INFO]: epoch 477: training loss 0.0317\n",
      "2025-05-29 21:04:20 [INFO]: epoch 478: training loss 0.0367\n",
      "2025-05-29 21:04:20 [INFO]: epoch 479: training loss 0.0273\n",
      "2025-05-29 21:04:20 [INFO]: epoch 480: training loss 0.0318\n",
      "2025-05-29 21:04:20 [INFO]: epoch 481: training loss 0.0282\n",
      "2025-05-29 21:04:20 [INFO]: epoch 482: training loss 0.0231\n",
      "2025-05-29 21:04:21 [INFO]: epoch 483: training loss 0.0305\n",
      "2025-05-29 21:04:21 [INFO]: epoch 484: training loss 0.0284\n",
      "2025-05-29 21:04:21 [INFO]: epoch 485: training loss 0.0321\n",
      "2025-05-29 21:04:21 [INFO]: epoch 486: training loss 0.0260\n",
      "2025-05-29 21:04:21 [INFO]: epoch 487: training loss 0.0275\n",
      "2025-05-29 21:04:21 [INFO]: epoch 488: training loss 0.0379\n",
      "2025-05-29 21:04:21 [INFO]: epoch 489: training loss 0.0340\n",
      "2025-05-29 21:04:21 [INFO]: epoch 490: training loss 0.0262\n",
      "2025-05-29 21:04:21 [INFO]: epoch 491: training loss 0.0311\n",
      "2025-05-29 21:04:21 [INFO]: epoch 492: training loss 0.0280\n",
      "2025-05-29 21:04:21 [INFO]: epoch 493: training loss 0.0285\n",
      "2025-05-29 21:04:21 [INFO]: epoch 494: training loss 0.0300\n",
      "2025-05-29 21:04:21 [INFO]: epoch 495: training loss 0.0267\n",
      "2025-05-29 21:04:21 [INFO]: epoch 496: training loss 0.0312\n",
      "2025-05-29 21:04:21 [INFO]: epoch 497: training loss 0.0303\n",
      "2025-05-29 21:04:21 [INFO]: epoch 498: training loss 0.0206\n",
      "2025-05-29 21:04:21 [INFO]: epoch 499: training loss 0.0349\n",
      "2025-05-29 21:04:21 [INFO]: epoch 500: training loss 0.0372\n",
      "2025-05-29 21:04:21 [INFO]: epoch 501: training loss 0.0284\n",
      "2025-05-29 21:04:21 [INFO]: epoch 502: training loss 0.0258\n",
      "2025-05-29 21:04:21 [INFO]: epoch 503: training loss 0.0368\n",
      "2025-05-29 21:04:21 [INFO]: epoch 504: training loss 0.0310\n",
      "2025-05-29 21:04:21 [INFO]: epoch 505: training loss 0.0237\n",
      "2025-05-29 21:04:21 [INFO]: epoch 506: training loss 0.0334\n",
      "2025-05-29 21:04:21 [INFO]: epoch 507: training loss 0.0330\n",
      "2025-05-29 21:04:21 [INFO]: epoch 508: training loss 0.0287\n",
      "2025-05-29 21:04:21 [INFO]: epoch 509: training loss 0.0276\n",
      "2025-05-29 21:04:21 [INFO]: epoch 510: training loss 0.0295\n",
      "2025-05-29 21:04:21 [INFO]: epoch 511: training loss 0.0364\n",
      "2025-05-29 21:04:21 [INFO]: epoch 512: training loss 0.0229\n",
      "2025-05-29 21:04:21 [INFO]: epoch 513: training loss 0.0222\n",
      "2025-05-29 21:04:21 [INFO]: epoch 514: training loss 0.0258\n",
      "2025-05-29 21:04:21 [INFO]: epoch 515: training loss 0.0322\n",
      "2025-05-29 21:04:21 [INFO]: epoch 516: training loss 0.0265\n",
      "2025-05-29 21:04:21 [INFO]: epoch 517: training loss 0.0231\n",
      "2025-05-29 21:04:21 [INFO]: epoch 518: training loss 0.0234\n",
      "2025-05-29 21:04:21 [INFO]: epoch 519: training loss 0.0254\n",
      "2025-05-29 21:04:21 [INFO]: epoch 520: training loss 0.0241\n",
      "2025-05-29 21:04:21 [INFO]: epoch 521: training loss 0.0195\n",
      "2025-05-29 21:04:21 [INFO]: epoch 522: training loss 0.0279\n",
      "2025-05-29 21:04:21 [INFO]: epoch 523: training loss 0.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:21 [INFO]: epoch 524: training loss 0.0247\n",
      "2025-05-29 21:04:21 [INFO]: epoch 525: training loss 0.0216\n",
      "2025-05-29 21:04:21 [INFO]: epoch 526: training loss 0.0251\n",
      "2025-05-29 21:04:21 [INFO]: epoch 527: training loss 0.0272\n",
      "2025-05-29 21:04:21 [INFO]: epoch 528: training loss 0.0281\n",
      "2025-05-29 21:04:21 [INFO]: epoch 529: training loss 0.0258\n",
      "2025-05-29 21:04:21 [INFO]: epoch 530: training loss 0.0221\n",
      "2025-05-29 21:04:21 [INFO]: epoch 531: training loss 0.0261\n",
      "2025-05-29 21:04:21 [INFO]: epoch 532: training loss 0.0282\n",
      "2025-05-29 21:04:21 [INFO]: epoch 533: training loss 0.0277\n",
      "2025-05-29 21:04:21 [INFO]: epoch 534: training loss 0.0245\n",
      "2025-05-29 21:04:21 [INFO]: epoch 535: training loss 0.0290\n",
      "2025-05-29 21:04:21 [INFO]: epoch 536: training loss 0.0250\n",
      "2025-05-29 21:04:21 [INFO]: epoch 537: training loss 0.0226\n",
      "2025-05-29 21:04:21 [INFO]: epoch 538: training loss 0.0275\n",
      "2025-05-29 21:04:21 [INFO]: epoch 539: training loss 0.0238\n",
      "2025-05-29 21:04:21 [INFO]: epoch 540: training loss 0.0214\n",
      "2025-05-29 21:04:21 [INFO]: epoch 541: training loss 0.0302\n",
      "2025-05-29 21:04:21 [INFO]: epoch 542: training loss 0.0231\n",
      "2025-05-29 21:04:21 [INFO]: epoch 543: training loss 0.0261\n",
      "2025-05-29 21:04:21 [INFO]: epoch 544: training loss 0.0233\n",
      "2025-05-29 21:04:21 [INFO]: epoch 545: training loss 0.0198\n",
      "2025-05-29 21:04:21 [INFO]: epoch 546: training loss 0.0263\n",
      "2025-05-29 21:04:21 [INFO]: epoch 547: training loss 0.0268\n",
      "2025-05-29 21:04:21 [INFO]: epoch 548: training loss 0.0279\n",
      "2025-05-29 21:04:21 [INFO]: epoch 549: training loss 0.0227\n",
      "2025-05-29 21:04:21 [INFO]: epoch 550: training loss 0.0289\n",
      "2025-05-29 21:04:21 [INFO]: epoch 551: training loss 0.0220\n",
      "2025-05-29 21:04:21 [INFO]: epoch 552: training loss 0.0282\n",
      "2025-05-29 21:04:21 [INFO]: epoch 553: training loss 0.0224\n",
      "2025-05-29 21:04:21 [INFO]: epoch 554: training loss 0.0218\n",
      "2025-05-29 21:04:21 [INFO]: epoch 555: training loss 0.0240\n",
      "2025-05-29 21:04:21 [INFO]: epoch 556: training loss 0.0260\n",
      "2025-05-29 21:04:21 [INFO]: epoch 557: training loss 0.0263\n",
      "2025-05-29 21:04:21 [INFO]: epoch 558: training loss 0.0256\n",
      "2025-05-29 21:04:21 [INFO]: epoch 559: training loss 0.0232\n",
      "2025-05-29 21:04:21 [INFO]: epoch 560: training loss 0.0270\n",
      "2025-05-29 21:04:21 [INFO]: epoch 561: training loss 0.0246\n",
      "2025-05-29 21:04:22 [INFO]: epoch 562: training loss 0.0222\n",
      "2025-05-29 21:04:22 [INFO]: epoch 563: training loss 0.0297\n",
      "2025-05-29 21:04:22 [INFO]: epoch 564: training loss 0.0312\n",
      "2025-05-29 21:04:22 [INFO]: epoch 565: training loss 0.0239\n",
      "2025-05-29 21:04:22 [INFO]: epoch 566: training loss 0.0339\n",
      "2025-05-29 21:04:22 [INFO]: epoch 567: training loss 0.0321\n",
      "2025-05-29 21:04:22 [INFO]: epoch 568: training loss 0.0261\n",
      "2025-05-29 21:04:22 [INFO]: epoch 569: training loss 0.0259\n",
      "2025-05-29 21:04:22 [INFO]: epoch 570: training loss 0.0248\n",
      "2025-05-29 21:04:22 [INFO]: epoch 571: training loss 0.0245\n",
      "2025-05-29 21:04:22 [INFO]: epoch 572: training loss 0.0223\n",
      "2025-05-29 21:04:22 [INFO]: epoch 573: training loss 0.0213\n",
      "2025-05-29 21:04:22 [INFO]: epoch 574: training loss 0.0221\n",
      "2025-05-29 21:04:22 [INFO]: epoch 575: training loss 0.0239\n",
      "2025-05-29 21:04:22 [INFO]: epoch 576: training loss 0.0246\n",
      "2025-05-29 21:04:22 [INFO]: epoch 577: training loss 0.0219\n",
      "2025-05-29 21:04:22 [INFO]: epoch 578: training loss 0.0268\n",
      "2025-05-29 21:04:22 [INFO]: epoch 579: training loss 0.0251\n",
      "2025-05-29 21:04:22 [INFO]: epoch 580: training loss 0.0252\n",
      "2025-05-29 21:04:22 [INFO]: epoch 581: training loss 0.0281\n",
      "2025-05-29 21:04:22 [INFO]: epoch 582: training loss 0.0205\n",
      "2025-05-29 21:04:22 [INFO]: epoch 583: training loss 0.0214\n",
      "2025-05-29 21:04:22 [INFO]: epoch 584: training loss 0.0246\n",
      "2025-05-29 21:04:22 [INFO]: epoch 585: training loss 0.0206\n",
      "2025-05-29 21:04:22 [INFO]: epoch 586: training loss 0.0226\n",
      "2025-05-29 21:04:22 [INFO]: epoch 587: training loss 0.0282\n",
      "2025-05-29 21:04:22 [INFO]: epoch 588: training loss 0.0222\n",
      "2025-05-29 21:04:22 [INFO]: epoch 589: training loss 0.0224\n",
      "2025-05-29 21:04:22 [INFO]: epoch 590: training loss 0.0246\n",
      "2025-05-29 21:04:22 [INFO]: epoch 591: training loss 0.0245\n",
      "2025-05-29 21:04:22 [INFO]: epoch 592: training loss 0.0230\n",
      "2025-05-29 21:04:22 [INFO]: epoch 593: training loss 0.0254\n",
      "2025-05-29 21:04:22 [INFO]: epoch 594: training loss 0.0244\n",
      "2025-05-29 21:04:22 [INFO]: epoch 595: training loss 0.0236\n",
      "2025-05-29 21:04:22 [INFO]: epoch 596: training loss 0.0238\n",
      "2025-05-29 21:04:22 [INFO]: epoch 597: training loss 0.0229\n",
      "2025-05-29 21:04:22 [INFO]: epoch 598: training loss 0.0270\n",
      "2025-05-29 21:04:22 [INFO]: epoch 599: training loss 0.0253\n",
      "2025-05-29 21:04:22 [INFO]: Finished training.\n",
      "2025-05-29 21:04:22 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 60%|██████████████████████████████████████████████████▍                                 | 3/5 [00:22<00:15,  7.58s/it]2025-05-29 21:04:22 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:04:22 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:04:22 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:04:22 [INFO]: epoch 0: training loss 1.1082\n",
      "2025-05-29 21:04:22 [INFO]: epoch 1: training loss 0.7749\n",
      "2025-05-29 21:04:22 [INFO]: epoch 2: training loss 0.6546\n",
      "2025-05-29 21:04:22 [INFO]: epoch 3: training loss 0.6168\n",
      "2025-05-29 21:04:22 [INFO]: epoch 4: training loss 0.5842\n",
      "2025-05-29 21:04:22 [INFO]: epoch 5: training loss 0.4861\n",
      "2025-05-29 21:04:22 [INFO]: epoch 6: training loss 0.4354\n",
      "2025-05-29 21:04:22 [INFO]: epoch 7: training loss 0.4536\n",
      "2025-05-29 21:04:22 [INFO]: epoch 8: training loss 0.4518\n",
      "2025-05-29 21:04:22 [INFO]: epoch 9: training loss 0.4257\n",
      "2025-05-29 21:04:22 [INFO]: epoch 10: training loss 0.4512\n",
      "2025-05-29 21:04:22 [INFO]: epoch 11: training loss 0.4412\n",
      "2025-05-29 21:04:22 [INFO]: epoch 12: training loss 0.4424\n",
      "2025-05-29 21:04:22 [INFO]: epoch 13: training loss 0.3873\n",
      "2025-05-29 21:04:22 [INFO]: epoch 14: training loss 0.3478\n",
      "2025-05-29 21:04:22 [INFO]: epoch 15: training loss 0.4000\n",
      "2025-05-29 21:04:22 [INFO]: epoch 16: training loss 0.3789\n",
      "2025-05-29 21:04:22 [INFO]: epoch 17: training loss 0.3513\n",
      "2025-05-29 21:04:22 [INFO]: epoch 18: training loss 0.3832\n",
      "2025-05-29 21:04:22 [INFO]: epoch 19: training loss 0.3696\n",
      "2025-05-29 21:04:22 [INFO]: epoch 20: training loss 0.3632\n",
      "2025-05-29 21:04:22 [INFO]: epoch 21: training loss 0.3571\n",
      "2025-05-29 21:04:22 [INFO]: epoch 22: training loss 0.3692\n",
      "2025-05-29 21:04:22 [INFO]: epoch 23: training loss 0.3355\n",
      "2025-05-29 21:04:22 [INFO]: epoch 24: training loss 0.3546\n",
      "2025-05-29 21:04:22 [INFO]: epoch 25: training loss 0.3289\n",
      "2025-05-29 21:04:22 [INFO]: epoch 26: training loss 0.3395\n",
      "2025-05-29 21:04:22 [INFO]: epoch 27: training loss 0.3217\n",
      "2025-05-29 21:04:22 [INFO]: epoch 28: training loss 0.3315\n",
      "2025-05-29 21:04:22 [INFO]: epoch 29: training loss 0.3371\n",
      "2025-05-29 21:04:22 [INFO]: epoch 30: training loss 0.3640\n",
      "2025-05-29 21:04:22 [INFO]: epoch 31: training loss 0.3484\n",
      "2025-05-29 21:04:22 [INFO]: epoch 32: training loss 0.3261\n",
      "2025-05-29 21:04:22 [INFO]: epoch 33: training loss 0.3218\n",
      "2025-05-29 21:04:22 [INFO]: epoch 34: training loss 0.3321\n",
      "2025-05-29 21:04:23 [INFO]: epoch 35: training loss 0.2956\n",
      "2025-05-29 21:04:23 [INFO]: epoch 36: training loss 0.3215\n",
      "2025-05-29 21:04:23 [INFO]: epoch 37: training loss 0.3159\n",
      "2025-05-29 21:04:23 [INFO]: epoch 38: training loss 0.3254\n",
      "2025-05-29 21:04:23 [INFO]: epoch 39: training loss 0.3130\n",
      "2025-05-29 21:04:23 [INFO]: epoch 40: training loss 0.3404\n",
      "2025-05-29 21:04:23 [INFO]: epoch 41: training loss 0.3104\n",
      "2025-05-29 21:04:23 [INFO]: epoch 42: training loss 0.3115\n",
      "2025-05-29 21:04:23 [INFO]: epoch 43: training loss 0.3082\n",
      "2025-05-29 21:04:23 [INFO]: epoch 44: training loss 0.3166\n",
      "2025-05-29 21:04:23 [INFO]: epoch 45: training loss 0.2935\n",
      "2025-05-29 21:04:23 [INFO]: epoch 46: training loss 0.2934\n",
      "2025-05-29 21:04:23 [INFO]: epoch 47: training loss 0.2966\n",
      "2025-05-29 21:04:23 [INFO]: epoch 48: training loss 0.2948\n",
      "2025-05-29 21:04:23 [INFO]: epoch 49: training loss 0.2834\n",
      "2025-05-29 21:04:23 [INFO]: epoch 50: training loss 0.2702\n",
      "2025-05-29 21:04:23 [INFO]: epoch 51: training loss 0.2667\n",
      "2025-05-29 21:04:23 [INFO]: epoch 52: training loss 0.2803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:23 [INFO]: epoch 53: training loss 0.2716\n",
      "2025-05-29 21:04:23 [INFO]: epoch 54: training loss 0.2719\n",
      "2025-05-29 21:04:23 [INFO]: epoch 55: training loss 0.2928\n",
      "2025-05-29 21:04:23 [INFO]: epoch 56: training loss 0.2659\n",
      "2025-05-29 21:04:23 [INFO]: epoch 57: training loss 0.2506\n",
      "2025-05-29 21:04:23 [INFO]: epoch 58: training loss 0.2746\n",
      "2025-05-29 21:04:23 [INFO]: epoch 59: training loss 0.2842\n",
      "2025-05-29 21:04:23 [INFO]: epoch 60: training loss 0.2810\n",
      "2025-05-29 21:04:23 [INFO]: epoch 61: training loss 0.2629\n",
      "2025-05-29 21:04:23 [INFO]: epoch 62: training loss 0.2506\n",
      "2025-05-29 21:04:23 [INFO]: epoch 63: training loss 0.2565\n",
      "2025-05-29 21:04:23 [INFO]: epoch 64: training loss 0.2611\n",
      "2025-05-29 21:04:23 [INFO]: epoch 65: training loss 0.2727\n",
      "2025-05-29 21:04:23 [INFO]: epoch 66: training loss 0.2590\n",
      "2025-05-29 21:04:23 [INFO]: epoch 67: training loss 0.2395\n",
      "2025-05-29 21:04:23 [INFO]: epoch 68: training loss 0.2292\n",
      "2025-05-29 21:04:23 [INFO]: epoch 69: training loss 0.2450\n",
      "2025-05-29 21:04:23 [INFO]: epoch 70: training loss 0.2254\n",
      "2025-05-29 21:04:23 [INFO]: epoch 71: training loss 0.2355\n",
      "2025-05-29 21:04:23 [INFO]: epoch 72: training loss 0.2477\n",
      "2025-05-29 21:04:23 [INFO]: epoch 73: training loss 0.2437\n",
      "2025-05-29 21:04:23 [INFO]: epoch 74: training loss 0.2565\n",
      "2025-05-29 21:04:23 [INFO]: epoch 75: training loss 0.2358\n",
      "2025-05-29 21:04:23 [INFO]: epoch 76: training loss 0.2177\n",
      "2025-05-29 21:04:23 [INFO]: epoch 77: training loss 0.2488\n",
      "2025-05-29 21:04:23 [INFO]: epoch 78: training loss 0.2239\n",
      "2025-05-29 21:04:23 [INFO]: epoch 79: training loss 0.2397\n",
      "2025-05-29 21:04:23 [INFO]: epoch 80: training loss 0.2406\n",
      "2025-05-29 21:04:23 [INFO]: epoch 81: training loss 0.2364\n",
      "2025-05-29 21:04:23 [INFO]: epoch 82: training loss 0.2137\n",
      "2025-05-29 21:04:23 [INFO]: epoch 83: training loss 0.2135\n",
      "2025-05-29 21:04:23 [INFO]: epoch 84: training loss 0.2076\n",
      "2025-05-29 21:04:23 [INFO]: epoch 85: training loss 0.2233\n",
      "2025-05-29 21:04:23 [INFO]: epoch 86: training loss 0.2188\n",
      "2025-05-29 21:04:23 [INFO]: epoch 87: training loss 0.2132\n",
      "2025-05-29 21:04:23 [INFO]: epoch 88: training loss 0.2294\n",
      "2025-05-29 21:04:23 [INFO]: epoch 89: training loss 0.2267\n",
      "2025-05-29 21:04:23 [INFO]: epoch 90: training loss 0.2244\n",
      "2025-05-29 21:04:23 [INFO]: epoch 91: training loss 0.2122\n",
      "2025-05-29 21:04:23 [INFO]: epoch 92: training loss 0.1801\n",
      "2025-05-29 21:04:23 [INFO]: epoch 93: training loss 0.1827\n",
      "2025-05-29 21:04:23 [INFO]: epoch 94: training loss 0.2001\n",
      "2025-05-29 21:04:23 [INFO]: epoch 95: training loss 0.2099\n",
      "2025-05-29 21:04:23 [INFO]: epoch 96: training loss 0.2047\n",
      "2025-05-29 21:04:23 [INFO]: epoch 97: training loss 0.1926\n",
      "2025-05-29 21:04:23 [INFO]: epoch 98: training loss 0.2084\n",
      "2025-05-29 21:04:23 [INFO]: epoch 99: training loss 0.2034\n",
      "2025-05-29 21:04:23 [INFO]: epoch 100: training loss 0.2202\n",
      "2025-05-29 21:04:23 [INFO]: epoch 101: training loss 0.2157\n",
      "2025-05-29 21:04:23 [INFO]: epoch 102: training loss 0.2069\n",
      "2025-05-29 21:04:23 [INFO]: epoch 103: training loss 0.2153\n",
      "2025-05-29 21:04:23 [INFO]: epoch 104: training loss 0.2130\n",
      "2025-05-29 21:04:23 [INFO]: epoch 105: training loss 0.2088\n",
      "2025-05-29 21:04:23 [INFO]: epoch 106: training loss 0.1941\n",
      "2025-05-29 21:04:23 [INFO]: epoch 107: training loss 0.1949\n",
      "2025-05-29 21:04:23 [INFO]: epoch 108: training loss 0.2032\n",
      "2025-05-29 21:04:23 [INFO]: epoch 109: training loss 0.1906\n",
      "2025-05-29 21:04:23 [INFO]: epoch 110: training loss 0.1801\n",
      "2025-05-29 21:04:23 [INFO]: epoch 111: training loss 0.1935\n",
      "2025-05-29 21:04:23 [INFO]: epoch 112: training loss 0.1934\n",
      "2025-05-29 21:04:23 [INFO]: epoch 113: training loss 0.1952\n",
      "2025-05-29 21:04:23 [INFO]: epoch 114: training loss 0.2129\n",
      "2025-05-29 21:04:24 [INFO]: epoch 115: training loss 0.1904\n",
      "2025-05-29 21:04:24 [INFO]: epoch 116: training loss 0.2010\n",
      "2025-05-29 21:04:24 [INFO]: epoch 117: training loss 0.2087\n",
      "2025-05-29 21:04:24 [INFO]: epoch 118: training loss 0.2073\n",
      "2025-05-29 21:04:24 [INFO]: epoch 119: training loss 0.1650\n",
      "2025-05-29 21:04:24 [INFO]: epoch 120: training loss 0.1714\n",
      "2025-05-29 21:04:24 [INFO]: epoch 121: training loss 0.1927\n",
      "2025-05-29 21:04:24 [INFO]: epoch 122: training loss 0.1807\n",
      "2025-05-29 21:04:24 [INFO]: epoch 123: training loss 0.1777\n",
      "2025-05-29 21:04:24 [INFO]: epoch 124: training loss 0.1599\n",
      "2025-05-29 21:04:24 [INFO]: epoch 125: training loss 0.1693\n",
      "2025-05-29 21:04:24 [INFO]: epoch 126: training loss 0.1769\n",
      "2025-05-29 21:04:24 [INFO]: epoch 127: training loss 0.1641\n",
      "2025-05-29 21:04:24 [INFO]: epoch 128: training loss 0.1814\n",
      "2025-05-29 21:04:24 [INFO]: epoch 129: training loss 0.1668\n",
      "2025-05-29 21:04:24 [INFO]: epoch 130: training loss 0.1514\n",
      "2025-05-29 21:04:24 [INFO]: epoch 131: training loss 0.1662\n",
      "2025-05-29 21:04:24 [INFO]: epoch 132: training loss 0.1568\n",
      "2025-05-29 21:04:24 [INFO]: epoch 133: training loss 0.1549\n",
      "2025-05-29 21:04:24 [INFO]: epoch 134: training loss 0.1653\n",
      "2025-05-29 21:04:24 [INFO]: epoch 135: training loss 0.1564\n",
      "2025-05-29 21:04:24 [INFO]: epoch 136: training loss 0.1578\n",
      "2025-05-29 21:04:24 [INFO]: epoch 137: training loss 0.1676\n",
      "2025-05-29 21:04:24 [INFO]: epoch 138: training loss 0.1749\n",
      "2025-05-29 21:04:24 [INFO]: epoch 139: training loss 0.1576\n",
      "2025-05-29 21:04:24 [INFO]: epoch 140: training loss 0.1508\n",
      "2025-05-29 21:04:24 [INFO]: epoch 141: training loss 0.1385\n",
      "2025-05-29 21:04:24 [INFO]: epoch 142: training loss 0.1457\n",
      "2025-05-29 21:04:24 [INFO]: epoch 143: training loss 0.1707\n",
      "2025-05-29 21:04:24 [INFO]: epoch 144: training loss 0.1668\n",
      "2025-05-29 21:04:24 [INFO]: epoch 145: training loss 0.1736\n",
      "2025-05-29 21:04:24 [INFO]: epoch 146: training loss 0.1558\n",
      "2025-05-29 21:04:24 [INFO]: epoch 147: training loss 0.1626\n",
      "2025-05-29 21:04:24 [INFO]: epoch 148: training loss 0.1522\n",
      "2025-05-29 21:04:24 [INFO]: epoch 149: training loss 0.1611\n",
      "2025-05-29 21:04:24 [INFO]: epoch 150: training loss 0.1736\n",
      "2025-05-29 21:04:24 [INFO]: epoch 151: training loss 0.1389\n",
      "2025-05-29 21:04:24 [INFO]: epoch 152: training loss 0.1362\n",
      "2025-05-29 21:04:24 [INFO]: epoch 153: training loss 0.1566\n",
      "2025-05-29 21:04:24 [INFO]: epoch 154: training loss 0.1575\n",
      "2025-05-29 21:04:24 [INFO]: epoch 155: training loss 0.1581\n",
      "2025-05-29 21:04:24 [INFO]: epoch 156: training loss 0.1540\n",
      "2025-05-29 21:04:24 [INFO]: epoch 157: training loss 0.1470\n",
      "2025-05-29 21:04:24 [INFO]: epoch 158: training loss 0.1507\n",
      "2025-05-29 21:04:24 [INFO]: epoch 159: training loss 0.1538\n",
      "2025-05-29 21:04:24 [INFO]: epoch 160: training loss 0.1742\n",
      "2025-05-29 21:04:24 [INFO]: epoch 161: training loss 0.1655\n",
      "2025-05-29 21:04:24 [INFO]: epoch 162: training loss 0.1673\n",
      "2025-05-29 21:04:24 [INFO]: epoch 163: training loss 0.1599\n",
      "2025-05-29 21:04:24 [INFO]: epoch 164: training loss 0.1482\n",
      "2025-05-29 21:04:24 [INFO]: epoch 165: training loss 0.1391\n",
      "2025-05-29 21:04:24 [INFO]: epoch 166: training loss 0.1634\n",
      "2025-05-29 21:04:24 [INFO]: epoch 167: training loss 0.1436\n",
      "2025-05-29 21:04:24 [INFO]: epoch 168: training loss 0.1449\n",
      "2025-05-29 21:04:24 [INFO]: epoch 169: training loss 0.1508\n",
      "2025-05-29 21:04:24 [INFO]: epoch 170: training loss 0.1363\n",
      "2025-05-29 21:04:24 [INFO]: epoch 171: training loss 0.1426\n",
      "2025-05-29 21:04:24 [INFO]: epoch 172: training loss 0.1422\n",
      "2025-05-29 21:04:24 [INFO]: epoch 173: training loss 0.1434\n",
      "2025-05-29 21:04:24 [INFO]: epoch 174: training loss 0.1482\n",
      "2025-05-29 21:04:24 [INFO]: epoch 175: training loss 0.1270\n",
      "2025-05-29 21:04:24 [INFO]: epoch 176: training loss 0.1335\n",
      "2025-05-29 21:04:24 [INFO]: epoch 177: training loss 0.1368\n",
      "2025-05-29 21:04:24 [INFO]: epoch 178: training loss 0.1269\n",
      "2025-05-29 21:04:24 [INFO]: epoch 179: training loss 0.1110\n",
      "2025-05-29 21:04:24 [INFO]: epoch 180: training loss 0.1332\n",
      "2025-05-29 21:04:24 [INFO]: epoch 181: training loss 0.1288\n",
      "2025-05-29 21:04:24 [INFO]: epoch 182: training loss 0.1332\n",
      "2025-05-29 21:04:24 [INFO]: epoch 183: training loss 0.1229\n",
      "2025-05-29 21:04:24 [INFO]: epoch 184: training loss 0.1295\n",
      "2025-05-29 21:04:24 [INFO]: epoch 185: training loss 0.1218\n",
      "2025-05-29 21:04:24 [INFO]: epoch 186: training loss 0.1314\n",
      "2025-05-29 21:04:24 [INFO]: epoch 187: training loss 0.1189\n",
      "2025-05-29 21:04:24 [INFO]: epoch 188: training loss 0.1290\n",
      "2025-05-29 21:04:24 [INFO]: epoch 189: training loss 0.1328\n",
      "2025-05-29 21:04:24 [INFO]: epoch 190: training loss 0.1291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:24 [INFO]: epoch 191: training loss 0.1454\n",
      "2025-05-29 21:04:24 [INFO]: epoch 192: training loss 0.1292\n",
      "2025-05-29 21:04:24 [INFO]: epoch 193: training loss 0.1264\n",
      "2025-05-29 21:04:24 [INFO]: epoch 194: training loss 0.1281\n",
      "2025-05-29 21:04:24 [INFO]: epoch 195: training loss 0.1303\n",
      "2025-05-29 21:04:25 [INFO]: epoch 196: training loss 0.1242\n",
      "2025-05-29 21:04:25 [INFO]: epoch 197: training loss 0.1122\n",
      "2025-05-29 21:04:25 [INFO]: epoch 198: training loss 0.1199\n",
      "2025-05-29 21:04:25 [INFO]: epoch 199: training loss 0.1295\n",
      "2025-05-29 21:04:25 [INFO]: epoch 200: training loss 0.1229\n",
      "2025-05-29 21:04:25 [INFO]: epoch 201: training loss 0.1185\n",
      "2025-05-29 21:04:25 [INFO]: epoch 202: training loss 0.1176\n",
      "2025-05-29 21:04:25 [INFO]: epoch 203: training loss 0.1303\n",
      "2025-05-29 21:04:25 [INFO]: epoch 204: training loss 0.1259\n",
      "2025-05-29 21:04:25 [INFO]: epoch 205: training loss 0.1160\n",
      "2025-05-29 21:04:25 [INFO]: epoch 206: training loss 0.1228\n",
      "2025-05-29 21:04:25 [INFO]: epoch 207: training loss 0.0993\n",
      "2025-05-29 21:04:25 [INFO]: epoch 208: training loss 0.1147\n",
      "2025-05-29 21:04:25 [INFO]: epoch 209: training loss 0.1091\n",
      "2025-05-29 21:04:25 [INFO]: epoch 210: training loss 0.1207\n",
      "2025-05-29 21:04:25 [INFO]: epoch 211: training loss 0.1114\n",
      "2025-05-29 21:04:25 [INFO]: epoch 212: training loss 0.1172\n",
      "2025-05-29 21:04:25 [INFO]: epoch 213: training loss 0.1197\n",
      "2025-05-29 21:04:25 [INFO]: epoch 214: training loss 0.1134\n",
      "2025-05-29 21:04:25 [INFO]: epoch 215: training loss 0.1148\n",
      "2025-05-29 21:04:25 [INFO]: epoch 216: training loss 0.1041\n",
      "2025-05-29 21:04:25 [INFO]: epoch 217: training loss 0.1213\n",
      "2025-05-29 21:04:25 [INFO]: epoch 218: training loss 0.0979\n",
      "2025-05-29 21:04:25 [INFO]: epoch 219: training loss 0.1122\n",
      "2025-05-29 21:04:25 [INFO]: epoch 220: training loss 0.1041\n",
      "2025-05-29 21:04:25 [INFO]: epoch 221: training loss 0.1177\n",
      "2025-05-29 21:04:25 [INFO]: epoch 222: training loss 0.1033\n",
      "2025-05-29 21:04:25 [INFO]: epoch 223: training loss 0.1027\n",
      "2025-05-29 21:04:25 [INFO]: epoch 224: training loss 0.1105\n",
      "2025-05-29 21:04:25 [INFO]: epoch 225: training loss 0.1124\n",
      "2025-05-29 21:04:25 [INFO]: epoch 226: training loss 0.1054\n",
      "2025-05-29 21:04:25 [INFO]: epoch 227: training loss 0.1137\n",
      "2025-05-29 21:04:25 [INFO]: epoch 228: training loss 0.1128\n",
      "2025-05-29 21:04:25 [INFO]: epoch 229: training loss 0.1004\n",
      "2025-05-29 21:04:25 [INFO]: epoch 230: training loss 0.0930\n",
      "2025-05-29 21:04:25 [INFO]: epoch 231: training loss 0.1124\n",
      "2025-05-29 21:04:25 [INFO]: epoch 232: training loss 0.1151\n",
      "2025-05-29 21:04:25 [INFO]: epoch 233: training loss 0.1141\n",
      "2025-05-29 21:04:25 [INFO]: epoch 234: training loss 0.0949\n",
      "2025-05-29 21:04:25 [INFO]: epoch 235: training loss 0.1008\n",
      "2025-05-29 21:04:25 [INFO]: epoch 236: training loss 0.0849\n",
      "2025-05-29 21:04:25 [INFO]: epoch 237: training loss 0.0876\n",
      "2025-05-29 21:04:25 [INFO]: epoch 238: training loss 0.0936\n",
      "2025-05-29 21:04:25 [INFO]: epoch 239: training loss 0.0921\n",
      "2025-05-29 21:04:25 [INFO]: epoch 240: training loss 0.0857\n",
      "2025-05-29 21:04:25 [INFO]: epoch 241: training loss 0.0932\n",
      "2025-05-29 21:04:25 [INFO]: epoch 242: training loss 0.0986\n",
      "2025-05-29 21:04:25 [INFO]: epoch 243: training loss 0.0917\n",
      "2025-05-29 21:04:25 [INFO]: epoch 244: training loss 0.0934\n",
      "2025-05-29 21:04:25 [INFO]: epoch 245: training loss 0.0872\n",
      "2025-05-29 21:04:25 [INFO]: epoch 246: training loss 0.0911\n",
      "2025-05-29 21:04:25 [INFO]: epoch 247: training loss 0.0933\n",
      "2025-05-29 21:04:25 [INFO]: epoch 248: training loss 0.0914\n",
      "2025-05-29 21:04:25 [INFO]: epoch 249: training loss 0.0844\n",
      "2025-05-29 21:04:25 [INFO]: epoch 250: training loss 0.0911\n",
      "2025-05-29 21:04:25 [INFO]: epoch 251: training loss 0.0803\n",
      "2025-05-29 21:04:25 [INFO]: epoch 252: training loss 0.0901\n",
      "2025-05-29 21:04:25 [INFO]: epoch 253: training loss 0.0950\n",
      "2025-05-29 21:04:25 [INFO]: epoch 254: training loss 0.0836\n",
      "2025-05-29 21:04:25 [INFO]: epoch 255: training loss 0.0868\n",
      "2025-05-29 21:04:25 [INFO]: epoch 256: training loss 0.0837\n",
      "2025-05-29 21:04:25 [INFO]: epoch 257: training loss 0.0971\n",
      "2025-05-29 21:04:25 [INFO]: epoch 258: training loss 0.0927\n",
      "2025-05-29 21:04:25 [INFO]: epoch 259: training loss 0.0841\n",
      "2025-05-29 21:04:25 [INFO]: epoch 260: training loss 0.0831\n",
      "2025-05-29 21:04:25 [INFO]: epoch 261: training loss 0.0907\n",
      "2025-05-29 21:04:25 [INFO]: epoch 262: training loss 0.0946\n",
      "2025-05-29 21:04:25 [INFO]: epoch 263: training loss 0.0908\n",
      "2025-05-29 21:04:25 [INFO]: epoch 264: training loss 0.0885\n",
      "2025-05-29 21:04:25 [INFO]: epoch 265: training loss 0.0944\n",
      "2025-05-29 21:04:25 [INFO]: epoch 266: training loss 0.0865\n",
      "2025-05-29 21:04:25 [INFO]: epoch 267: training loss 0.0816\n",
      "2025-05-29 21:04:25 [INFO]: epoch 268: training loss 0.0837\n",
      "2025-05-29 21:04:25 [INFO]: epoch 269: training loss 0.0792\n",
      "2025-05-29 21:04:25 [INFO]: epoch 270: training loss 0.0862\n",
      "2025-05-29 21:04:25 [INFO]: epoch 271: training loss 0.0879\n",
      "2025-05-29 21:04:25 [INFO]: epoch 272: training loss 0.0820\n",
      "2025-05-29 21:04:25 [INFO]: epoch 273: training loss 0.0858\n",
      "2025-05-29 21:04:25 [INFO]: epoch 274: training loss 0.0854\n",
      "2025-05-29 21:04:25 [INFO]: epoch 275: training loss 0.0814\n",
      "2025-05-29 21:04:26 [INFO]: epoch 276: training loss 0.0810\n",
      "2025-05-29 21:04:26 [INFO]: epoch 277: training loss 0.0795\n",
      "2025-05-29 21:04:26 [INFO]: epoch 278: training loss 0.0807\n",
      "2025-05-29 21:04:26 [INFO]: epoch 279: training loss 0.0852\n",
      "2025-05-29 21:04:26 [INFO]: epoch 280: training loss 0.0863\n",
      "2025-05-29 21:04:26 [INFO]: epoch 281: training loss 0.0766\n",
      "2025-05-29 21:04:26 [INFO]: epoch 282: training loss 0.0701\n",
      "2025-05-29 21:04:26 [INFO]: epoch 283: training loss 0.0791\n",
      "2025-05-29 21:04:26 [INFO]: epoch 284: training loss 0.0854\n",
      "2025-05-29 21:04:26 [INFO]: epoch 285: training loss 0.0725\n",
      "2025-05-29 21:04:26 [INFO]: epoch 286: training loss 0.0805\n",
      "2025-05-29 21:04:26 [INFO]: epoch 287: training loss 0.0863\n",
      "2025-05-29 21:04:26 [INFO]: epoch 288: training loss 0.0738\n",
      "2025-05-29 21:04:26 [INFO]: epoch 289: training loss 0.0818\n",
      "2025-05-29 21:04:26 [INFO]: epoch 290: training loss 0.0808\n",
      "2025-05-29 21:04:26 [INFO]: epoch 291: training loss 0.0728\n",
      "2025-05-29 21:04:26 [INFO]: epoch 292: training loss 0.0767\n",
      "2025-05-29 21:04:26 [INFO]: epoch 293: training loss 0.0709\n",
      "2025-05-29 21:04:26 [INFO]: epoch 294: training loss 0.0754\n",
      "2025-05-29 21:04:26 [INFO]: epoch 295: training loss 0.0892\n",
      "2025-05-29 21:04:26 [INFO]: epoch 296: training loss 0.0704\n",
      "2025-05-29 21:04:26 [INFO]: epoch 297: training loss 0.0804\n",
      "2025-05-29 21:04:26 [INFO]: epoch 298: training loss 0.0786\n",
      "2025-05-29 21:04:26 [INFO]: epoch 299: training loss 0.0677\n",
      "2025-05-29 21:04:26 [INFO]: epoch 300: training loss 0.0666\n",
      "2025-05-29 21:04:26 [INFO]: epoch 301: training loss 0.0721\n",
      "2025-05-29 21:04:26 [INFO]: epoch 302: training loss 0.0779\n",
      "2025-05-29 21:04:26 [INFO]: epoch 303: training loss 0.0707\n",
      "2025-05-29 21:04:26 [INFO]: epoch 304: training loss 0.0681\n",
      "2025-05-29 21:04:26 [INFO]: epoch 305: training loss 0.0785\n",
      "2025-05-29 21:04:26 [INFO]: epoch 306: training loss 0.0755\n",
      "2025-05-29 21:04:26 [INFO]: epoch 307: training loss 0.0770\n",
      "2025-05-29 21:04:26 [INFO]: epoch 308: training loss 0.0662\n",
      "2025-05-29 21:04:26 [INFO]: epoch 309: training loss 0.0661\n",
      "2025-05-29 21:04:26 [INFO]: epoch 310: training loss 0.0666\n",
      "2025-05-29 21:04:26 [INFO]: epoch 311: training loss 0.0701\n",
      "2025-05-29 21:04:26 [INFO]: epoch 312: training loss 0.0806\n",
      "2025-05-29 21:04:26 [INFO]: epoch 313: training loss 0.0786\n",
      "2025-05-29 21:04:26 [INFO]: epoch 314: training loss 0.0640\n",
      "2025-05-29 21:04:26 [INFO]: epoch 315: training loss 0.0670\n",
      "2025-05-29 21:04:26 [INFO]: epoch 316: training loss 0.0708\n",
      "2025-05-29 21:04:26 [INFO]: epoch 317: training loss 0.0757\n",
      "2025-05-29 21:04:26 [INFO]: epoch 318: training loss 0.0793\n",
      "2025-05-29 21:04:26 [INFO]: epoch 319: training loss 0.0803\n",
      "2025-05-29 21:04:26 [INFO]: epoch 320: training loss 0.0593\n",
      "2025-05-29 21:04:26 [INFO]: epoch 321: training loss 0.0746\n",
      "2025-05-29 21:04:26 [INFO]: epoch 322: training loss 0.0787\n",
      "2025-05-29 21:04:26 [INFO]: epoch 323: training loss 0.0743\n",
      "2025-05-29 21:04:26 [INFO]: epoch 324: training loss 0.0783\n",
      "2025-05-29 21:04:26 [INFO]: epoch 325: training loss 0.0510\n",
      "2025-05-29 21:04:26 [INFO]: epoch 326: training loss 0.0621\n",
      "2025-05-29 21:04:26 [INFO]: epoch 327: training loss 0.0695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:26 [INFO]: epoch 328: training loss 0.0549\n",
      "2025-05-29 21:04:26 [INFO]: epoch 329: training loss 0.0635\n",
      "2025-05-29 21:04:26 [INFO]: epoch 330: training loss 0.0580\n",
      "2025-05-29 21:04:26 [INFO]: epoch 331: training loss 0.0526\n",
      "2025-05-29 21:04:26 [INFO]: epoch 332: training loss 0.0661\n",
      "2025-05-29 21:04:26 [INFO]: epoch 333: training loss 0.0547\n",
      "2025-05-29 21:04:26 [INFO]: epoch 334: training loss 0.0632\n",
      "2025-05-29 21:04:26 [INFO]: epoch 335: training loss 0.0600\n",
      "2025-05-29 21:04:26 [INFO]: epoch 336: training loss 0.0516\n",
      "2025-05-29 21:04:26 [INFO]: epoch 337: training loss 0.0488\n",
      "2025-05-29 21:04:26 [INFO]: epoch 338: training loss 0.0566\n",
      "2025-05-29 21:04:26 [INFO]: epoch 339: training loss 0.0551\n",
      "2025-05-29 21:04:26 [INFO]: epoch 340: training loss 0.0548\n",
      "2025-05-29 21:04:26 [INFO]: epoch 341: training loss 0.0537\n",
      "2025-05-29 21:04:26 [INFO]: epoch 342: training loss 0.0548\n",
      "2025-05-29 21:04:26 [INFO]: epoch 343: training loss 0.0636\n",
      "2025-05-29 21:04:26 [INFO]: epoch 344: training loss 0.0485\n",
      "2025-05-29 21:04:26 [INFO]: epoch 345: training loss 0.0552\n",
      "2025-05-29 21:04:26 [INFO]: epoch 346: training loss 0.0555\n",
      "2025-05-29 21:04:26 [INFO]: epoch 347: training loss 0.0549\n",
      "2025-05-29 21:04:26 [INFO]: epoch 348: training loss 0.0525\n",
      "2025-05-29 21:04:26 [INFO]: epoch 349: training loss 0.0595\n",
      "2025-05-29 21:04:26 [INFO]: epoch 350: training loss 0.0653\n",
      "2025-05-29 21:04:26 [INFO]: epoch 351: training loss 0.0536\n",
      "2025-05-29 21:04:26 [INFO]: epoch 352: training loss 0.0526\n",
      "2025-05-29 21:04:26 [INFO]: epoch 353: training loss 0.0494\n",
      "2025-05-29 21:04:26 [INFO]: epoch 354: training loss 0.0498\n",
      "2025-05-29 21:04:26 [INFO]: epoch 355: training loss 0.0525\n",
      "2025-05-29 21:04:27 [INFO]: epoch 356: training loss 0.0500\n",
      "2025-05-29 21:04:27 [INFO]: epoch 357: training loss 0.0513\n",
      "2025-05-29 21:04:27 [INFO]: epoch 358: training loss 0.0556\n",
      "2025-05-29 21:04:27 [INFO]: epoch 359: training loss 0.0521\n",
      "2025-05-29 21:04:27 [INFO]: epoch 360: training loss 0.0543\n",
      "2025-05-29 21:04:27 [INFO]: epoch 361: training loss 0.0542\n",
      "2025-05-29 21:04:27 [INFO]: epoch 362: training loss 0.0422\n",
      "2025-05-29 21:04:27 [INFO]: epoch 363: training loss 0.0665\n",
      "2025-05-29 21:04:27 [INFO]: epoch 364: training loss 0.0503\n",
      "2025-05-29 21:04:27 [INFO]: epoch 365: training loss 0.0474\n",
      "2025-05-29 21:04:27 [INFO]: epoch 366: training loss 0.0476\n",
      "2025-05-29 21:04:27 [INFO]: epoch 367: training loss 0.0455\n",
      "2025-05-29 21:04:27 [INFO]: epoch 368: training loss 0.0425\n",
      "2025-05-29 21:04:27 [INFO]: epoch 369: training loss 0.0581\n",
      "2025-05-29 21:04:27 [INFO]: epoch 370: training loss 0.0587\n",
      "2025-05-29 21:04:27 [INFO]: epoch 371: training loss 0.0486\n",
      "2025-05-29 21:04:27 [INFO]: epoch 372: training loss 0.0460\n",
      "2025-05-29 21:04:27 [INFO]: epoch 373: training loss 0.0537\n",
      "2025-05-29 21:04:27 [INFO]: epoch 374: training loss 0.0409\n",
      "2025-05-29 21:04:27 [INFO]: epoch 375: training loss 0.0410\n",
      "2025-05-29 21:04:27 [INFO]: epoch 376: training loss 0.0400\n",
      "2025-05-29 21:04:27 [INFO]: epoch 377: training loss 0.0492\n",
      "2025-05-29 21:04:27 [INFO]: epoch 378: training loss 0.0616\n",
      "2025-05-29 21:04:27 [INFO]: epoch 379: training loss 0.0552\n",
      "2025-05-29 21:04:27 [INFO]: epoch 380: training loss 0.0458\n",
      "2025-05-29 21:04:27 [INFO]: epoch 381: training loss 0.0548\n",
      "2025-05-29 21:04:27 [INFO]: epoch 382: training loss 0.0513\n",
      "2025-05-29 21:04:27 [INFO]: epoch 383: training loss 0.0507\n",
      "2025-05-29 21:04:27 [INFO]: epoch 384: training loss 0.0492\n",
      "2025-05-29 21:04:27 [INFO]: epoch 385: training loss 0.0547\n",
      "2025-05-29 21:04:27 [INFO]: epoch 386: training loss 0.0533\n",
      "2025-05-29 21:04:27 [INFO]: epoch 387: training loss 0.0427\n",
      "2025-05-29 21:04:27 [INFO]: epoch 388: training loss 0.0472\n",
      "2025-05-29 21:04:27 [INFO]: epoch 389: training loss 0.0484\n",
      "2025-05-29 21:04:27 [INFO]: epoch 390: training loss 0.0445\n",
      "2025-05-29 21:04:27 [INFO]: epoch 391: training loss 0.0463\n",
      "2025-05-29 21:04:27 [INFO]: epoch 392: training loss 0.0457\n",
      "2025-05-29 21:04:27 [INFO]: epoch 393: training loss 0.0513\n",
      "2025-05-29 21:04:27 [INFO]: epoch 394: training loss 0.0483\n",
      "2025-05-29 21:04:27 [INFO]: epoch 395: training loss 0.0469\n",
      "2025-05-29 21:04:27 [INFO]: epoch 396: training loss 0.0485\n",
      "2025-05-29 21:04:27 [INFO]: epoch 397: training loss 0.0433\n",
      "2025-05-29 21:04:27 [INFO]: epoch 398: training loss 0.0445\n",
      "2025-05-29 21:04:27 [INFO]: epoch 399: training loss 0.0423\n",
      "2025-05-29 21:04:27 [INFO]: epoch 400: training loss 0.0462\n",
      "2025-05-29 21:04:27 [INFO]: epoch 401: training loss 0.0422\n",
      "2025-05-29 21:04:27 [INFO]: epoch 402: training loss 0.0414\n",
      "2025-05-29 21:04:27 [INFO]: epoch 403: training loss 0.0429\n",
      "2025-05-29 21:04:27 [INFO]: epoch 404: training loss 0.0389\n",
      "2025-05-29 21:04:27 [INFO]: epoch 405: training loss 0.0479\n",
      "2025-05-29 21:04:27 [INFO]: epoch 406: training loss 0.0401\n",
      "2025-05-29 21:04:27 [INFO]: epoch 407: training loss 0.0366\n",
      "2025-05-29 21:04:27 [INFO]: epoch 408: training loss 0.0340\n",
      "2025-05-29 21:04:27 [INFO]: epoch 409: training loss 0.0389\n",
      "2025-05-29 21:04:27 [INFO]: epoch 410: training loss 0.0349\n",
      "2025-05-29 21:04:27 [INFO]: epoch 411: training loss 0.0335\n",
      "2025-05-29 21:04:27 [INFO]: epoch 412: training loss 0.0334\n",
      "2025-05-29 21:04:27 [INFO]: epoch 413: training loss 0.0320\n",
      "2025-05-29 21:04:27 [INFO]: epoch 414: training loss 0.0315\n",
      "2025-05-29 21:04:27 [INFO]: epoch 415: training loss 0.0283\n",
      "2025-05-29 21:04:27 [INFO]: epoch 416: training loss 0.0334\n",
      "2025-05-29 21:04:27 [INFO]: epoch 417: training loss 0.0336\n",
      "2025-05-29 21:04:27 [INFO]: epoch 418: training loss 0.0313\n",
      "2025-05-29 21:04:27 [INFO]: epoch 419: training loss 0.0357\n",
      "2025-05-29 21:04:27 [INFO]: epoch 420: training loss 0.0361\n",
      "2025-05-29 21:04:27 [INFO]: epoch 421: training loss 0.0314\n",
      "2025-05-29 21:04:27 [INFO]: epoch 422: training loss 0.0316\n",
      "2025-05-29 21:04:27 [INFO]: epoch 423: training loss 0.0343\n",
      "2025-05-29 21:04:27 [INFO]: epoch 424: training loss 0.0273\n",
      "2025-05-29 21:04:27 [INFO]: epoch 425: training loss 0.0314\n",
      "2025-05-29 21:04:27 [INFO]: epoch 426: training loss 0.0363\n",
      "2025-05-29 21:04:27 [INFO]: epoch 427: training loss 0.0301\n",
      "2025-05-29 21:04:27 [INFO]: epoch 428: training loss 0.0277\n",
      "2025-05-29 21:04:27 [INFO]: epoch 429: training loss 0.0361\n",
      "2025-05-29 21:04:27 [INFO]: epoch 430: training loss 0.0408\n",
      "2025-05-29 21:04:27 [INFO]: epoch 431: training loss 0.0281\n",
      "2025-05-29 21:04:27 [INFO]: epoch 432: training loss 0.0401\n",
      "2025-05-29 21:04:27 [INFO]: epoch 433: training loss 0.0383\n",
      "2025-05-29 21:04:27 [INFO]: epoch 434: training loss 0.0377\n",
      "2025-05-29 21:04:27 [INFO]: epoch 435: training loss 0.0316\n",
      "2025-05-29 21:04:28 [INFO]: epoch 436: training loss 0.0342\n",
      "2025-05-29 21:04:28 [INFO]: epoch 437: training loss 0.0314\n",
      "2025-05-29 21:04:28 [INFO]: epoch 438: training loss 0.0478\n",
      "2025-05-29 21:04:28 [INFO]: epoch 439: training loss 0.0313\n",
      "2025-05-29 21:04:28 [INFO]: epoch 440: training loss 0.0308\n",
      "2025-05-29 21:04:28 [INFO]: epoch 441: training loss 0.0450\n",
      "2025-05-29 21:04:28 [INFO]: epoch 442: training loss 0.0412\n",
      "2025-05-29 21:04:28 [INFO]: epoch 443: training loss 0.0352\n",
      "2025-05-29 21:04:28 [INFO]: epoch 444: training loss 0.0375\n",
      "2025-05-29 21:04:28 [INFO]: epoch 445: training loss 0.0283\n",
      "2025-05-29 21:04:28 [INFO]: epoch 446: training loss 0.0368\n",
      "2025-05-29 21:04:28 [INFO]: epoch 447: training loss 0.0385\n",
      "2025-05-29 21:04:28 [INFO]: epoch 448: training loss 0.0284\n",
      "2025-05-29 21:04:28 [INFO]: epoch 449: training loss 0.0347\n",
      "2025-05-29 21:04:28 [INFO]: epoch 450: training loss 0.0369\n",
      "2025-05-29 21:04:28 [INFO]: epoch 451: training loss 0.0285\n",
      "2025-05-29 21:04:28 [INFO]: epoch 452: training loss 0.0279\n",
      "2025-05-29 21:04:28 [INFO]: epoch 453: training loss 0.0324\n",
      "2025-05-29 21:04:28 [INFO]: epoch 454: training loss 0.0305\n",
      "2025-05-29 21:04:28 [INFO]: epoch 455: training loss 0.0449\n",
      "2025-05-29 21:04:28 [INFO]: epoch 456: training loss 0.0260\n",
      "2025-05-29 21:04:28 [INFO]: epoch 457: training loss 0.0313\n",
      "2025-05-29 21:04:28 [INFO]: epoch 458: training loss 0.0344\n",
      "2025-05-29 21:04:28 [INFO]: epoch 459: training loss 0.0256\n",
      "2025-05-29 21:04:28 [INFO]: epoch 460: training loss 0.0288\n",
      "2025-05-29 21:04:28 [INFO]: epoch 461: training loss 0.0365\n",
      "2025-05-29 21:04:28 [INFO]: epoch 462: training loss 0.0313\n",
      "2025-05-29 21:04:28 [INFO]: epoch 463: training loss 0.0311\n",
      "2025-05-29 21:04:28 [INFO]: epoch 464: training loss 0.0283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:28 [INFO]: epoch 465: training loss 0.0340\n",
      "2025-05-29 21:04:28 [INFO]: epoch 466: training loss 0.0315\n",
      "2025-05-29 21:04:28 [INFO]: epoch 467: training loss 0.0315\n",
      "2025-05-29 21:04:28 [INFO]: epoch 468: training loss 0.0299\n",
      "2025-05-29 21:04:28 [INFO]: epoch 469: training loss 0.0265\n",
      "2025-05-29 21:04:28 [INFO]: epoch 470: training loss 0.0339\n",
      "2025-05-29 21:04:28 [INFO]: epoch 471: training loss 0.0370\n",
      "2025-05-29 21:04:28 [INFO]: epoch 472: training loss 0.0270\n",
      "2025-05-29 21:04:28 [INFO]: epoch 473: training loss 0.0314\n",
      "2025-05-29 21:04:28 [INFO]: epoch 474: training loss 0.0317\n",
      "2025-05-29 21:04:28 [INFO]: epoch 475: training loss 0.0342\n",
      "2025-05-29 21:04:28 [INFO]: epoch 476: training loss 0.0291\n",
      "2025-05-29 21:04:28 [INFO]: epoch 477: training loss 0.0326\n",
      "2025-05-29 21:04:28 [INFO]: epoch 478: training loss 0.0335\n",
      "2025-05-29 21:04:28 [INFO]: epoch 479: training loss 0.0292\n",
      "2025-05-29 21:04:28 [INFO]: epoch 480: training loss 0.0354\n",
      "2025-05-29 21:04:28 [INFO]: epoch 481: training loss 0.0273\n",
      "2025-05-29 21:04:28 [INFO]: epoch 482: training loss 0.0401\n",
      "2025-05-29 21:04:28 [INFO]: epoch 483: training loss 0.0345\n",
      "2025-05-29 21:04:28 [INFO]: epoch 484: training loss 0.0290\n",
      "2025-05-29 21:04:28 [INFO]: epoch 485: training loss 0.0350\n",
      "2025-05-29 21:04:28 [INFO]: epoch 486: training loss 0.0347\n",
      "2025-05-29 21:04:28 [INFO]: epoch 487: training loss 0.0292\n",
      "2025-05-29 21:04:28 [INFO]: epoch 488: training loss 0.0287\n",
      "2025-05-29 21:04:28 [INFO]: epoch 489: training loss 0.0319\n",
      "2025-05-29 21:04:28 [INFO]: epoch 490: training loss 0.0307\n",
      "2025-05-29 21:04:28 [INFO]: epoch 491: training loss 0.0296\n",
      "2025-05-29 21:04:28 [INFO]: epoch 492: training loss 0.0284\n",
      "2025-05-29 21:04:28 [INFO]: epoch 493: training loss 0.0323\n",
      "2025-05-29 21:04:28 [INFO]: epoch 494: training loss 0.0299\n",
      "2025-05-29 21:04:28 [INFO]: epoch 495: training loss 0.0246\n",
      "2025-05-29 21:04:28 [INFO]: epoch 496: training loss 0.0229\n",
      "2025-05-29 21:04:28 [INFO]: epoch 497: training loss 0.0227\n",
      "2025-05-29 21:04:28 [INFO]: epoch 498: training loss 0.0292\n",
      "2025-05-29 21:04:28 [INFO]: epoch 499: training loss 0.0309\n",
      "2025-05-29 21:04:28 [INFO]: epoch 500: training loss 0.0302\n",
      "2025-05-29 21:04:28 [INFO]: epoch 501: training loss 0.0270\n",
      "2025-05-29 21:04:28 [INFO]: epoch 502: training loss 0.0307\n",
      "2025-05-29 21:04:28 [INFO]: epoch 503: training loss 0.0253\n",
      "2025-05-29 21:04:28 [INFO]: epoch 504: training loss 0.0300\n",
      "2025-05-29 21:04:28 [INFO]: epoch 505: training loss 0.0329\n",
      "2025-05-29 21:04:28 [INFO]: epoch 506: training loss 0.0262\n",
      "2025-05-29 21:04:28 [INFO]: epoch 507: training loss 0.0332\n",
      "2025-05-29 21:04:28 [INFO]: epoch 508: training loss 0.0296\n",
      "2025-05-29 21:04:28 [INFO]: epoch 509: training loss 0.0309\n",
      "2025-05-29 21:04:28 [INFO]: epoch 510: training loss 0.0236\n",
      "2025-05-29 21:04:28 [INFO]: epoch 511: training loss 0.0287\n",
      "2025-05-29 21:04:28 [INFO]: epoch 512: training loss 0.0282\n",
      "2025-05-29 21:04:28 [INFO]: epoch 513: training loss 0.0296\n",
      "2025-05-29 21:04:28 [INFO]: epoch 514: training loss 0.0288\n",
      "2025-05-29 21:04:28 [INFO]: epoch 515: training loss 0.0284\n",
      "2025-05-29 21:04:29 [INFO]: epoch 516: training loss 0.0298\n",
      "2025-05-29 21:04:29 [INFO]: epoch 517: training loss 0.0286\n",
      "2025-05-29 21:04:29 [INFO]: epoch 518: training loss 0.0284\n",
      "2025-05-29 21:04:29 [INFO]: epoch 519: training loss 0.0287\n",
      "2025-05-29 21:04:29 [INFO]: epoch 520: training loss 0.0285\n",
      "2025-05-29 21:04:29 [INFO]: epoch 521: training loss 0.0255\n",
      "2025-05-29 21:04:29 [INFO]: epoch 522: training loss 0.0338\n",
      "2025-05-29 21:04:29 [INFO]: epoch 523: training loss 0.0307\n",
      "2025-05-29 21:04:29 [INFO]: epoch 524: training loss 0.0267\n",
      "2025-05-29 21:04:29 [INFO]: epoch 525: training loss 0.0285\n",
      "2025-05-29 21:04:29 [INFO]: epoch 526: training loss 0.0238\n",
      "2025-05-29 21:04:29 [INFO]: epoch 527: training loss 0.0254\n",
      "2025-05-29 21:04:29 [INFO]: epoch 528: training loss 0.0414\n",
      "2025-05-29 21:04:29 [INFO]: epoch 529: training loss 0.0255\n",
      "2025-05-29 21:04:29 [INFO]: epoch 530: training loss 0.0286\n",
      "2025-05-29 21:04:29 [INFO]: epoch 531: training loss 0.0382\n",
      "2025-05-29 21:04:29 [INFO]: epoch 532: training loss 0.0330\n",
      "2025-05-29 21:04:29 [INFO]: epoch 533: training loss 0.0257\n",
      "2025-05-29 21:04:29 [INFO]: epoch 534: training loss 0.0325\n",
      "2025-05-29 21:04:29 [INFO]: epoch 535: training loss 0.0300\n",
      "2025-05-29 21:04:29 [INFO]: epoch 536: training loss 0.0284\n",
      "2025-05-29 21:04:29 [INFO]: epoch 537: training loss 0.0398\n",
      "2025-05-29 21:04:29 [INFO]: epoch 538: training loss 0.0296\n",
      "2025-05-29 21:04:29 [INFO]: epoch 539: training loss 0.0284\n",
      "2025-05-29 21:04:29 [INFO]: epoch 540: training loss 0.0355\n",
      "2025-05-29 21:04:29 [INFO]: epoch 541: training loss 0.0307\n",
      "2025-05-29 21:04:29 [INFO]: epoch 542: training loss 0.0347\n",
      "2025-05-29 21:04:29 [INFO]: epoch 543: training loss 0.0222\n",
      "2025-05-29 21:04:29 [INFO]: epoch 544: training loss 0.0238\n",
      "2025-05-29 21:04:29 [INFO]: epoch 545: training loss 0.0247\n",
      "2025-05-29 21:04:29 [INFO]: epoch 546: training loss 0.0277\n",
      "2025-05-29 21:04:29 [INFO]: epoch 547: training loss 0.0271\n",
      "2025-05-29 21:04:29 [INFO]: epoch 548: training loss 0.0253\n",
      "2025-05-29 21:04:29 [INFO]: epoch 549: training loss 0.0237\n",
      "2025-05-29 21:04:29 [INFO]: epoch 550: training loss 0.0268\n",
      "2025-05-29 21:04:29 [INFO]: epoch 551: training loss 0.0307\n",
      "2025-05-29 21:04:29 [INFO]: epoch 552: training loss 0.0269\n",
      "2025-05-29 21:04:29 [INFO]: epoch 553: training loss 0.0243\n",
      "2025-05-29 21:04:29 [INFO]: epoch 554: training loss 0.0336\n",
      "2025-05-29 21:04:29 [INFO]: epoch 555: training loss 0.0338\n",
      "2025-05-29 21:04:29 [INFO]: epoch 556: training loss 0.0291\n",
      "2025-05-29 21:04:29 [INFO]: epoch 557: training loss 0.0273\n",
      "2025-05-29 21:04:29 [INFO]: epoch 558: training loss 0.0270\n",
      "2025-05-29 21:04:29 [INFO]: epoch 559: training loss 0.0278\n",
      "2025-05-29 21:04:29 [INFO]: epoch 560: training loss 0.0291\n",
      "2025-05-29 21:04:29 [INFO]: epoch 561: training loss 0.0252\n",
      "2025-05-29 21:04:29 [INFO]: epoch 562: training loss 0.0324\n",
      "2025-05-29 21:04:29 [INFO]: epoch 563: training loss 0.0278\n",
      "2025-05-29 21:04:29 [INFO]: epoch 564: training loss 0.0265\n",
      "2025-05-29 21:04:29 [INFO]: epoch 565: training loss 0.0295\n",
      "2025-05-29 21:04:29 [INFO]: epoch 566: training loss 0.0302\n",
      "2025-05-29 21:04:29 [INFO]: epoch 567: training loss 0.0295\n",
      "2025-05-29 21:04:29 [INFO]: epoch 568: training loss 0.0371\n",
      "2025-05-29 21:04:29 [INFO]: epoch 569: training loss 0.0311\n",
      "2025-05-29 21:04:29 [INFO]: epoch 570: training loss 0.0318\n",
      "2025-05-29 21:04:29 [INFO]: epoch 571: training loss 0.0411\n",
      "2025-05-29 21:04:29 [INFO]: epoch 572: training loss 0.0256\n",
      "2025-05-29 21:04:29 [INFO]: epoch 573: training loss 0.0239\n",
      "2025-05-29 21:04:29 [INFO]: epoch 574: training loss 0.0316\n",
      "2025-05-29 21:04:29 [INFO]: epoch 575: training loss 0.0248\n",
      "2025-05-29 21:04:29 [INFO]: epoch 576: training loss 0.0248\n",
      "2025-05-29 21:04:29 [INFO]: epoch 577: training loss 0.0286\n",
      "2025-05-29 21:04:29 [INFO]: epoch 578: training loss 0.0245\n",
      "2025-05-29 21:04:29 [INFO]: epoch 579: training loss 0.0281\n",
      "2025-05-29 21:04:29 [INFO]: epoch 580: training loss 0.0317\n",
      "2025-05-29 21:04:29 [INFO]: epoch 581: training loss 0.0236\n",
      "2025-05-29 21:04:29 [INFO]: epoch 582: training loss 0.0278\n",
      "2025-05-29 21:04:29 [INFO]: epoch 583: training loss 0.0298\n",
      "2025-05-29 21:04:29 [INFO]: epoch 584: training loss 0.0234\n",
      "2025-05-29 21:04:29 [INFO]: epoch 585: training loss 0.0346\n",
      "2025-05-29 21:04:29 [INFO]: epoch 586: training loss 0.0303\n",
      "2025-05-29 21:04:29 [INFO]: epoch 587: training loss 0.0243\n",
      "2025-05-29 21:04:29 [INFO]: epoch 588: training loss 0.0231\n",
      "2025-05-29 21:04:29 [INFO]: epoch 589: training loss 0.0250\n",
      "2025-05-29 21:04:29 [INFO]: epoch 590: training loss 0.0274\n",
      "2025-05-29 21:04:29 [INFO]: epoch 591: training loss 0.0318\n",
      "2025-05-29 21:04:29 [INFO]: epoch 592: training loss 0.0215\n",
      "2025-05-29 21:04:29 [INFO]: epoch 593: training loss 0.0255\n",
      "2025-05-29 21:04:29 [INFO]: epoch 594: training loss 0.0280\n",
      "2025-05-29 21:04:30 [INFO]: epoch 595: training loss 0.0219\n",
      "2025-05-29 21:04:30 [INFO]: epoch 596: training loss 0.0209\n",
      "2025-05-29 21:04:30 [INFO]: epoch 597: training loss 0.0249\n",
      "2025-05-29 21:04:30 [INFO]: epoch 598: training loss 0.0230\n",
      "2025-05-29 21:04:30 [INFO]: epoch 599: training loss 0.0208\n",
      "2025-05-29 21:04:30 [INFO]: Finished training.\n",
      "2025-05-29 21:04:30 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████▏                | 4/5 [00:30<00:07,  7.59s/it]2025-05-29 21:04:30 [INFO]: No given device, using default device: cuda\n",
      "2025-05-29 21:04:30 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-05-29 21:04:30 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-05-29 21:04:30 [INFO]: epoch 0: training loss 1.2801\n",
      "2025-05-29 21:04:30 [INFO]: epoch 1: training loss 0.7032\n",
      "2025-05-29 21:04:30 [INFO]: epoch 2: training loss 0.6927\n",
      "2025-05-29 21:04:30 [INFO]: epoch 3: training loss 0.6916\n",
      "2025-05-29 21:04:30 [INFO]: epoch 4: training loss 0.6000\n",
      "2025-05-29 21:04:30 [INFO]: epoch 5: training loss 0.6480\n",
      "2025-05-29 21:04:30 [INFO]: epoch 6: training loss 0.6373\n",
      "2025-05-29 21:04:30 [INFO]: epoch 7: training loss 0.6182\n",
      "2025-05-29 21:04:30 [INFO]: epoch 8: training loss 0.5938\n",
      "2025-05-29 21:04:30 [INFO]: epoch 9: training loss 0.5668\n",
      "2025-05-29 21:04:30 [INFO]: epoch 10: training loss 0.5504\n",
      "2025-05-29 21:04:30 [INFO]: epoch 11: training loss 0.5090\n",
      "2025-05-29 21:04:30 [INFO]: epoch 12: training loss 0.5336\n",
      "2025-05-29 21:04:30 [INFO]: epoch 13: training loss 0.6360\n",
      "2025-05-29 21:04:30 [INFO]: epoch 14: training loss 0.5600\n",
      "2025-05-29 21:04:30 [INFO]: epoch 15: training loss 0.5078\n",
      "2025-05-29 21:04:30 [INFO]: epoch 16: training loss 0.4918\n",
      "2025-05-29 21:04:30 [INFO]: epoch 17: training loss 0.5170\n",
      "2025-05-29 21:04:30 [INFO]: epoch 18: training loss 0.5375\n",
      "2025-05-29 21:04:30 [INFO]: epoch 19: training loss 0.5151\n",
      "2025-05-29 21:04:30 [INFO]: epoch 20: training loss 0.4925\n",
      "2025-05-29 21:04:30 [INFO]: epoch 21: training loss 0.4468\n",
      "2025-05-29 21:04:30 [INFO]: epoch 22: training loss 0.4493\n",
      "2025-05-29 21:04:30 [INFO]: epoch 23: training loss 0.4738\n",
      "2025-05-29 21:04:30 [INFO]: epoch 24: training loss 0.4417\n",
      "2025-05-29 21:04:30 [INFO]: epoch 25: training loss 0.4784\n",
      "2025-05-29 21:04:30 [INFO]: epoch 26: training loss 0.5056\n",
      "2025-05-29 21:04:30 [INFO]: epoch 27: training loss 0.4842\n",
      "2025-05-29 21:04:30 [INFO]: epoch 28: training loss 0.4520\n",
      "2025-05-29 21:04:30 [INFO]: epoch 29: training loss 0.4431\n",
      "2025-05-29 21:04:30 [INFO]: epoch 30: training loss 0.4573\n",
      "2025-05-29 21:04:30 [INFO]: epoch 31: training loss 0.4882\n",
      "2025-05-29 21:04:30 [INFO]: epoch 32: training loss 0.4462\n",
      "2025-05-29 21:04:30 [INFO]: epoch 33: training loss 0.4272\n",
      "2025-05-29 21:04:30 [INFO]: epoch 34: training loss 0.4381\n",
      "2025-05-29 21:04:30 [INFO]: epoch 35: training loss 0.4364\n",
      "2025-05-29 21:04:30 [INFO]: epoch 36: training loss 0.4174\n",
      "2025-05-29 21:04:30 [INFO]: epoch 37: training loss 0.4230\n",
      "2025-05-29 21:04:30 [INFO]: epoch 38: training loss 0.4197\n",
      "2025-05-29 21:04:30 [INFO]: epoch 39: training loss 0.3990\n",
      "2025-05-29 21:04:30 [INFO]: epoch 40: training loss 0.3945\n",
      "2025-05-29 21:04:30 [INFO]: epoch 41: training loss 0.4336\n",
      "2025-05-29 21:04:30 [INFO]: epoch 42: training loss 0.4305\n",
      "2025-05-29 21:04:30 [INFO]: epoch 43: training loss 0.3938\n",
      "2025-05-29 21:04:30 [INFO]: epoch 44: training loss 0.4216\n",
      "2025-05-29 21:04:30 [INFO]: epoch 45: training loss 0.3927\n",
      "2025-05-29 21:04:30 [INFO]: epoch 46: training loss 0.3968\n",
      "2025-05-29 21:04:30 [INFO]: epoch 47: training loss 0.3801\n",
      "2025-05-29 21:04:30 [INFO]: epoch 48: training loss 0.3839\n",
      "2025-05-29 21:04:30 [INFO]: epoch 49: training loss 0.3685\n",
      "2025-05-29 21:04:30 [INFO]: epoch 50: training loss 0.3766\n",
      "2025-05-29 21:04:30 [INFO]: epoch 51: training loss 0.3551\n",
      "2025-05-29 21:04:30 [INFO]: epoch 52: training loss 0.3594\n",
      "2025-05-29 21:04:30 [INFO]: epoch 53: training loss 0.3732\n",
      "2025-05-29 21:04:30 [INFO]: epoch 54: training loss 0.3822\n",
      "2025-05-29 21:04:30 [INFO]: epoch 55: training loss 0.3736\n",
      "2025-05-29 21:04:30 [INFO]: epoch 56: training loss 0.3735\n",
      "2025-05-29 21:04:30 [INFO]: epoch 57: training loss 0.3867\n",
      "2025-05-29 21:04:30 [INFO]: epoch 58: training loss 0.3197\n",
      "2025-05-29 21:04:30 [INFO]: epoch 59: training loss 0.3598\n",
      "2025-05-29 21:04:30 [INFO]: epoch 60: training loss 0.3636\n",
      "2025-05-29 21:04:30 [INFO]: epoch 61: training loss 0.3166\n",
      "2025-05-29 21:04:30 [INFO]: epoch 62: training loss 0.3112\n",
      "2025-05-29 21:04:30 [INFO]: epoch 63: training loss 0.3565\n",
      "2025-05-29 21:04:30 [INFO]: epoch 64: training loss 0.3158\n",
      "2025-05-29 21:04:30 [INFO]: epoch 65: training loss 0.3451\n",
      "2025-05-29 21:04:30 [INFO]: epoch 66: training loss 0.3370\n",
      "2025-05-29 21:04:31 [INFO]: epoch 67: training loss 0.3218\n",
      "2025-05-29 21:04:31 [INFO]: epoch 68: training loss 0.3228\n",
      "2025-05-29 21:04:31 [INFO]: epoch 69: training loss 0.3348\n",
      "2025-05-29 21:04:31 [INFO]: epoch 70: training loss 0.3343\n",
      "2025-05-29 21:04:31 [INFO]: epoch 71: training loss 0.3164\n",
      "2025-05-29 21:04:31 [INFO]: epoch 72: training loss 0.3237\n",
      "2025-05-29 21:04:31 [INFO]: epoch 73: training loss 0.3566\n",
      "2025-05-29 21:04:31 [INFO]: epoch 74: training loss 0.3056\n",
      "2025-05-29 21:04:31 [INFO]: epoch 75: training loss 0.3349\n",
      "2025-05-29 21:04:31 [INFO]: epoch 76: training loss 0.3035\n",
      "2025-05-29 21:04:31 [INFO]: epoch 77: training loss 0.3083\n",
      "2025-05-29 21:04:31 [INFO]: epoch 78: training loss 0.3462\n",
      "2025-05-29 21:04:31 [INFO]: epoch 79: training loss 0.2962\n",
      "2025-05-29 21:04:31 [INFO]: epoch 80: training loss 0.3283\n",
      "2025-05-29 21:04:31 [INFO]: epoch 81: training loss 0.2884\n",
      "2025-05-29 21:04:31 [INFO]: epoch 82: training loss 0.2925\n",
      "2025-05-29 21:04:31 [INFO]: epoch 83: training loss 0.3297\n",
      "2025-05-29 21:04:31 [INFO]: epoch 84: training loss 0.2936\n",
      "2025-05-29 21:04:31 [INFO]: epoch 85: training loss 0.2893\n",
      "2025-05-29 21:04:31 [INFO]: epoch 86: training loss 0.2830\n",
      "2025-05-29 21:04:31 [INFO]: epoch 87: training loss 0.2956\n",
      "2025-05-29 21:04:31 [INFO]: epoch 88: training loss 0.2844\n",
      "2025-05-29 21:04:31 [INFO]: epoch 89: training loss 0.3227\n",
      "2025-05-29 21:04:31 [INFO]: epoch 90: training loss 0.2971\n",
      "2025-05-29 21:04:31 [INFO]: epoch 91: training loss 0.3058\n",
      "2025-05-29 21:04:31 [INFO]: epoch 92: training loss 0.2901\n",
      "2025-05-29 21:04:31 [INFO]: epoch 93: training loss 0.2816\n",
      "2025-05-29 21:04:31 [INFO]: epoch 94: training loss 0.3181\n",
      "2025-05-29 21:04:31 [INFO]: epoch 95: training loss 0.2997\n",
      "2025-05-29 21:04:31 [INFO]: epoch 96: training loss 0.2875\n",
      "2025-05-29 21:04:31 [INFO]: epoch 97: training loss 0.2848\n",
      "2025-05-29 21:04:31 [INFO]: epoch 98: training loss 0.2783\n",
      "2025-05-29 21:04:31 [INFO]: epoch 99: training loss 0.2810\n",
      "2025-05-29 21:04:31 [INFO]: epoch 100: training loss 0.2684\n",
      "2025-05-29 21:04:31 [INFO]: epoch 101: training loss 0.2989\n",
      "2025-05-29 21:04:31 [INFO]: epoch 102: training loss 0.2488\n",
      "2025-05-29 21:04:31 [INFO]: epoch 103: training loss 0.2686\n",
      "2025-05-29 21:04:31 [INFO]: epoch 104: training loss 0.2551\n",
      "2025-05-29 21:04:31 [INFO]: epoch 105: training loss 0.2485\n",
      "2025-05-29 21:04:31 [INFO]: epoch 106: training loss 0.2499\n",
      "2025-05-29 21:04:31 [INFO]: epoch 107: training loss 0.2816\n",
      "2025-05-29 21:04:31 [INFO]: epoch 108: training loss 0.2567\n",
      "2025-05-29 21:04:31 [INFO]: epoch 109: training loss 0.2507\n",
      "2025-05-29 21:04:31 [INFO]: epoch 110: training loss 0.2645\n",
      "2025-05-29 21:04:31 [INFO]: epoch 111: training loss 0.2523\n",
      "2025-05-29 21:04:31 [INFO]: epoch 112: training loss 0.2477\n",
      "2025-05-29 21:04:31 [INFO]: epoch 113: training loss 0.2479\n",
      "2025-05-29 21:04:31 [INFO]: epoch 114: training loss 0.2425\n",
      "2025-05-29 21:04:31 [INFO]: epoch 115: training loss 0.2372\n",
      "2025-05-29 21:04:31 [INFO]: epoch 116: training loss 0.2421\n",
      "2025-05-29 21:04:31 [INFO]: epoch 117: training loss 0.2196\n",
      "2025-05-29 21:04:31 [INFO]: epoch 118: training loss 0.2531\n",
      "2025-05-29 21:04:31 [INFO]: epoch 119: training loss 0.2412\n",
      "2025-05-29 21:04:31 [INFO]: epoch 120: training loss 0.2249\n",
      "2025-05-29 21:04:31 [INFO]: epoch 121: training loss 0.2262\n",
      "2025-05-29 21:04:31 [INFO]: epoch 122: training loss 0.2319\n",
      "2025-05-29 21:04:31 [INFO]: epoch 123: training loss 0.2266\n",
      "2025-05-29 21:04:31 [INFO]: epoch 124: training loss 0.2238\n",
      "2025-05-29 21:04:31 [INFO]: epoch 125: training loss 0.2218\n",
      "2025-05-29 21:04:31 [INFO]: epoch 126: training loss 0.2262\n",
      "2025-05-29 21:04:31 [INFO]: epoch 127: training loss 0.2420\n",
      "2025-05-29 21:04:31 [INFO]: epoch 128: training loss 0.2387\n",
      "2025-05-29 21:04:31 [INFO]: epoch 129: training loss 0.2012\n",
      "2025-05-29 21:04:31 [INFO]: epoch 130: training loss 0.2296\n",
      "2025-05-29 21:04:31 [INFO]: epoch 131: training loss 0.2430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:31 [INFO]: epoch 132: training loss 0.2226\n",
      "2025-05-29 21:04:31 [INFO]: epoch 133: training loss 0.2274\n",
      "2025-05-29 21:04:31 [INFO]: epoch 134: training loss 0.2141\n",
      "2025-05-29 21:04:31 [INFO]: epoch 135: training loss 0.2216\n",
      "2025-05-29 21:04:31 [INFO]: epoch 136: training loss 0.2088\n",
      "2025-05-29 21:04:31 [INFO]: epoch 137: training loss 0.2406\n",
      "2025-05-29 21:04:31 [INFO]: epoch 138: training loss 0.2267\n",
      "2025-05-29 21:04:31 [INFO]: epoch 139: training loss 0.2164\n",
      "2025-05-29 21:04:31 [INFO]: epoch 140: training loss 0.2322\n",
      "2025-05-29 21:04:31 [INFO]: epoch 141: training loss 0.2167\n",
      "2025-05-29 21:04:31 [INFO]: epoch 142: training loss 0.2111\n",
      "2025-05-29 21:04:31 [INFO]: epoch 143: training loss 0.2183\n",
      "2025-05-29 21:04:31 [INFO]: epoch 144: training loss 0.2489\n",
      "2025-05-29 21:04:31 [INFO]: epoch 145: training loss 0.2349\n",
      "2025-05-29 21:04:31 [INFO]: epoch 146: training loss 0.2299\n",
      "2025-05-29 21:04:32 [INFO]: epoch 147: training loss 0.2436\n",
      "2025-05-29 21:04:32 [INFO]: epoch 148: training loss 0.2011\n",
      "2025-05-29 21:04:32 [INFO]: epoch 149: training loss 0.2154\n",
      "2025-05-29 21:04:32 [INFO]: epoch 150: training loss 0.2184\n",
      "2025-05-29 21:04:32 [INFO]: epoch 151: training loss 0.1970\n",
      "2025-05-29 21:04:32 [INFO]: epoch 152: training loss 0.1818\n",
      "2025-05-29 21:04:32 [INFO]: epoch 153: training loss 0.2104\n",
      "2025-05-29 21:04:32 [INFO]: epoch 154: training loss 0.1985\n",
      "2025-05-29 21:04:32 [INFO]: epoch 155: training loss 0.1995\n",
      "2025-05-29 21:04:32 [INFO]: epoch 156: training loss 0.2011\n",
      "2025-05-29 21:04:32 [INFO]: epoch 157: training loss 0.2012\n",
      "2025-05-29 21:04:32 [INFO]: epoch 158: training loss 0.2090\n",
      "2025-05-29 21:04:32 [INFO]: epoch 159: training loss 0.1889\n",
      "2025-05-29 21:04:32 [INFO]: epoch 160: training loss 0.1866\n",
      "2025-05-29 21:04:32 [INFO]: epoch 161: training loss 0.1852\n",
      "2025-05-29 21:04:32 [INFO]: epoch 162: training loss 0.1942\n",
      "2025-05-29 21:04:32 [INFO]: epoch 163: training loss 0.2031\n",
      "2025-05-29 21:04:32 [INFO]: epoch 164: training loss 0.1978\n",
      "2025-05-29 21:04:32 [INFO]: epoch 165: training loss 0.2033\n",
      "2025-05-29 21:04:32 [INFO]: epoch 166: training loss 0.1903\n",
      "2025-05-29 21:04:32 [INFO]: epoch 167: training loss 0.1773\n",
      "2025-05-29 21:04:32 [INFO]: epoch 168: training loss 0.1868\n",
      "2025-05-29 21:04:32 [INFO]: epoch 169: training loss 0.2026\n",
      "2025-05-29 21:04:32 [INFO]: epoch 170: training loss 0.2014\n",
      "2025-05-29 21:04:32 [INFO]: epoch 171: training loss 0.1882\n",
      "2025-05-29 21:04:32 [INFO]: epoch 172: training loss 0.1700\n",
      "2025-05-29 21:04:32 [INFO]: epoch 173: training loss 0.1937\n",
      "2025-05-29 21:04:32 [INFO]: epoch 174: training loss 0.1677\n",
      "2025-05-29 21:04:32 [INFO]: epoch 175: training loss 0.1838\n",
      "2025-05-29 21:04:32 [INFO]: epoch 176: training loss 0.1669\n",
      "2025-05-29 21:04:32 [INFO]: epoch 177: training loss 0.1789\n",
      "2025-05-29 21:04:32 [INFO]: epoch 178: training loss 0.1758\n",
      "2025-05-29 21:04:32 [INFO]: epoch 179: training loss 0.1791\n",
      "2025-05-29 21:04:32 [INFO]: epoch 180: training loss 0.1604\n",
      "2025-05-29 21:04:32 [INFO]: epoch 181: training loss 0.1766\n",
      "2025-05-29 21:04:32 [INFO]: epoch 182: training loss 0.1897\n",
      "2025-05-29 21:04:32 [INFO]: epoch 183: training loss 0.1787\n",
      "2025-05-29 21:04:32 [INFO]: epoch 184: training loss 0.1601\n",
      "2025-05-29 21:04:32 [INFO]: epoch 185: training loss 0.1687\n",
      "2025-05-29 21:04:32 [INFO]: epoch 186: training loss 0.2043\n",
      "2025-05-29 21:04:32 [INFO]: epoch 187: training loss 0.1736\n",
      "2025-05-29 21:04:32 [INFO]: epoch 188: training loss 0.1751\n",
      "2025-05-29 21:04:32 [INFO]: epoch 189: training loss 0.1874\n",
      "2025-05-29 21:04:32 [INFO]: epoch 190: training loss 0.1849\n",
      "2025-05-29 21:04:32 [INFO]: epoch 191: training loss 0.1649\n",
      "2025-05-29 21:04:32 [INFO]: epoch 192: training loss 0.1797\n",
      "2025-05-29 21:04:32 [INFO]: epoch 193: training loss 0.1716\n",
      "2025-05-29 21:04:32 [INFO]: epoch 194: training loss 0.1582\n",
      "2025-05-29 21:04:32 [INFO]: epoch 195: training loss 0.1553\n",
      "2025-05-29 21:04:32 [INFO]: epoch 196: training loss 0.1390\n",
      "2025-05-29 21:04:32 [INFO]: epoch 197: training loss 0.1512\n",
      "2025-05-29 21:04:32 [INFO]: epoch 198: training loss 0.1763\n",
      "2025-05-29 21:04:32 [INFO]: epoch 199: training loss 0.1701\n",
      "2025-05-29 21:04:32 [INFO]: epoch 200: training loss 0.1516\n",
      "2025-05-29 21:04:32 [INFO]: epoch 201: training loss 0.1518\n",
      "2025-05-29 21:04:32 [INFO]: epoch 202: training loss 0.1402\n",
      "2025-05-29 21:04:32 [INFO]: epoch 203: training loss 0.1614\n",
      "2025-05-29 21:04:32 [INFO]: epoch 204: training loss 0.1542\n",
      "2025-05-29 21:04:32 [INFO]: epoch 205: training loss 0.1416\n",
      "2025-05-29 21:04:32 [INFO]: epoch 206: training loss 0.1422\n",
      "2025-05-29 21:04:32 [INFO]: epoch 207: training loss 0.1555\n",
      "2025-05-29 21:04:32 [INFO]: epoch 208: training loss 0.1425\n",
      "2025-05-29 21:04:32 [INFO]: epoch 209: training loss 0.1288\n",
      "2025-05-29 21:04:32 [INFO]: epoch 210: training loss 0.1507\n",
      "2025-05-29 21:04:32 [INFO]: epoch 211: training loss 0.1635\n",
      "2025-05-29 21:04:32 [INFO]: epoch 212: training loss 0.1417\n",
      "2025-05-29 21:04:32 [INFO]: epoch 213: training loss 0.1472\n",
      "2025-05-29 21:04:32 [INFO]: epoch 214: training loss 0.1401\n",
      "2025-05-29 21:04:32 [INFO]: epoch 215: training loss 0.1407\n",
      "2025-05-29 21:04:32 [INFO]: epoch 216: training loss 0.1340\n",
      "2025-05-29 21:04:32 [INFO]: epoch 217: training loss 0.1529\n",
      "2025-05-29 21:04:32 [INFO]: epoch 218: training loss 0.1543\n",
      "2025-05-29 21:04:32 [INFO]: epoch 219: training loss 0.1454\n",
      "2025-05-29 21:04:32 [INFO]: epoch 220: training loss 0.1379\n",
      "2025-05-29 21:04:32 [INFO]: epoch 221: training loss 0.1448\n",
      "2025-05-29 21:04:32 [INFO]: epoch 222: training loss 0.1688\n",
      "2025-05-29 21:04:32 [INFO]: epoch 223: training loss 0.1614\n",
      "2025-05-29 21:04:32 [INFO]: epoch 224: training loss 0.1381\n",
      "2025-05-29 21:04:32 [INFO]: epoch 225: training loss 0.1376\n",
      "2025-05-29 21:04:32 [INFO]: epoch 226: training loss 0.1551\n",
      "2025-05-29 21:04:33 [INFO]: epoch 227: training loss 0.1371\n",
      "2025-05-29 21:04:33 [INFO]: epoch 228: training loss 0.1317\n",
      "2025-05-29 21:04:33 [INFO]: epoch 229: training loss 0.1507\n",
      "2025-05-29 21:04:33 [INFO]: epoch 230: training loss 0.1422\n",
      "2025-05-29 21:04:33 [INFO]: epoch 231: training loss 0.1461\n",
      "2025-05-29 21:04:33 [INFO]: epoch 232: training loss 0.1283\n",
      "2025-05-29 21:04:33 [INFO]: epoch 233: training loss 0.1378\n",
      "2025-05-29 21:04:33 [INFO]: epoch 234: training loss 0.1334\n",
      "2025-05-29 21:04:33 [INFO]: epoch 235: training loss 0.1416\n",
      "2025-05-29 21:04:33 [INFO]: epoch 236: training loss 0.1260\n",
      "2025-05-29 21:04:33 [INFO]: epoch 237: training loss 0.1314\n",
      "2025-05-29 21:04:33 [INFO]: epoch 238: training loss 0.1454\n",
      "2025-05-29 21:04:33 [INFO]: epoch 239: training loss 0.1319\n",
      "2025-05-29 21:04:33 [INFO]: epoch 240: training loss 0.1355\n",
      "2025-05-29 21:04:33 [INFO]: epoch 241: training loss 0.1322\n",
      "2025-05-29 21:04:33 [INFO]: epoch 242: training loss 0.1276\n",
      "2025-05-29 21:04:33 [INFO]: epoch 243: training loss 0.1332\n",
      "2025-05-29 21:04:33 [INFO]: epoch 244: training loss 0.1259\n",
      "2025-05-29 21:04:33 [INFO]: epoch 245: training loss 0.1373\n",
      "2025-05-29 21:04:33 [INFO]: epoch 246: training loss 0.1549\n",
      "2025-05-29 21:04:33 [INFO]: epoch 247: training loss 0.1422\n",
      "2025-05-29 21:04:33 [INFO]: epoch 248: training loss 0.1190\n",
      "2025-05-29 21:04:33 [INFO]: epoch 249: training loss 0.1232\n",
      "2025-05-29 21:04:33 [INFO]: epoch 250: training loss 0.1217\n",
      "2025-05-29 21:04:33 [INFO]: epoch 251: training loss 0.1385\n",
      "2025-05-29 21:04:33 [INFO]: epoch 252: training loss 0.1176\n",
      "2025-05-29 21:04:33 [INFO]: epoch 253: training loss 0.1084\n",
      "2025-05-29 21:04:33 [INFO]: epoch 254: training loss 0.1143\n",
      "2025-05-29 21:04:33 [INFO]: epoch 255: training loss 0.1166\n",
      "2025-05-29 21:04:33 [INFO]: epoch 256: training loss 0.1233\n",
      "2025-05-29 21:04:33 [INFO]: epoch 257: training loss 0.1349\n",
      "2025-05-29 21:04:33 [INFO]: epoch 258: training loss 0.1173\n",
      "2025-05-29 21:04:33 [INFO]: epoch 259: training loss 0.1221\n",
      "2025-05-29 21:04:33 [INFO]: epoch 260: training loss 0.1382\n",
      "2025-05-29 21:04:33 [INFO]: epoch 261: training loss 0.1076\n",
      "2025-05-29 21:04:33 [INFO]: epoch 262: training loss 0.1097\n",
      "2025-05-29 21:04:33 [INFO]: epoch 263: training loss 0.1244\n",
      "2025-05-29 21:04:33 [INFO]: epoch 264: training loss 0.1171\n",
      "2025-05-29 21:04:33 [INFO]: epoch 265: training loss 0.1240\n",
      "2025-05-29 21:04:33 [INFO]: epoch 266: training loss 0.1183\n",
      "2025-05-29 21:04:33 [INFO]: epoch 267: training loss 0.1058\n",
      "2025-05-29 21:04:33 [INFO]: epoch 268: training loss 0.1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:33 [INFO]: epoch 269: training loss 0.1097\n",
      "2025-05-29 21:04:33 [INFO]: epoch 270: training loss 0.1114\n",
      "2025-05-29 21:04:33 [INFO]: epoch 271: training loss 0.1014\n",
      "2025-05-29 21:04:33 [INFO]: epoch 272: training loss 0.1013\n",
      "2025-05-29 21:04:33 [INFO]: epoch 273: training loss 0.1012\n",
      "2025-05-29 21:04:33 [INFO]: epoch 274: training loss 0.1006\n",
      "2025-05-29 21:04:33 [INFO]: epoch 275: training loss 0.1005\n",
      "2025-05-29 21:04:33 [INFO]: epoch 276: training loss 0.1119\n",
      "2025-05-29 21:04:33 [INFO]: epoch 277: training loss 0.1023\n",
      "2025-05-29 21:04:33 [INFO]: epoch 278: training loss 0.0957\n",
      "2025-05-29 21:04:33 [INFO]: epoch 279: training loss 0.1026\n",
      "2025-05-29 21:04:33 [INFO]: epoch 280: training loss 0.1041\n",
      "2025-05-29 21:04:33 [INFO]: epoch 281: training loss 0.1041\n",
      "2025-05-29 21:04:33 [INFO]: epoch 282: training loss 0.0998\n",
      "2025-05-29 21:04:33 [INFO]: epoch 283: training loss 0.1007\n",
      "2025-05-29 21:04:33 [INFO]: epoch 284: training loss 0.1097\n",
      "2025-05-29 21:04:33 [INFO]: epoch 285: training loss 0.1010\n",
      "2025-05-29 21:04:33 [INFO]: epoch 286: training loss 0.1040\n",
      "2025-05-29 21:04:33 [INFO]: epoch 287: training loss 0.1134\n",
      "2025-05-29 21:04:33 [INFO]: epoch 288: training loss 0.0974\n",
      "2025-05-29 21:04:33 [INFO]: epoch 289: training loss 0.1014\n",
      "2025-05-29 21:04:33 [INFO]: epoch 290: training loss 0.0867\n",
      "2025-05-29 21:04:33 [INFO]: epoch 291: training loss 0.1003\n",
      "2025-05-29 21:04:33 [INFO]: epoch 292: training loss 0.1021\n",
      "2025-05-29 21:04:33 [INFO]: epoch 293: training loss 0.0988\n",
      "2025-05-29 21:04:33 [INFO]: epoch 294: training loss 0.0833\n",
      "2025-05-29 21:04:33 [INFO]: epoch 295: training loss 0.0955\n",
      "2025-05-29 21:04:33 [INFO]: epoch 296: training loss 0.0936\n",
      "2025-05-29 21:04:33 [INFO]: epoch 297: training loss 0.0785\n",
      "2025-05-29 21:04:33 [INFO]: epoch 298: training loss 0.0736\n",
      "2025-05-29 21:04:33 [INFO]: epoch 299: training loss 0.0862\n",
      "2025-05-29 21:04:33 [INFO]: epoch 300: training loss 0.0923\n",
      "2025-05-29 21:04:33 [INFO]: epoch 301: training loss 0.0889\n",
      "2025-05-29 21:04:33 [INFO]: epoch 302: training loss 0.0835\n",
      "2025-05-29 21:04:33 [INFO]: epoch 303: training loss 0.0836\n",
      "2025-05-29 21:04:33 [INFO]: epoch 304: training loss 0.0861\n",
      "2025-05-29 21:04:33 [INFO]: epoch 305: training loss 0.0803\n",
      "2025-05-29 21:04:33 [INFO]: epoch 306: training loss 0.0810\n",
      "2025-05-29 21:04:34 [INFO]: epoch 307: training loss 0.0861\n",
      "2025-05-29 21:04:34 [INFO]: epoch 308: training loss 0.0901\n",
      "2025-05-29 21:04:34 [INFO]: epoch 309: training loss 0.0827\n",
      "2025-05-29 21:04:34 [INFO]: epoch 310: training loss 0.0746\n",
      "2025-05-29 21:04:34 [INFO]: epoch 311: training loss 0.0775\n",
      "2025-05-29 21:04:34 [INFO]: epoch 312: training loss 0.0830\n",
      "2025-05-29 21:04:34 [INFO]: epoch 313: training loss 0.0962\n",
      "2025-05-29 21:04:34 [INFO]: epoch 314: training loss 0.0880\n",
      "2025-05-29 21:04:34 [INFO]: epoch 315: training loss 0.0805\n",
      "2025-05-29 21:04:34 [INFO]: epoch 316: training loss 0.0724\n",
      "2025-05-29 21:04:34 [INFO]: epoch 317: training loss 0.0792\n",
      "2025-05-29 21:04:34 [INFO]: epoch 318: training loss 0.0779\n",
      "2025-05-29 21:04:34 [INFO]: epoch 319: training loss 0.0685\n",
      "2025-05-29 21:04:34 [INFO]: epoch 320: training loss 0.0743\n",
      "2025-05-29 21:04:34 [INFO]: epoch 321: training loss 0.0870\n",
      "2025-05-29 21:04:34 [INFO]: epoch 322: training loss 0.0749\n",
      "2025-05-29 21:04:34 [INFO]: epoch 323: training loss 0.0767\n",
      "2025-05-29 21:04:34 [INFO]: epoch 324: training loss 0.0830\n",
      "2025-05-29 21:04:34 [INFO]: epoch 325: training loss 0.0735\n",
      "2025-05-29 21:04:34 [INFO]: epoch 326: training loss 0.0700\n",
      "2025-05-29 21:04:34 [INFO]: epoch 327: training loss 0.0885\n",
      "2025-05-29 21:04:34 [INFO]: epoch 328: training loss 0.0802\n",
      "2025-05-29 21:04:34 [INFO]: epoch 329: training loss 0.0814\n",
      "2025-05-29 21:04:34 [INFO]: epoch 330: training loss 0.0746\n",
      "2025-05-29 21:04:34 [INFO]: epoch 331: training loss 0.0840\n",
      "2025-05-29 21:04:34 [INFO]: epoch 332: training loss 0.0842\n",
      "2025-05-29 21:04:34 [INFO]: epoch 333: training loss 0.0717\n",
      "2025-05-29 21:04:34 [INFO]: epoch 334: training loss 0.0728\n",
      "2025-05-29 21:04:34 [INFO]: epoch 335: training loss 0.0671\n",
      "2025-05-29 21:04:34 [INFO]: epoch 336: training loss 0.0725\n",
      "2025-05-29 21:04:34 [INFO]: epoch 337: training loss 0.0779\n",
      "2025-05-29 21:04:34 [INFO]: epoch 338: training loss 0.0697\n",
      "2025-05-29 21:04:34 [INFO]: epoch 339: training loss 0.0693\n",
      "2025-05-29 21:04:34 [INFO]: epoch 340: training loss 0.0671\n",
      "2025-05-29 21:04:34 [INFO]: epoch 341: training loss 0.0675\n",
      "2025-05-29 21:04:34 [INFO]: epoch 342: training loss 0.0657\n",
      "2025-05-29 21:04:34 [INFO]: epoch 343: training loss 0.0743\n",
      "2025-05-29 21:04:34 [INFO]: epoch 344: training loss 0.0681\n",
      "2025-05-29 21:04:34 [INFO]: epoch 345: training loss 0.0638\n",
      "2025-05-29 21:04:34 [INFO]: epoch 346: training loss 0.0683\n",
      "2025-05-29 21:04:34 [INFO]: epoch 347: training loss 0.0623\n",
      "2025-05-29 21:04:34 [INFO]: epoch 348: training loss 0.0600\n",
      "2025-05-29 21:04:34 [INFO]: epoch 349: training loss 0.0743\n",
      "2025-05-29 21:04:34 [INFO]: epoch 350: training loss 0.0691\n",
      "2025-05-29 21:04:34 [INFO]: epoch 351: training loss 0.0682\n",
      "2025-05-29 21:04:34 [INFO]: epoch 352: training loss 0.0632\n",
      "2025-05-29 21:04:34 [INFO]: epoch 353: training loss 0.0635\n",
      "2025-05-29 21:04:34 [INFO]: epoch 354: training loss 0.0654\n",
      "2025-05-29 21:04:34 [INFO]: epoch 355: training loss 0.0615\n",
      "2025-05-29 21:04:34 [INFO]: epoch 356: training loss 0.0693\n",
      "2025-05-29 21:04:34 [INFO]: epoch 357: training loss 0.0616\n",
      "2025-05-29 21:04:34 [INFO]: epoch 358: training loss 0.0574\n",
      "2025-05-29 21:04:34 [INFO]: epoch 359: training loss 0.0636\n",
      "2025-05-29 21:04:34 [INFO]: epoch 360: training loss 0.0603\n",
      "2025-05-29 21:04:34 [INFO]: epoch 361: training loss 0.0583\n",
      "2025-05-29 21:04:34 [INFO]: epoch 362: training loss 0.0584\n",
      "2025-05-29 21:04:34 [INFO]: epoch 363: training loss 0.0545\n",
      "2025-05-29 21:04:34 [INFO]: epoch 364: training loss 0.0648\n",
      "2025-05-29 21:04:34 [INFO]: epoch 365: training loss 0.0614\n",
      "2025-05-29 21:04:34 [INFO]: epoch 366: training loss 0.0487\n",
      "2025-05-29 21:04:34 [INFO]: epoch 367: training loss 0.0528\n",
      "2025-05-29 21:04:34 [INFO]: epoch 368: training loss 0.0571\n",
      "2025-05-29 21:04:34 [INFO]: epoch 369: training loss 0.0604\n",
      "2025-05-29 21:04:34 [INFO]: epoch 370: training loss 0.0580\n",
      "2025-05-29 21:04:34 [INFO]: epoch 371: training loss 0.0541\n",
      "2025-05-29 21:04:34 [INFO]: epoch 372: training loss 0.0550\n",
      "2025-05-29 21:04:34 [INFO]: epoch 373: training loss 0.0580\n",
      "2025-05-29 21:04:34 [INFO]: epoch 374: training loss 0.0582\n",
      "2025-05-29 21:04:34 [INFO]: epoch 375: training loss 0.0606\n",
      "2025-05-29 21:04:34 [INFO]: epoch 376: training loss 0.0552\n",
      "2025-05-29 21:04:34 [INFO]: epoch 377: training loss 0.0603\n",
      "2025-05-29 21:04:34 [INFO]: epoch 378: training loss 0.0564\n",
      "2025-05-29 21:04:34 [INFO]: epoch 379: training loss 0.0561\n",
      "2025-05-29 21:04:34 [INFO]: epoch 380: training loss 0.0560\n",
      "2025-05-29 21:04:34 [INFO]: epoch 381: training loss 0.0533\n",
      "2025-05-29 21:04:34 [INFO]: epoch 382: training loss 0.0634\n",
      "2025-05-29 21:04:34 [INFO]: epoch 383: training loss 0.0606\n",
      "2025-05-29 21:04:34 [INFO]: epoch 384: training loss 0.0693\n",
      "2025-05-29 21:04:34 [INFO]: epoch 385: training loss 0.0578\n",
      "2025-05-29 21:04:34 [INFO]: epoch 386: training loss 0.0531\n",
      "2025-05-29 21:04:35 [INFO]: epoch 387: training loss 0.0533\n",
      "2025-05-29 21:04:35 [INFO]: epoch 388: training loss 0.0576\n",
      "2025-05-29 21:04:35 [INFO]: epoch 389: training loss 0.0525\n",
      "2025-05-29 21:04:35 [INFO]: epoch 390: training loss 0.0540\n",
      "2025-05-29 21:04:35 [INFO]: epoch 391: training loss 0.0551\n",
      "2025-05-29 21:04:35 [INFO]: epoch 392: training loss 0.0580\n",
      "2025-05-29 21:04:35 [INFO]: epoch 393: training loss 0.0424\n",
      "2025-05-29 21:04:35 [INFO]: epoch 394: training loss 0.0462\n",
      "2025-05-29 21:04:35 [INFO]: epoch 395: training loss 0.0575\n",
      "2025-05-29 21:04:35 [INFO]: epoch 396: training loss 0.0536\n",
      "2025-05-29 21:04:35 [INFO]: epoch 397: training loss 0.0470\n",
      "2025-05-29 21:04:35 [INFO]: epoch 398: training loss 0.0442\n",
      "2025-05-29 21:04:35 [INFO]: epoch 399: training loss 0.0492\n",
      "2025-05-29 21:04:35 [INFO]: epoch 400: training loss 0.0433\n",
      "2025-05-29 21:04:35 [INFO]: epoch 401: training loss 0.0536\n",
      "2025-05-29 21:04:35 [INFO]: epoch 402: training loss 0.0500\n",
      "2025-05-29 21:04:35 [INFO]: epoch 403: training loss 0.0537\n",
      "2025-05-29 21:04:35 [INFO]: epoch 404: training loss 0.0492\n",
      "2025-05-29 21:04:35 [INFO]: epoch 405: training loss 0.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:35 [INFO]: epoch 406: training loss 0.0507\n",
      "2025-05-29 21:04:35 [INFO]: epoch 407: training loss 0.0538\n",
      "2025-05-29 21:04:35 [INFO]: epoch 408: training loss 0.0550\n",
      "2025-05-29 21:04:35 [INFO]: epoch 409: training loss 0.0467\n",
      "2025-05-29 21:04:35 [INFO]: epoch 410: training loss 0.0517\n",
      "2025-05-29 21:04:35 [INFO]: epoch 411: training loss 0.0528\n",
      "2025-05-29 21:04:35 [INFO]: epoch 412: training loss 0.0495\n",
      "2025-05-29 21:04:35 [INFO]: epoch 413: training loss 0.0465\n",
      "2025-05-29 21:04:35 [INFO]: epoch 414: training loss 0.0468\n",
      "2025-05-29 21:04:35 [INFO]: epoch 415: training loss 0.0502\n",
      "2025-05-29 21:04:35 [INFO]: epoch 416: training loss 0.0520\n",
      "2025-05-29 21:04:35 [INFO]: epoch 417: training loss 0.0471\n",
      "2025-05-29 21:04:35 [INFO]: epoch 418: training loss 0.0437\n",
      "2025-05-29 21:04:35 [INFO]: epoch 419: training loss 0.0469\n",
      "2025-05-29 21:04:35 [INFO]: epoch 420: training loss 0.0432\n",
      "2025-05-29 21:04:35 [INFO]: epoch 421: training loss 0.0451\n",
      "2025-05-29 21:04:35 [INFO]: epoch 422: training loss 0.0424\n",
      "2025-05-29 21:04:35 [INFO]: epoch 423: training loss 0.0407\n",
      "2025-05-29 21:04:35 [INFO]: epoch 424: training loss 0.0411\n",
      "2025-05-29 21:04:35 [INFO]: epoch 425: training loss 0.0398\n",
      "2025-05-29 21:04:35 [INFO]: epoch 426: training loss 0.0437\n",
      "2025-05-29 21:04:35 [INFO]: epoch 427: training loss 0.0469\n",
      "2025-05-29 21:04:35 [INFO]: epoch 428: training loss 0.0381\n",
      "2025-05-29 21:04:35 [INFO]: epoch 429: training loss 0.0448\n",
      "2025-05-29 21:04:35 [INFO]: epoch 430: training loss 0.0454\n",
      "2025-05-29 21:04:35 [INFO]: epoch 431: training loss 0.0429\n",
      "2025-05-29 21:04:35 [INFO]: epoch 432: training loss 0.0442\n",
      "2025-05-29 21:04:35 [INFO]: epoch 433: training loss 0.0443\n",
      "2025-05-29 21:04:35 [INFO]: epoch 434: training loss 0.0463\n",
      "2025-05-29 21:04:35 [INFO]: epoch 435: training loss 0.0479\n",
      "2025-05-29 21:04:35 [INFO]: epoch 436: training loss 0.0480\n",
      "2025-05-29 21:04:35 [INFO]: epoch 437: training loss 0.0386\n",
      "2025-05-29 21:04:35 [INFO]: epoch 438: training loss 0.0397\n",
      "2025-05-29 21:04:35 [INFO]: epoch 439: training loss 0.0496\n",
      "2025-05-29 21:04:35 [INFO]: epoch 440: training loss 0.0519\n",
      "2025-05-29 21:04:35 [INFO]: epoch 441: training loss 0.0455\n",
      "2025-05-29 21:04:35 [INFO]: epoch 442: training loss 0.0410\n",
      "2025-05-29 21:04:35 [INFO]: epoch 443: training loss 0.0387\n",
      "2025-05-29 21:04:35 [INFO]: epoch 444: training loss 0.0421\n",
      "2025-05-29 21:04:35 [INFO]: epoch 445: training loss 0.0435\n",
      "2025-05-29 21:04:35 [INFO]: epoch 446: training loss 0.0482\n",
      "2025-05-29 21:04:35 [INFO]: epoch 447: training loss 0.0437\n",
      "2025-05-29 21:04:35 [INFO]: epoch 448: training loss 0.0442\n",
      "2025-05-29 21:04:35 [INFO]: epoch 449: training loss 0.0374\n",
      "2025-05-29 21:04:35 [INFO]: epoch 450: training loss 0.0417\n",
      "2025-05-29 21:04:35 [INFO]: epoch 451: training loss 0.0392\n",
      "2025-05-29 21:04:35 [INFO]: epoch 452: training loss 0.0477\n",
      "2025-05-29 21:04:35 [INFO]: epoch 453: training loss 0.0401\n",
      "2025-05-29 21:04:35 [INFO]: epoch 454: training loss 0.0366\n",
      "2025-05-29 21:04:35 [INFO]: epoch 455: training loss 0.0427\n",
      "2025-05-29 21:04:35 [INFO]: epoch 456: training loss 0.0392\n",
      "2025-05-29 21:04:35 [INFO]: epoch 457: training loss 0.0359\n",
      "2025-05-29 21:04:35 [INFO]: epoch 458: training loss 0.0354\n",
      "2025-05-29 21:04:35 [INFO]: epoch 459: training loss 0.0343\n",
      "2025-05-29 21:04:35 [INFO]: epoch 460: training loss 0.0363\n",
      "2025-05-29 21:04:35 [INFO]: epoch 461: training loss 0.0386\n",
      "2025-05-29 21:04:35 [INFO]: epoch 462: training loss 0.0352\n",
      "2025-05-29 21:04:35 [INFO]: epoch 463: training loss 0.0316\n",
      "2025-05-29 21:04:35 [INFO]: epoch 464: training loss 0.0308\n",
      "2025-05-29 21:04:35 [INFO]: epoch 465: training loss 0.0325\n",
      "2025-05-29 21:04:35 [INFO]: epoch 466: training loss 0.0349\n",
      "2025-05-29 21:04:36 [INFO]: epoch 467: training loss 0.0348\n",
      "2025-05-29 21:04:36 [INFO]: epoch 468: training loss 0.0312\n",
      "2025-05-29 21:04:36 [INFO]: epoch 469: training loss 0.0350\n",
      "2025-05-29 21:04:36 [INFO]: epoch 470: training loss 0.0310\n",
      "2025-05-29 21:04:36 [INFO]: epoch 471: training loss 0.0313\n",
      "2025-05-29 21:04:36 [INFO]: epoch 472: training loss 0.0383\n",
      "2025-05-29 21:04:36 [INFO]: epoch 473: training loss 0.0367\n",
      "2025-05-29 21:04:36 [INFO]: epoch 474: training loss 0.0310\n",
      "2025-05-29 21:04:36 [INFO]: epoch 475: training loss 0.0306\n",
      "2025-05-29 21:04:36 [INFO]: epoch 476: training loss 0.0383\n",
      "2025-05-29 21:04:36 [INFO]: epoch 477: training loss 0.0331\n",
      "2025-05-29 21:04:36 [INFO]: epoch 478: training loss 0.0324\n",
      "2025-05-29 21:04:36 [INFO]: epoch 479: training loss 0.0334\n",
      "2025-05-29 21:04:36 [INFO]: epoch 480: training loss 0.0325\n",
      "2025-05-29 21:04:36 [INFO]: epoch 481: training loss 0.0314\n",
      "2025-05-29 21:04:36 [INFO]: epoch 482: training loss 0.0331\n",
      "2025-05-29 21:04:36 [INFO]: epoch 483: training loss 0.0349\n",
      "2025-05-29 21:04:36 [INFO]: epoch 484: training loss 0.0312\n",
      "2025-05-29 21:04:36 [INFO]: epoch 485: training loss 0.0363\n",
      "2025-05-29 21:04:36 [INFO]: epoch 486: training loss 0.0311\n",
      "2025-05-29 21:04:36 [INFO]: epoch 487: training loss 0.0319\n",
      "2025-05-29 21:04:36 [INFO]: epoch 488: training loss 0.0310\n",
      "2025-05-29 21:04:36 [INFO]: epoch 489: training loss 0.0319\n",
      "2025-05-29 21:04:36 [INFO]: epoch 490: training loss 0.0326\n",
      "2025-05-29 21:04:36 [INFO]: epoch 491: training loss 0.0300\n",
      "2025-05-29 21:04:36 [INFO]: epoch 492: training loss 0.0285\n",
      "2025-05-29 21:04:36 [INFO]: epoch 493: training loss 0.0380\n",
      "2025-05-29 21:04:36 [INFO]: epoch 494: training loss 0.0347\n",
      "2025-05-29 21:04:36 [INFO]: epoch 495: training loss 0.0354\n",
      "2025-05-29 21:04:36 [INFO]: epoch 496: training loss 0.0295\n",
      "2025-05-29 21:04:36 [INFO]: epoch 497: training loss 0.0314\n",
      "2025-05-29 21:04:36 [INFO]: epoch 498: training loss 0.0376\n",
      "2025-05-29 21:04:36 [INFO]: epoch 499: training loss 0.0328\n",
      "2025-05-29 21:04:36 [INFO]: epoch 500: training loss 0.0265\n",
      "2025-05-29 21:04:36 [INFO]: epoch 501: training loss 0.0381\n",
      "2025-05-29 21:04:36 [INFO]: epoch 502: training loss 0.0325\n",
      "2025-05-29 21:04:36 [INFO]: epoch 503: training loss 0.0340\n",
      "2025-05-29 21:04:36 [INFO]: epoch 504: training loss 0.0290\n",
      "2025-05-29 21:04:36 [INFO]: epoch 505: training loss 0.0276\n",
      "2025-05-29 21:04:36 [INFO]: epoch 506: training loss 0.0307\n",
      "2025-05-29 21:04:36 [INFO]: epoch 507: training loss 0.0279\n",
      "2025-05-29 21:04:36 [INFO]: epoch 508: training loss 0.0246\n",
      "2025-05-29 21:04:36 [INFO]: epoch 509: training loss 0.0319\n",
      "2025-05-29 21:04:36 [INFO]: epoch 510: training loss 0.0291\n",
      "2025-05-29 21:04:36 [INFO]: epoch 511: training loss 0.0304\n",
      "2025-05-29 21:04:36 [INFO]: epoch 512: training loss 0.0325\n",
      "2025-05-29 21:04:36 [INFO]: epoch 513: training loss 0.0291\n",
      "2025-05-29 21:04:36 [INFO]: epoch 514: training loss 0.0306\n",
      "2025-05-29 21:04:36 [INFO]: epoch 515: training loss 0.0272\n",
      "2025-05-29 21:04:36 [INFO]: epoch 516: training loss 0.0283\n",
      "2025-05-29 21:04:36 [INFO]: epoch 517: training loss 0.0331\n",
      "2025-05-29 21:04:36 [INFO]: epoch 518: training loss 0.0272\n",
      "2025-05-29 21:04:36 [INFO]: epoch 519: training loss 0.0351\n",
      "2025-05-29 21:04:36 [INFO]: epoch 520: training loss 0.0382\n",
      "2025-05-29 21:04:36 [INFO]: epoch 521: training loss 0.0257\n",
      "2025-05-29 21:04:36 [INFO]: epoch 522: training loss 0.0282\n",
      "2025-05-29 21:04:36 [INFO]: epoch 523: training loss 0.0239\n",
      "2025-05-29 21:04:36 [INFO]: epoch 524: training loss 0.0254\n",
      "2025-05-29 21:04:36 [INFO]: epoch 525: training loss 0.0312\n",
      "2025-05-29 21:04:36 [INFO]: epoch 526: training loss 0.0281\n",
      "2025-05-29 21:04:36 [INFO]: epoch 527: training loss 0.0249\n",
      "2025-05-29 21:04:36 [INFO]: epoch 528: training loss 0.0246\n",
      "2025-05-29 21:04:36 [INFO]: epoch 529: training loss 0.0305\n",
      "2025-05-29 21:04:36 [INFO]: epoch 530: training loss 0.0282\n",
      "2025-05-29 21:04:36 [INFO]: epoch 531: training loss 0.0289\n",
      "2025-05-29 21:04:36 [INFO]: epoch 532: training loss 0.0287\n",
      "2025-05-29 21:04:36 [INFO]: epoch 533: training loss 0.0280\n",
      "2025-05-29 21:04:36 [INFO]: epoch 534: training loss 0.0268\n",
      "2025-05-29 21:04:36 [INFO]: epoch 535: training loss 0.0295\n",
      "2025-05-29 21:04:36 [INFO]: epoch 536: training loss 0.0283\n",
      "2025-05-29 21:04:36 [INFO]: epoch 537: training loss 0.0283\n",
      "2025-05-29 21:04:36 [INFO]: epoch 538: training loss 0.0245\n",
      "2025-05-29 21:04:36 [INFO]: epoch 539: training loss 0.0303\n",
      "2025-05-29 21:04:36 [INFO]: epoch 540: training loss 0.0312\n",
      "2025-05-29 21:04:36 [INFO]: epoch 541: training loss 0.0239\n",
      "2025-05-29 21:04:36 [INFO]: epoch 542: training loss 0.0258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 21:04:36 [INFO]: epoch 543: training loss 0.0297\n",
      "2025-05-29 21:04:36 [INFO]: epoch 544: training loss 0.0262\n",
      "2025-05-29 21:04:36 [INFO]: epoch 545: training loss 0.0307\n",
      "2025-05-29 21:04:36 [INFO]: epoch 546: training loss 0.0265\n",
      "2025-05-29 21:04:37 [INFO]: epoch 547: training loss 0.0286\n",
      "2025-05-29 21:04:37 [INFO]: epoch 548: training loss 0.0335\n",
      "2025-05-29 21:04:37 [INFO]: epoch 549: training loss 0.0322\n",
      "2025-05-29 21:04:37 [INFO]: epoch 550: training loss 0.0271\n",
      "2025-05-29 21:04:37 [INFO]: epoch 551: training loss 0.0279\n",
      "2025-05-29 21:04:37 [INFO]: epoch 552: training loss 0.0252\n",
      "2025-05-29 21:04:37 [INFO]: epoch 553: training loss 0.0276\n",
      "2025-05-29 21:04:37 [INFO]: epoch 554: training loss 0.0287\n",
      "2025-05-29 21:04:37 [INFO]: epoch 555: training loss 0.0196\n",
      "2025-05-29 21:04:37 [INFO]: epoch 556: training loss 0.0262\n",
      "2025-05-29 21:04:37 [INFO]: epoch 557: training loss 0.0232\n",
      "2025-05-29 21:04:37 [INFO]: epoch 558: training loss 0.0304\n",
      "2025-05-29 21:04:37 [INFO]: epoch 559: training loss 0.0192\n",
      "2025-05-29 21:04:37 [INFO]: epoch 560: training loss 0.0269\n",
      "2025-05-29 21:04:37 [INFO]: epoch 561: training loss 0.0276\n",
      "2025-05-29 21:04:37 [INFO]: epoch 562: training loss 0.0270\n",
      "2025-05-29 21:04:37 [INFO]: epoch 563: training loss 0.0306\n",
      "2025-05-29 21:04:37 [INFO]: epoch 564: training loss 0.0259\n",
      "2025-05-29 21:04:37 [INFO]: epoch 565: training loss 0.0296\n",
      "2025-05-29 21:04:37 [INFO]: epoch 566: training loss 0.0246\n",
      "2025-05-29 21:04:37 [INFO]: epoch 567: training loss 0.0248\n",
      "2025-05-29 21:04:37 [INFO]: epoch 568: training loss 0.0248\n",
      "2025-05-29 21:04:37 [INFO]: epoch 569: training loss 0.0228\n",
      "2025-05-29 21:04:37 [INFO]: epoch 570: training loss 0.0210\n",
      "2025-05-29 21:04:37 [INFO]: epoch 571: training loss 0.0224\n",
      "2025-05-29 21:04:37 [INFO]: epoch 572: training loss 0.0279\n",
      "2025-05-29 21:04:37 [INFO]: epoch 573: training loss 0.0203\n",
      "2025-05-29 21:04:37 [INFO]: epoch 574: training loss 0.0206\n",
      "2025-05-29 21:04:37 [INFO]: epoch 575: training loss 0.0222\n",
      "2025-05-29 21:04:37 [INFO]: epoch 576: training loss 0.0220\n",
      "2025-05-29 21:04:37 [INFO]: epoch 577: training loss 0.0228\n",
      "2025-05-29 21:04:37 [INFO]: epoch 578: training loss 0.0214\n",
      "2025-05-29 21:04:37 [INFO]: epoch 579: training loss 0.0202\n",
      "2025-05-29 21:04:37 [INFO]: epoch 580: training loss 0.0251\n",
      "2025-05-29 21:04:37 [INFO]: epoch 581: training loss 0.0245\n",
      "2025-05-29 21:04:37 [INFO]: epoch 582: training loss 0.0237\n",
      "2025-05-29 21:04:37 [INFO]: epoch 583: training loss 0.0233\n",
      "2025-05-29 21:04:37 [INFO]: epoch 584: training loss 0.0242\n",
      "2025-05-29 21:04:37 [INFO]: epoch 585: training loss 0.0267\n",
      "2025-05-29 21:04:37 [INFO]: epoch 586: training loss 0.0213\n",
      "2025-05-29 21:04:37 [INFO]: epoch 587: training loss 0.0262\n",
      "2025-05-29 21:04:37 [INFO]: epoch 588: training loss 0.0293\n",
      "2025-05-29 21:04:37 [INFO]: epoch 589: training loss 0.0200\n",
      "2025-05-29 21:04:37 [INFO]: epoch 590: training loss 0.0258\n",
      "2025-05-29 21:04:37 [INFO]: epoch 591: training loss 0.0228\n",
      "2025-05-29 21:04:37 [INFO]: epoch 592: training loss 0.0244\n",
      "2025-05-29 21:04:37 [INFO]: epoch 593: training loss 0.0297\n",
      "2025-05-29 21:04:37 [INFO]: epoch 594: training loss 0.0248\n",
      "2025-05-29 21:04:37 [INFO]: epoch 595: training loss 0.0238\n",
      "2025-05-29 21:04:37 [INFO]: epoch 596: training loss 0.0239\n",
      "2025-05-29 21:04:37 [INFO]: epoch 597: training loss 0.0191\n",
      "2025-05-29 21:04:37 [INFO]: epoch 598: training loss 0.0253\n",
      "2025-05-29 21:04:37 [INFO]: epoch 599: training loss 0.0295\n",
      "2025-05-29 21:04:37 [INFO]: Finished training.\n",
      "2025-05-29 21:04:37 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:37<00:00,  7.59s/it]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "imputation_list_3 = []\n",
    "imputation_list_4 = []\n",
    "\n",
    "for model in range(3,5):\n",
    "    \n",
    "    if model ==3:\n",
    "        epoch = 300\n",
    "    else:\n",
    "        epoch = 600\n",
    "        \n",
    "    for i in tqdm( range(5) ):             # 运行5次计算标准差\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        saits = SAITS(n_steps=30, n_features=6, n_layers=2, d_model=128, d_inner=64, n_heads=4, d_k=64, d_v=64, dropout=0.1, batch_size=10, epochs = epoch)\n",
    "\n",
    "        dataset = {\"X\": X_missed_2}\n",
    "\n",
    "        saits.fit(dataset)               \n",
    "        imputation = saits.impute(dataset)  \n",
    "        \n",
    "        if model ==3:\n",
    "            imputation_list_3.append(imputation)\n",
    "        else:\n",
    "            imputation_list_4.append(imputation)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9d868c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "(5, 30, 6)\n",
      "(5, 30, 6)\n"
     ]
    }
   ],
   "source": [
    "print(len(imputation_list_3))\n",
    "print(len(imputation_list_4))\n",
    "\n",
    "imp_3 = np.array(imputation_list_3).reshape((5,30,6)) \n",
    "print(imp_3.shape)\n",
    "\n",
    "imp_4 = np.array(imputation_list_4).reshape((5,30,6)) \n",
    "print(imp_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2ca2f8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 6)\n",
      "(30, 6)\n"
     ]
    }
   ],
   "source": [
    "res_3 = torch.from_numpy(imp_3)\n",
    "\n",
    "res3_005 = torch.quantile( res_3, 0.05, dim=0 ).cpu().numpy()\n",
    "res3_05 = torch.quantile( res_3, 0.5, dim=0 ).cpu().numpy()\n",
    "res3_095 = torch.quantile( res_3, 0.95, dim=0 ).cpu().numpy()\n",
    "res3_x= torch.quantile( res_3, 0.8, dim=0 ).cpu().numpy()\n",
    "print(res3_005.shape)\n",
    "\n",
    "\n",
    "\n",
    "res_4 = torch.from_numpy(imp_4)\n",
    "\n",
    "res4_005 = torch.quantile( res_4, 0.05, dim=0 ).cpu().numpy()\n",
    "res4_05 = torch.quantile( res_4, 0.5, dim=0 ).cpu().numpy()\n",
    "res4_095 = torch.quantile( res_4, 0.95, dim=0 ).cpu().numpy()\n",
    "res4_x= torch.quantile( res_4, 0.2, dim=0 ).cpu().numpy()\n",
    "print(res4_005.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "81da71b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(7,4))\n",
    "\n",
    "X_missed = X_missed_2\n",
    "\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(res4_x[:,3], c='k', label='imp', alpha=0.8) \n",
    "plt.fill_between( range(0,30), res3_005[:,3], res3_095[:,3], color='#ff7f0e', alpha=0.4)  # alpha是透明程度\n",
    "plt.fill_between( range(0,30), res4_005[:,3], res4_095[:,3], color='#ff7f0e', alpha=0.8)  \n",
    "plt.plot(X_missed[0][:,3].cpu().numpy(), c='#1f77b4', marker='o',alpha=1, markersize='4') # 观测值的蓝色把之前的值覆盖了 生成区域的值还是之前的0.5分位数的\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'第一个样本的图'\n",
    "X_missed = X_missed_1\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(res2_x[:,3], c='k', label='imp', alpha=0.8) \n",
    "plt.fill_between( range(0,30), res1_005[:,3], res1_095[:,3], color='#ff7f0e', alpha=0.4)  # alpha是透明程度\n",
    "plt.fill_between( range(0,30), res2_005[:,3], res2_095[:,3], color='#ff7f0e', alpha=0.8)  \n",
    "plt.plot(X_missed[0][:,3].cpu().numpy(), c='#1f77b4', marker='o',alpha=1, markersize='4') # 观测值的蓝色把之前的值覆盖了 生成区域的值还是之前的0.5分位数的\n",
    "\n",
    "\n",
    "plt.savefig('分位数',bbox_inches='tight',dpi=500)\n",
    "plt.savefig('分位数.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
